# Validate Tree Detection and Crown Delineation

In this section we'll use the benchmark data made available in the [`NeonTreeEvaluation`](https://github.com/weecology/NeonTreeEvaluation_package) data set ([Weinstein et al. 2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) to evaluate our process for lidar-based tree detection. We'll implement our tree detection process via the [`cloud2trees`]https://github.com/georgewoolsey/cloud2trees) package

First, load the standard libraries

```{r, warning=FALSE, message=FALSE}
# bread-and-butter
library(tidyverse) # the tidyverse
library(viridis) # viridis colors
library(harrypotter) # hp colors
library(RColorBrewer) # brewer colors
library(scales) # work with number and plot scales
library(latex2exp)

# visualization
library(mapview) # interactive html maps
library(kableExtra) # tables
library(patchwork) # combine plots
library(ggnewscale) # ggnewscale

# spatial analysis
library(sf) # simple features
library(lidR) # lidar data
library(cloud2trees) # tha cloud2trees
library(NeonTreeEvaluation) # benchmark data
```

```{r, include=F, warning=F, message=F}
# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  # , results = 'hide'
  , fig.width = 10.5
  , fig.height = 7
)
# option to put satellite imagery as base layer of mapview maps
  mapview::mapviewOptions(
    homebutton = FALSE
    # , basemaps = c("Esri.WorldImagery","OpenStreetMap")
    , basemaps = c("OpenStreetMap", "Esri.WorldImagery")
  )
# clean session
remove(list = ls())
gc()
```

## `NeonTreeEvaluation` overview

Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) developed: 

>a benchmark dataset of individual canopy crowns derived from multi-sensor imagery in the National Ecological Observatory Network (Table 1) that provides: 1) co-registered remote sensing data from multiple sensors (LiDAR, RGB imagery, and hyperspectral imagery) to allow comparisons of methods based on any single sensor (e.g., for LiDAR based methods), or any combination of sensors (e.g., combining RGB and hyperspectral), and 2) three types of evaluation data to allow assessing both 'tree detection', defined as the identifying the location of individual trees using evaluation data with a point at the crown center , and 'crown delineation' defined as identifying the boundary edge of crowns across a broad range of forest types. The benchmark is designed to allow flexibility in both workflow and sensor selection. (p. 2)

**Table 1.** Summary of datasets included in the benchmark dataset. All sensor data has been cropped to the extent of NEON field sampling plots.

```{r, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.show='hold',results='asis'}
# https://journals.plos.org/ploscompbiol/article/figure?id=10.1371/journal.pcbi.1009180.t001
knitr::include_graphics("../data/journal.pcbi.1009180.t001.PNG")
```

Note the three data labeled as "Evaluation data" in the table. If you are asking "why three evaluation datasets?", the authors provide some detail:

>The inclusion of multiple evaluation types is critical because each type of evaluation data has strengths and limitations in evaluating model performance. Field collected stems are the most common evaluation data used in crown detection work due to high confidence that each stem represents a location of a single tree. However, the position of a tree stem can fail to accurately represent the position of the crown as viewed from above due to a combination of spatial errors in alignment with the image data and the tendency for trees to grow at acute angles (tree lean is not measured in the NEON data), such that the center of the crown and position of the stem can be offset by several meters....Image-annotated crowns are relatively easy to scale, allowing the collection of data for a wide range of forest types and for annotation of every visible crown in the image. Using image-annotated crowns supports the evaluation of methods across a broad range of forest types and allows both recall and precision to be calculated. However, since these annotations are not generated by an observer in the field there can be errors due to interpreting the images. This problem is solved using field-annotated crowns in which an observer annotates the remote-sensing imagery on a tablet while in the field [33]. The main limitation to this approach is that it is labor intensive, meaning that only a relatively small amount of validation data can be collected, making it difficult to obtain a large number of crowns across broad scales or assess model precision. Given the tradeoffs in each evaluation type, providing multiple criteria is a useful way of balancing the need for broad scale model verification with rigorous evaluation of field-based measurements. (p. 14-15)

To evaluate the performance of our aerial point cloud-based algorithm for 1) tree detection and 2) crown delineation using `NeonTreeEvaluation` we need to ensure our tree polygon data is formatted properly:

>This package takes a standard submission format of predicted crowns in either bounding box or polygons as input and returns the evaluation scores of the detections for each of the three evaluation datasets. This reproducible workflow will facilitate creating a transparent process for future comparisons among crown detection algorithms. (p. 14)

The authors describe the "standard submission format" on the [package GitHub](https://github.com/weecology/NeonTreeEvaluation_package):

> Each row contains information for one predicted bounding box. The plot_name should be named the same as the files in the dataset without extension (e.g. SJER_021_2018 not SJER_021_2018.tif) and not the full path to the file on disk. Not all evaluation data are available for all plots. Functions like `evaluate_field_crowns` and `evaluate_image_crowns` will look for matching plot name and ignore other plots. Depending on the speed of the algorithm, the simplest thing to do is predict all images in the RGB folder (see list_rgb()) and the package will handle matching images with the correct data to the correct evaluation procedure...Instead of bounding boxes, some methods may return polygons. To submit as polygons, create a single unprojected shapefile with polygons in image coordinates. Polygons must be complete with no holes. Here is an example of the above csv file in polygon format. Here the xmin, xmax, etc. columns are ignored since the information is stored in the geometry data.

``` r
Simple feature collection with 6 features and 7 fields
geometry type:  POLYGON
dimension:      XY
bbox:           xmin: 30.39723 ymin: 122.1164 xmax: 397.5746 ymax: 400
CRS:            NA
       xmin     ymin      xmax     ymax     score label     plot_name
1  41.01716 230.8854 151.08607 342.6985 0.8098674  Tree DSNY_014_2019
2 357.32129 122.1164 397.57458 159.3758 0.6968824  Tree DSNY_014_2019
3  30.39723 136.9157  73.79434 184.9473 0.5713338  Tree DSNY_014_2019
4 260.65921 285.6689 299.68811 326.7933 0.5511004  Tree DSNY_014_2019
5 179.34564 371.6130 232.49385 400.0000 0.4697072  Tree DSNY_014_2019
6 316.27377 378.9802 363.67542 400.0000 0.3259409  Tree DSNY_014_2019
                     st_sfc.lst.
1 POLYGON ((41.01716 230.8854...
2 POLYGON ((357.3213 122.1164...
3 POLYGON ((30.39723 136.9157...
4 POLYGON ((260.6592 285.6689...
5 POLYGON ((179.3456 371.613,...
6 POLYGON ((316.2738 378.9802...
```

So we are going to: run `cloud2trees::cloud2trees()` on all lidar data, combine into a single tree list with a row unique by a detected tree and the `plot_name` column (e.g. "SJER_021_2018"), as an unprojected `sf` data with polygons in image coordinates. We may need to run `cloud2trees::simplify_multipolygon_crowns()` prior to submission.

## lidar data in `NeonTreeEvaluation`

we first have to download evaluation data from the Zenodo archive (1GB), use the download() function to place the data in the correct package location. Download the much larger training data, set `training=TRUE`.

```{r, warning=FALSE, message=F,results=F}
NeonTreeEvaluation::download(training = T, force = F)
```

```{r, include=FALSE, eval=FALSE}
# what is in this thing?
lsf.str("package:NeonTreeEvaluation")
# where is this data 
NeonTreeEvaluation::list_rgb() %>% 
  dirname() %>% 
  unique()
# what's this?
NeonTreeEvaluation::field %>% dplyr::glimpse()
```

let's find what data is available

```{r}
# i did some digging around and the lidar data is here
lidar_dir_temp <- system.file(package = "NeonTreeEvaluation", "extdata", "NeonTreeEvaluation", "evaluation", "LiDAR")
# files
lidar_files_temp <- lidar_dir_temp %>% list.files(recursive = T, pattern = ".*\\.(laz|las)$", full.names = T) %>% unique()
# look at this
lidar_files_temp %>% basename() %>% sample(size = 9)
```

```{r}
# let's pull out all sites with `.laz` data and create a data frame for tracking purposes
lidar_df <- lidar_files_temp %>% 
    dplyr::as_tibble() %>% 
    dplyr::rename(f_path = 1) %>% 
    # create some other variables
    dplyr::mutate(
      plot_name = f_path %>% basename() %>% stringr::str_remove_all("\\.(laz|las)$")
    )
# what?
lidar_df %>% dplyr::glimpse()
```

that's a lot of files...let's only process the sites with evaluation data

```{r}
# there are functions to get a list of all evaluation data
# let's use these to filter our lidar files
plotnames_temp <- c(
    NeonTreeEvaluation::list_annotations()
    , NeonTreeEvaluation::list_field_stems()
    # this one includes file paths, so we have to clean
    , NeonTreeEvaluation::list_field_crowns() %>% 
      stringr::str_match(pattern="(\\w+).tif") %>% 
      .[,2]
    # there are plot_names from the submission data too
    , NeonTreeEvaluation::submission_polygons$plot_name %>% unique()
    , NeonTreeEvaluation::submission$plot_name %>% unique()
  ) %>% 
  unique()
# huh?
plotnames_temp %>% sample(11)
```

filter our lidar data list

```{r}
lidar_df <- lidar_df %>% 
  #filter based on plots in evaluation data
  dplyr::filter(plot_name %in% plotnames_temp) %>% 
  # pull out site 
  dplyr::mutate(
    siteID = stringr::str_extract(plot_name, "[A-Z]+")
  )
# what?
lidar_df %>% dplyr::glimpse()
```

we will want to limit our evaluation to only sites with conifer trees since `cloud2trees` implements methods developed specifically to quantify conifer forest structure that may not be appropriate for other uses.

we'll use the field data in the package to look for NEON sites with conifer trees. We'll use the NEON plant list to identify conifer species: [https://data.neonscience.org/taxonomic-lists?taxonTypeCode=PLANT](https://data.neonscience.org/taxonomic-lists?taxonTypeCode=PLANT) (click "DOWNLOAD TAXONOMIC LIST"). We'll filter for species belonging to Class Pinopsida.

```{r}
conifer_spp <-
  readr::read_csv(
    "../data/OS_TAXON_PLANT-20220330T142149.csv"
    , show_col_types = F
    , progress = F
  ) %>% 
  dplyr::filter(
    tolower(`class`) %in% c("pinopsida")
  ) %>% 
  dplyr::mutate(
    taxonID = toupper(taxonID)
    , vernacularName = tolower(vernacularName)
    , genus = stringr::str_to_title(genus)
  ) %>% 
  dplyr::distinct(taxonID, vernacularName, genus)
```

what are some of these conifers?

```{r}
# huh?
conifer_spp %>% 
  dplyr::slice_sample(n = 10) %>% 
  kableExtra::kbl(caption = "Conifer species taxonID examples") %>% 
  kableExtra::kable_styling()
```

filter for NEON sites that have conifer trees based on field data from all terrestrial NEON sites with qualifying woody vegetation: [https://data.neonscience.org/data-products/DP1.10098.001](https://data.neonscience.org/data-products/DP1.10098.001)

```{r}
conifer_sites <- NeonTreeEvaluation::field %>% 
  dplyr::left_join(
    conifer_spp %>% dplyr::mutate(is_conifer = 1)
    , by = "taxonID"
  ) %>% 
  dplyr::mutate(is_conifer = dplyr::coalesce(is_conifer, 0)) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    tot = dplyr::n()
    , conifer = sum(is_conifer)
    , latitude = mean(plotLatitude)
    , longitude = mean(plotLongitude)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(pct_conifer = conifer/tot)
```

what is the breakdown of woody vegetation sampled in NEON sites by the percent conifer?

```{r}
conifer_sites %>% 
  dplyr::select(-c(longitude,latitude)) %>% 
  dplyr::arrange(desc(pct_conifer), desc(tot)) %>% 
  dplyr::slice_head(n = 19) %>% 
  kableExtra::kbl(caption = "Conifers in NEON sites", digits = 2) %>% 
  kableExtra::kable_styling()
```

let's only keep NEON sites with >50% of the woody vegetation sampled as conifer

```{r}
# minimum pct conifer
min_conifer_pct <- .5
# data frame of sites
conifer_sites <- conifer_sites %>%
  dplyr::filter(pct_conifer>=min_conifer_pct)
```

finally, we'll filter our lidar processing data for only these conifer sites

```{r}
lidar_df <- lidar_df %>% 
  dplyr::inner_join(conifer_sites, by = "siteID") %>% 
  sf::st_as_sf(coords = c("longitude","latitude"), crs = 4326, remove = F)
```

what NEON sites have conifers and the most lidar plots

```{r}
lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::count(siteID) %>% 
  dplyr::arrange(desc(n)) %>% 
  dplyr::slice_head(n=11) %>% 
  kableExtra::kbl(caption = "NEON sites with conifers and lidar plots") %>% 
  kableExtra::kable_styling()
```

where are these though?

```{r}
lidar_df %>% 
  dplyr::count(siteID) %>% 
  mapview::mapview(
    zcol = "siteID", legend = F
    , layer.name = "NEON site"
    , col.regions = viridis::turbo(n=nrow(conifer_sites))
  )
```

that's pretty good geographic coverage and in places that we expect to have conifers ;D

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Example validation process

now that we have our lidar data that we can test our process against, let's walk through the validation for a single plot

```{r}

```

