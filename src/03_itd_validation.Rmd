# Validate Tree Detection and Crown Delineation

In this section we'll use the benchmark data made available in the [`NeonTreeEvaluation`](https://github.com/weecology/NeonTreeEvaluation_package) data set ([Weinstein et al. 2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) to evaluate our process for lidar-based tree detection. We'll implement our tree detection process via the [`cloud2trees`](https://github.com/georgewoolsey/cloud2trees) package.

Puliti et al. [(2023)](https://doi.org/10.48550/arXiv.2309.01279) also provide the benchmarking dataset "FOR-instance" for dense airborne laser scanning data. The dataset comprises five curated UAV-based laser scanning data collections from diverse global locations, representing various forest types. The data was collected over five sites in Norway, Czech Republic, Austria, Australia, and New Zealand and covers mature forests for the following forest types: boreal (42% of the number of trees) and temperate coniferous (5%), temperate mixed deciduous (29%), coniferous plantation (13%), and dry sclerophyll forests (11%). We might also consider this data for validation of our methods.

First, load the standard libraries

```{r, warning=FALSE, message=FALSE}
# bread-and-butter
library(tidyverse) # the tidyverse
library(viridis) # viridis colors
library(harrypotter) # hp colors
library(RColorBrewer) # brewer colors
library(scales) # work with number and plot scales
library(latex2exp)

# visualization
library(mapview) # interactive html maps
library(kableExtra) # tables
library(patchwork) # combine plots
library(ggnewscale) # ggnewscale
library(rgl) # rgl plotting

# spatial analysis
library(sf) # simple features
library(lidR) # lidar data
library(cloud2trees) # tha cloud2trees
library(NeonTreeEvaluation) # benchmark data
```

```{r, include=F, warning=F, message=F}
# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  # , results = 'hide'
  , fig.width = 10.5
  , fig.height = 7
)
# option to put satellite imagery as base layer of mapview maps
  mapview::mapviewOptions(
    homebutton = FALSE
    # , basemaps = c("Esri.WorldImagery","OpenStreetMap")
    , basemaps = c("OpenStreetMap", "Esri.WorldImagery")
  )
# clean session
remove(list = ls())
gc()
```

## ITD tuning{#itd_more}

Before we perform tree extraction on the evaluation data we'll use the training data to tune our process for lidar-based tree detection implemented via the [`cloud2trees`]https://github.com/georgewoolsey/cloud2trees) package. Specifically, we are going to use the training data to determine the "best" ITD window function to use for a given NEON site since the `cloud2trees` package [strongly recommends](https://github.com/georgewoolsey/cloud2trees?tab=readme-ov-file#individual-tree-detection-itd-tuning) that "a different window size function is defined for each region of your study area with significantly different forest structure". See the `lidR` [package book section](https://r-lidar.github.io/lidRbook/itd.html) by point cloud processing expert [Jean-Romain Roussel](https://github.com/Jean-Romain) for excellent detail on ITD and defining window size.

### Default variable window functions

We discussed our method for individual tree detection (ITD) in [this prior section](#itd). For the purpose of working across different conifer sites with potentially vastly different forest structures we are going to augment the default ITD variable window size functions in `cloud2trees::itd_ws_functions()`

Let's see what the default variable window functions look like

```{r}
# get ws by ht for each fn
ws_fn_df <- 1:length(cloud2trees::itd_ws_functions()) %>%
  purrr::map(function(x){
    nm <- cloud2trees::itd_ws_functions()[x] %>% names() %>% as.character()
    fn <- cloud2trees::itd_ws_functions()[[x]]
    # est
    height <- seq(from=0,to=60,by=0.5)
    ws <- fn(height) %>% unlist()
    df <- dplyr::tibble(
        height = height
        , ws = ws
      ) %>% 
      dplyr::mutate(ws_fn_nm = nm) %>% 
      dplyr::relocate(ws_fn_nm)
    return(df)
  }) %>% 
  dplyr::bind_rows()
# huh?
ws_fn_df %>% dplyr::glimpse()
```

plot of `cloud2trees` default ITD variable window function

```{r}
# define custom colors
pal_ws <- 
  c(
    RColorBrewer::brewer.pal(n = 7, name = "Purples")[2:6] %>% rev()
    , RColorBrewer::brewer.pal(n = 11, name = "BrBG")[7:11] %>% rev()
    , RColorBrewer::brewer.pal(n = 7, name = "Oranges")[2:6] %>% rev()
    , RColorBrewer::brewer.pal(n = 7, name = "Greys")[2:6] %>% rev()
  )
# scales::show_col(pal_ws)
# plot
ws_fn_df %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = height, y = ws, color = ws_fn_nm)) +
  ggplot2::geom_line(lwd=1) +
  ggplot2::scale_color_manual(values = pal_ws[c(1,6,11)]) +
  ggplot2::xlim(-3,NA) +
  ggplot2::ylim(-0.1,NA) +
  ggplot2::labs(
    x = "heights", y = "ws"
    , color = "variable\nwindow\nfunction"
    , subtitle = "`cloud2trees` default ITD variable window function"
  ) +
  ggplot2::theme_light() +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(lwd = 6))
  )
```

### Augmented variable window functions

now we'll augment the default functions to make:

* the exponential function (concave up) have slightly less ("les"), less ("less"), much less ("lesss") and slightly more ("mor") concavity (4 new functions)
* the logarithmic function (concave down) have slightly more ("mor"), more ("morr"), slightly less ("les"), and less ("less") concavity (4 new functions)
* the linear function have a higher and lower intercept and a higher and lower slope (4 new functions)

for a total of 15 functions tested

```{r}
# set up initial list with default functions
my_ws_functions <- cloud2trees::itd_ws_functions()
# add to list
my_ws_functions$log_les_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > exp(5)-1 ~ 5
      , TRUE ~ log(x+1)
    )
    return(y)
}
# add to list
my_ws_functions$log_less_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > exp(3/0.5)-1 ~ 3
      , TRUE ~ 0.5*log(x+1)
    )
    return(y)
}
# add to list
my_ws_functions$log_morr_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > exp(9/2)-1 ~ 9
      , TRUE ~ 2*log(x+1)
    )
    return(y)
}
# add to list
my_ws_functions$log_mor_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > exp(7/1.5)-1 ~ 7
      , TRUE ~ 1.5*log(x+1)
    )
    return(y)
}
# add to list
my_ws_functions$exp_mor_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > log(8+1)/log(1.1) ~ 8
      , TRUE ~ (1.1^x)-1
    )
    return(y)
}
# add to list
my_ws_functions$exp_lesss_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > log(5+1)/log(1.015) ~ 5
      , TRUE ~ (1.015^x)-1
    )
    return(y)
}
# add to list
my_ws_functions$exp_less_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > log(5+1)/log(1.03) ~ 5
      , TRUE ~ (1.03^x)-1
    )
    return(y)
}
# add to list
my_ws_functions$exp_les_ccv_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > log(5+1)/log(1.05) ~ 5
      , TRUE ~ (1.05^x)-1
    )
    return(y)
}
# add to list
my_ws_functions$lin_hi_int_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > (8-1.2)/0.14 ~ 8
      , TRUE ~ 1.2 + (x * 0.14)
    )
    return(y)
}
# add to list
my_ws_functions$lin_lo_int_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < (0.5+2)/0.14 ~ 0.5
      , x > (6+2)/0.14 ~ 6
      , TRUE ~ -2 + (x * 0.14)
    )
    return(y)
}
# add to list
my_ws_functions$lin_hi_slp_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > (8-0.75)/0.3 ~ 8
      , TRUE ~ 0.75 + (x * 0.3)
    )
    return(y)
}
# add to list
my_ws_functions$lin_lo_slp_fn <- function (x) {
    y <- dplyr::case_when(
      is.na(x) ~ 0.001
      , x < 0 ~ 0.001
      , x > (4-0.75)/0.04 ~ 4
      , TRUE ~ 0.75 + (x * 0.04)
    )
    return(y)
}
```

```{r, include=FALSE, eval=FALSE}
ggplot2::ggplot() +
    ggplot2::geom_function(ggplot2::aes(color="lin_fn"), fun = my_ws_functions[["lin_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="exp_fn"), fun = my_ws_functions[["exp_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="log_fn"), fun = my_ws_functions[["log_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="log_mor_ccv_fn"), fun = my_ws_functions[["log_mor_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="log_morr_ccv_fn"), fun = my_ws_functions[["log_morr_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="log_les_ccv_fn"), fun = my_ws_functions[["log_les_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="log_less_ccv_fn"), fun = my_ws_functions[["log_less_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="exp_morrr_ccv_fn"), fun = my_ws_functions[["exp_morrr_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="exp_morr_ccv_fn"), fun = my_ws_functions[["exp_morr_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="exp_mor_ccv_fn"), fun = my_ws_functions[["exp_mor_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="exp_less_ccv_fn"), fun = my_ws_functions[["exp_less_ccv_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="lin_hi_int_fn"), fun = my_ws_functions[["lin_hi_int_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="lin_lo_int_fn"), fun = my_ws_functions[["lin_lo_int_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="lin_hi_slp_fn"), fun = my_ws_functions[["lin_hi_slp_fn"]]) +
    ggplot2::geom_function(ggplot2::aes(color="lin_lo_slp_fn"), fun = my_ws_functions[["lin_lo_slp_fn"]]) +
    ggplot2::xlim(-1,60) +
    ggplot2::ylim(-1,12)
```

run each function over a range of heights to see what they return on a plot

```{r}
# get ws by ht for each fn
ws_fn_df <- 1:length(my_ws_functions) %>%
  purrr::map(function(x){
    nm <- my_ws_functions[x] %>% names() %>% as.character()
    fn <- my_ws_functions[[x]]
    # est
    height <- seq(from=0,to=60,by=0.5)
    ws <- fn(height) %>% unlist()
    df <- dplyr::tibble(
        height = height
        , ws = ws
      ) %>% 
      dplyr::mutate(ws_fn_nm = nm) %>% 
      dplyr::relocate(ws_fn_nm)
    return(df)
  }) %>% 
  dplyr::bind_rows()
# huh?
ws_fn_df %>% dplyr::glimpse()
```

plot of all ITD variable window functions for testing

```{r}
ws_fn_df %>% 
  ggplot2::ggplot(mapping = ggplot2::aes(x = height, y = ws, color = ws_fn_nm)) +
  ggplot2::geom_line(lwd=1) +
  ggplot2::scale_color_manual(values = pal_ws) +
  ggplot2::xlim(-3,NA) +
  ggplot2::ylim(-0.1,NA) +
  ggplot2::labs(
    x = "heights", y = "ws"
    , color = "variable\nwindow\nfunction"
    , subtitle = "ITD variable window functions for testing"
  ) +
  ggplot2::theme_light() +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(lwd = 6))
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(ws_fn_df)
gc()
```

## `NeonTreeEvaluation` benchmark

Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) developed: 

>a benchmark dataset of individual canopy crowns derived from multi-sensor imagery in the National Ecological Observatory Network (Table 1) that provides: 1) co-registered remote sensing data from multiple sensors (LiDAR, RGB imagery, and hyperspectral imagery) to allow comparisons of methods based on any single sensor (e.g., for LiDAR based methods), or any combination of sensors (e.g., combining RGB and hyperspectral), and 2) three types of evaluation data to allow assessing both 'tree detection', defined as the identifying the location of individual trees using evaluation data with a point at the crown center , and 'crown delineation' defined as identifying the boundary edge of crowns across a broad range of forest types. The benchmark is designed to allow flexibility in both workflow and sensor selection. (p. 2)

**Table 1.** Summary of datasets included in the benchmark dataset. All sensor data has been cropped to the extent of NEON field sampling plots.

```{r, echo=FALSE, out.width="100%", out.height="100%", fig.align='center', fig.show='hold',results='asis'}
# https://journals.plos.org/ploscompbiol/article/figure?id=10.1371/journal.pcbi.1009180.t001
knitr::include_graphics("https://i.ibb.co/9kpdPKLN/journal-pcbi-1009180-t001.png")
```

Note the three data labeled as "Evaluation data" in the table. If you are asking "why three evaluation datasets?", the authors provide some detail:

>The inclusion of multiple evaluation types is critical because each type of evaluation data has strengths and limitations in evaluating model performance. Field collected stems are the most common evaluation data used in crown detection work due to high confidence that each stem represents a location of a single tree. However, the position of a tree stem can fail to accurately represent the position of the crown as viewed from above due to a combination of spatial errors in alignment with the image data and the tendency for trees to grow at acute angles (tree lean is not measured in the NEON data), such that the center of the crown and position of the stem can be offset by several meters....Image-annotated crowns are relatively easy to scale, allowing the collection of data for a wide range of forest types and for annotation of every visible crown in the image. Using image-annotated crowns supports the evaluation of methods across a broad range of forest types and allows both recall and precision to be calculated. However, since these annotations are not generated by an observer in the field there can be errors due to interpreting the images. This problem is solved using field-annotated crowns in which an observer annotates the remote-sensing imagery on a tablet while in the field [33]. The main limitation to this approach is that it is labor intensive, meaning that only a relatively small amount of validation data can be collected, making it difficult to obtain a large number of crowns across broad scales or assess model precision. Given the tradeoffs in each evaluation type, providing multiple criteria is a useful way of balancing the need for broad scale model verification with rigorous evaluation of field-based measurements. (p. 14-15)

To evaluate the performance of our aerial point cloud-based algorithm for 1) tree detection and 2) crown delineation using `NeonTreeEvaluation` we need to ensure our tree polygon data is formatted properly:

>This package takes a standard submission format of predicted crowns in either bounding box or polygons as input and returns the evaluation scores of the detections for each of the three evaluation datasets. This reproducible workflow will facilitate creating a transparent process for future comparisons among crown detection algorithms. (p. 14)

The authors describe the "standard submission format" on the [package GitHub](https://github.com/weecology/NeonTreeEvaluation_package):

> Each row contains information for one predicted bounding box. The plot_name should be named the same as the files in the dataset without extension (e.g. SJER_021_2018 not SJER_021_2018.tif) and not the full path to the file on disk. Not all evaluation data are available for all plots. Functions like `evaluate_field_crowns` and `evaluate_image_crowns` will look for matching plot name and ignore other plots. Depending on the speed of the algorithm, the simplest thing to do is predict all images in the RGB folder (see list_rgb()) and the package will handle matching images with the correct data to the correct evaluation procedure...Instead of bounding boxes, some methods may return polygons. To submit as polygons, create a single unprojected shapefile with polygons in image coordinates. Polygons must be complete with no holes. Here is an example of the above csv file in polygon format. Here the xmin, xmax, etc. columns are ignored since the information is stored in the geometry data.

``` r
Simple feature collection with 6 features and 7 fields
geometry type:  POLYGON
dimension:      XY
bbox:           xmin: 30.39723 ymin: 122.1164 xmax: 397.5746 ymax: 400
CRS:            NA
       xmin     ymin      xmax     ymax     score label     plot_name
1  41.01716 230.8854 151.08607 342.6985 0.8098674  Tree DSNY_014_2019
2 357.32129 122.1164 397.57458 159.3758 0.6968824  Tree DSNY_014_2019
3  30.39723 136.9157  73.79434 184.9473 0.5713338  Tree DSNY_014_2019
4 260.65921 285.6689 299.68811 326.7933 0.5511004  Tree DSNY_014_2019
5 179.34564 371.6130 232.49385 400.0000 0.4697072  Tree DSNY_014_2019
6 316.27377 378.9802 363.67542 400.0000 0.3259409  Tree DSNY_014_2019
                     st_sfc.lst.
1 POLYGON ((41.01716 230.8854...
2 POLYGON ((357.3213 122.1164...
3 POLYGON ((30.39723 136.9157...
4 POLYGON ((260.6592 285.6689...
5 POLYGON ((179.3456 371.613,...
6 POLYGON ((316.2738 378.9802...
```

So we are going to: run `cloud2trees::cloud2trees()` on all of the lidar data for which there is evaluation (i.e. ground truth) data, combine into a single tree list with a row unique by a detected tree and the `plot_name` column (e.g. "SJER_021_2018"), as an unprojected `sf` data with polygons in image coordinates. We may need to run `cloud2trees::simplify_multipolygon_crowns()` prior to submission.

### lidar data in `NeonTreeEvaluation`{#neon_lidar}

we first have to download evaluation data from the Zenodo archive (1GB), use the `NeonTreeEvaluation::download()` function to place the data in the correct package location. Download the much larger training data, set `training=TRUE`. We are going to use the training data to determine the "best" ITD window function to use for a given NEON site since the `cloud2trees` package [strongly recommends](https://github.com/georgewoolsey/cloud2trees?tab=readme-ov-file#individual-tree-detection-itd-tuning) that "a different window size function is defined for each region of your study area with significantly different forest structure".

Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) describe the training data: 

>During our research on canopy crown detection algorithms, we annotated many geographic tiles separate from the evaluation data. The training sites were selected to capture a range of forest conditions...The training tiles were chosen at random from the NEON data portal, with the requirement that they did not contain a large amount of missing data and they did not overlap with any evaluation plots. Depending on the tree density at the site, we either annotated the entire 1 km2 tile or cropped it to a smaller size to create more tractable sizes for annotation. This data is released alongside the benchmark dataset; however, our goal is to promote the best possible crown-delineation algorithm regardless of training data, and it is not necessary to use this training data to generate predictions. Given the large size of training tiles, the training annotations were less thoroughly reviewed and were only based on the RGB imagery. (p. 11)

*Note: the current version of `NeonTreeEvaluation::download()` does not download the training data even if setting `training = T`. I dug around and found the archive of the training data at [https://zenodo.org/records/5914554](https://zenodo.org/records/5914554). I'll need to circle back with a GitHub pull request to fix the `NeonTreeEvaluation::download()` function so that it gets the training data which is hosted at the link mentioned previously and does not reside at where the function currently points to ([https://zenodo.org/records/4770593](https://zenodo.org/records/4770593)). I also manually copied the files in "NeonTreeEvaluation/extdata/NeonTreeEvaluation/training/RGB" to "NeonTreeEvaluation/extdata/NeonTreeEvaluation/evaluation/RGB" so that evaluation could be performed. See `list.dirs(system.file(package = "NeonTreeEvaluation"),recursive=T)`*

```{r, warning=FALSE, message=F,results=F,eval=FALSE}
NeonTreeEvaluation::download(training = T, force = F)
```

```{r, include=FALSE, eval=FALSE}
# what is in this thing?
lsf.str("package:NeonTreeEvaluation")
# where is this data 
NeonTreeEvaluation::list_rgb() %>% 
  dirname() %>% 
  unique()
# what's this?
NeonTreeEvaluation::field %>% dplyr::glimpse()
```

#### Training data

Let's find what *training* data is available. We'll use this data to determine the best window size function for use in our individual tree detection (ITD) process

```{r}
# i did some digging around and the TRAINING lidar data is here
training_dir_temp <- system.file(package = "NeonTreeEvaluation", "extdata", "NeonTreeEvaluation", "training", "LiDAR")
# files
training_files_temp <- training_dir_temp %>% list.files(recursive = T, pattern = ".*\\.(laz|las)$", full.names = T) %>% unique()
```

let's pull out all NEON sites with point cloud data and create a data frame for tracking purposes

```{r}
# let's pull out all sites with `.laz` data and create a data frame for tracking purposes
lidar_df <- training_files_temp %>% 
    dplyr::as_tibble() %>% 
    dplyr::rename(f_path = 1) %>% 
    # create some other variables
    dplyr::mutate(
      plot_name = f_path %>% basename() %>% stringr::str_remove_all("\\.(laz|las)$")
      , data_type = "training"
    )
# what?
lidar_df %>% dplyr::glimpse()
```

#### Evaluation data

Let's find what *evaluation* data is available. We'll use this data to evaluate our point cloud-based tree detection methodology.

```{r}
# i did some digging around and the EVALUATION lidar data is here
eval_dir_temp <- system.file(package = "NeonTreeEvaluation", "extdata", "NeonTreeEvaluation", "evaluation", "LiDAR")
# files
eval_files_temp <- eval_dir_temp %>% list.files(recursive = T, pattern = ".*\\.(laz|las)$", full.names = T) %>% unique()
```

bind the rows onto our tracking data

```{r}
lidar_df <- lidar_df %>% 
  dplyr::bind_rows(
    eval_files_temp %>% 
    dplyr::as_tibble() %>% 
    dplyr::rename(f_path = 1) %>% 
    # create some other variables
    dplyr::mutate(
      plot_name = f_path %>% basename() %>% stringr::str_remove_all("\\.(laz|las)$")
      , data_type = "evaluation"
    )
  ) %>% 
  dplyr::mutate(data_type = as.factor(data_type))
```

#### Filter for ground truth data

that's a lot of files...let's only process the sites with evaluation data or image annotated crowns classified as training annotations

```{r}
# there are functions to get a list of all evaluation data
# let's use these to filter our lidar files
plotnames_temp <- c(
    NeonTreeEvaluation::list_annotations()
    , NeonTreeEvaluation::list_field_stems()
    # this one includes file paths, so we have to clean
    , NeonTreeEvaluation::list_field_crowns() %>% 
      stringr::str_match(pattern="(\\w+).tif") %>% 
      .[,2]
    # there are plot_names from the submission data too
    , NeonTreeEvaluation::submission_polygons$plot_name %>% unique()
    , NeonTreeEvaluation::submission$plot_name %>% unique()
    # make sure we have all image annotated crowns
    , system.file(package = "NeonTreeEvaluation", "extdata", "NeonTreeEvaluation", "annotations") %>% 
      list.files() %>% 
      basename() %>% 
      stringr::str_remove_all("\\.(xml)$") %>% 
      unique()
  ) %>% 
  unique()
# huh?
# plotnames_temp %>% sample(11) %>% sort()
```

The field collected stem validation data will take a bit more to get the actual plots for which there is validation data. We have to pull `clean_field_data()` from [`NeonTreeEvaluation`](https://github.com/weecology/NeonTreeEvaluation_package/blob/724bdb0a9a3ea6433450f62c69a8fadce9ccdb66/R/evaluate_field_stems.R) as an internal function which is used to filter `NeonTreeEvaluation::field`...we'll than match other filters used in `NeonTreeEvaluation::evaluate_field_stems()`

```{r}
# this is an internal function from `NeonTreeEvaluation::evaluate_field_stems()`
clean_field_data<-function(field){
  field$area<-field$maxCrownDiameter*field$ninetyCrownDiameter
  field<-field %>%  filter(!is.na(itcEasting),!stringr::str_detect(eventID,"2014"),growthForm %in% c("single bole tree","multi-bole tree","small tree","sapling"),stemDiameter>15) %>%
    droplevels() %>%
    filter(height>3|is.na(height))

  #Limit difference in heights
  to_remove<-field %>% group_by(individualID) %>%
    summarize(mean=mean(height),sum_difference = abs(sum(diff(height)))) %>%
    filter(sum_difference > 8)
  field<-field %>%
    filter(!individualID %in% to_remove$individualID)
}
# apply all the filters for field collected stems used in `NeonTreeEvaluation::evaluate_field_stems()`
field_plots_temp <- NeonTreeEvaluation::field %>% 
  clean_field_data() %>% 
  dplyr::filter(height>3) %>% 
  dplyr::group_by(plotID,individualID) %>%
  dplyr::summarize(samples=length(unique(eventID))) %>%
  dplyr::filter(samples>1) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(plotID=as.character(plotID)) %>% 
  dplyr::pull(plotID) %>% 
  unique()
# `NeonTreeEvaluation::evaluate_field_stems()` also requires rgb and chm data
chm_temp <- NeonTreeEvaluation::list_chm() %>% 
  basename() %>% 
  stringr::str_match("(\\w+)_\\d+") %>% 
  .[,2] %>% 
  unique()
field_plots_temp <- field_plots_temp[field_plots_temp %in% chm_temp]
rgb_temp <- NeonTreeEvaluation::list_rgb() %>% 
  basename() %>% 
  stringr::str_match("(\\w+)_\\d+") %>% 
  .[,2] %>% 
  unique()
field_plots_temp <- field_plots_temp[field_plots_temp %in% rgb_temp]
```

filter our lidar data list to keep only point clouds in plots for which there is validation data

```{r}
lidar_df <-
  lidar_df %>% 
  #filter based on plots in evaluation data
  dplyr::filter(plot_name %in% plotnames_temp) %>% 
  # filter for field plots
  # pull out site 
  dplyr::mutate(
    plot_temp = stringr::str_extract(
      string = plot_name
      , pattern = field_plots_temp %>%
        unique() %>%
        toupper() %>%
        paste(collapse = "|")
    )
  ) %>% 
  # flag the data with image annotated crowns or field stems which we'll use to train our ws fn
  # if this doesn't make sense yet, keep reading down
  dplyr::mutate(
    has_image_annotation = plot_name %in% NeonTreeEvaluation::list_annotations()
    , has_field_stems = !is.na(plot_temp)
  ) %>%
  # keep only those with field stems or image annotated crowns
  dplyr::filter(has_image_annotation | has_field_stems) %>% 
  dplyr::select(-c(plot_temp))
```

We need to put the NEON site identifier in our data. We'll use the field-collected tree data from NEON sites made available which will also serve to filter our point cloud list for forested NEON sites only.

```{r}
# get a list of siteID
neon_sites_temp <-
  NeonTreeEvaluation::field %>% 
  sf::st_drop_geometry() %>% 
  dplyr::distinct(siteID) %>% 
  # there are siteID's here too
  dplyr::bind_rows(
    NeonTreeEvaluation::crown_polygons %>% 
      sf::st_drop_geometry() %>% 
      dplyr::distinct(siteID)
  ) %>% 
  dplyr::mutate(siteID = as.character(siteID)) %>% 
  dplyr::distinct(siteID) %>% 
  dplyr::pull(siteID)
# extract the siteID from the plot_name
lidar_df <- lidar_df %>% 
  # pull out site 
  dplyr::mutate(
    siteID = stringr::str_extract(
      string = plot_name
      , pattern = neon_sites_temp %>%
        unique() %>%
        toupper() %>%
        paste(collapse = "|")
    )
  ) %>% 
  dplyr::filter(!is.na(siteID))
```

what is the breakdown of training vs eval?

```{r}
lidar_df %>% dplyr::count(data_type)
```

#### Filter for conifer sites

we will want to limit our evaluation to only sites with conifer trees since `cloud2trees` implements methods developed specifically to quantify conifer forest structure that may not be appropriate for other forest types

we'll use the field data in the package to look for NEON sites with conifer trees. We'll use the NEON plant list to identify conifer species: [https://data.neonscience.org/taxonomic-lists?taxonTypeCode=PLANT](https://data.neonscience.org/taxonomic-lists?taxonTypeCode=PLANT) (click "DOWNLOAD TAXONOMIC LIST"). We'll filter for species belonging to Class Pinopsida.

```{r}
conifer_spp <-
  readr::read_csv(
    "../data/OS_TAXON_PLANT-20220330T142149.csv"
    , show_col_types = F
    , progress = F
  ) %>% 
  dplyr::filter(
    tolower(`class`) %in% c("pinopsida")
  ) %>% 
  dplyr::mutate(
    taxonID = toupper(taxonID)
    , vernacularName = tolower(vernacularName)
    , genus = stringr::str_to_title(genus)
  ) %>% 
  dplyr::distinct(taxonID, vernacularName, genus)
```

what are some of these conifers?

```{r}
# huh?
conifer_spp %>% 
  dplyr::slice_sample(n = 10) %>% 
  kableExtra::kbl(caption = "Conifer species taxonID examples") %>% 
  kableExtra::kable_styling()
```

filter for NEON sites that have conifer trees based on field data from all terrestrial NEON sites with qualifying woody vegetation: [https://data.neonscience.org/data-products/DP1.10098.001](https://data.neonscience.org/data-products/DP1.10098.001)

```{r}
conifer_sites <- NeonTreeEvaluation::field %>% 
  dplyr::left_join(
    conifer_spp %>% dplyr::mutate(is_conifer = 1)
    , by = "taxonID"
  ) %>% 
  dplyr::mutate(is_conifer = dplyr::coalesce(is_conifer, 0)) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    tot = dplyr::n()
    , conifer = sum(is_conifer)
    , latitude = mean(plotLatitude)
    , longitude = mean(plotLongitude)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(pct_conifer = conifer/tot)
```

what is the breakdown of woody vegetation sampled in NEON sites by the percent conifer?

```{r}
conifer_sites %>% 
  dplyr::select(-c(longitude,latitude)) %>% 
  dplyr::arrange(desc(pct_conifer), desc(tot)) %>% 
  dplyr::slice_head(n = 19) %>% 
  kableExtra::kbl(
    caption = "Conifers in NEON sites"
    , digits = 2
    , col.names = c(
      "site","total trees","conifer trees", "% conifer"
    )
  ) %>% 
  kableExtra::kable_styling()
```

let's only keep NEON sites with a minimum threshold of the woody vegetation sampled as conifer

```{r}
# minimum pct conifer
min_conifer_pct <- .5
```

now join on our proportion of conifer by NEON site to our lidar data list

```{r}
lidar_df <- lidar_df %>% 
  dplyr::left_join(conifer_sites, by = "siteID") %>% 
  sf::st_as_sf(coords = c("longitude","latitude"), crs = 4326, remove = F)
```

before we filter for conifer sites only, let's check which sites we have lidar data from and whether or not they are classified as a conifer site based on this threshold of `r scales::percent(min_conifer_pct,accuracy=1)`

```{r}
lidar_df %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(pct_conifer = dplyr::first(pct_conifer)) %>% 
  dplyr::mutate(
    is_conifer_site = ifelse(pct_conifer>min_conifer_pct,"conifer site","non-conifer site")
    , pct_conifer = scales::percent(pct_conifer,accuracy=0.1)
  ) %>% 
  mapview::mapview(
    zcol = "is_conifer_site", legend = T
    , layer.name = "conifer site?"
    , col.regions = viridis::viridis(n=2,begin = 0.1,end = 0.9)
  )
```

looks reasonable, let's filter our point cloud processing data for conifer sites based on our proportional threshold of `r scales::percent(min_conifer_pct,accuracy=1)`

```{r}
# data frame of sites
lidar_df <- lidar_df %>%
  dplyr::filter(pct_conifer>min_conifer_pct) %>% 
  # filter out corrupt las files
  # these don't load in lasR...maybe extrabytes issue???
  dplyr::filter(
    !plot_name %in% c(
      "NIWO_005_2018"
      , "SOAP_014_2018"
      , "MOAB_003_2018"
      , "NIWO_009_2018"
      , "TEAK_028_2018"
      , "YELL_058_2020"
      , "RMNP_011_2018"
      , "YELL_030_2018"
      , "YELL_051_2019"
      , "SOAP_014_2019"
      , "TEAK_005_2018"
      , "DSNY_019_2019"
      , "HEAL_004_2019"
      , "SOAP_049_2018"
      , "TEAK_016_2019"
      , "YELL_006_2018"
    )
  ) %>% 
  dplyr::mutate(
    # these plots not corrupt but there are no field trees to evaluate against
    has_field_stems = dplyr::case_when(
      plot_name %in% c(
        "DEJU_021_2018" 
        , "DEJU_021_2019"
        , "JERC_055_2018"
        , "JERC_055_2019"
        , "ABBY_064_2018"
        , "ABBY_064_2019"
      ) ~ F
      , T ~ has_field_stems
    )
  ) %>% 
  dplyr::filter(
    data_type == "training" | has_field_stems | has_image_annotation 
  )
```

what NEON sites have conifers and training/evaluation data?

```{r}
lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    n = dplyr::n()
    , training_plots = sum(ifelse(data_type=="training",1,0))
    , evaluation_plots = sum(ifelse(data_type=="evaluation",1,0))
    , evaluation_ann_plots = sum(ifelse(has_image_annotation & data_type=="evaluation",1,0))
    , evaluation_fld_plots = sum(ifelse(has_field_stems & data_type=="evaluation",1,0))
  ) %>% 
  dplyr::arrange(desc(n)) %>% 
  dplyr::select(-n) %>% 
  kableExtra::kbl(
    caption = "NEON sites with conifers and lidar plots"
    , col.names = c(
      "site","training plots","evaluation plots"
      ,"evaluation plots<br>with ground truth<br>image ann. crowns"
      ,"evaluation plots<br>with ground truth<br>field collected stems"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

note that the sum of the evaluation plots with image annotated crowns and with field collected stems does not necessarily equate to the number of evaluation plots since one plot can have both image annotated and field collected data.

what is the spatial distribution of these conifer sites for which lidar data is available?

```{r}
lidar_df %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    training_plots = sum(ifelse(data_type=="training",1,0))
    , evaluation_plots = sum(ifelse(data_type=="evaluation",1,0))
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    data_type = dplyr::case_when(
      training_plots>0 & evaluation_plots==0 ~ "training"
      , training_plots>0 & evaluation_plots>0 ~ "training+evaluation"
      , training_plots==0 & evaluation_plots>0 ~ "evaluation"
      , T ~ "other"
    )
  ) %>% 
  mapview::mapview(
    zcol = "data_type", legend = T
    , layer.name = "data type"
    , col.regions = viridis::magma(n=2,begin = 0.2,end = 0.8)
  )
```

#### Augment training data{#trn_dta}

While the map above shows that the training and evaluation data are in places that we expect to have conifers, the spatial coverage of training data is limited. This is especially true when one considers that even NEON sites that are close in proximity can have very different forest types. For example, the "TEAK" site east of Fresno, CA has both training and evaluation data and features vegetation described as "dominant tree species include red and white ﬁr (*Abies magniﬁca* and *Abies concolor*), Jeﬀrey pine (*Pinus jeﬀreyi*), and lodgepole pine (*Pinus contorta*)" ([source](https://www.neonscience.org/field-sites/teak)). While the "SOAP" site that is close in proximity and lacks training data has vegetation described as "Ponderosa pine (*Pinus ponderosa*) and incense cedar (*Calocedrus decurrens*) dominate the overstory" ([source](https://www.neonscience.org/field-sites/soap)). 

We will augment the training data by NEON site to validate against the image annotated crown ground truth using `NeonTreeEvaluation::evaluate_image_crowns()` by:

* if a site has image annotated crowns marked as training data, use those plots
* otherwise, if a site has at least 5 image annotated crown evaluation data sets borrow two plots to validate against
* otherwise, if a site has at least 4 image annotated crown evaluation data sets borrow one plot to validate against

We will augment the training data by NEON site to validate against the field collected stems ground truth using `NeonTreeEvaluation::evaluate_field_stems()` by:

* if a site has at least 5 field collected stem validation data sets, borrow two plots to validate against even if the site has image annotated crowns marked as out-of-box training data
* otherwise, if a site has at least 4 field collected stem evaluation data sets, borrow one plot to validate against

We will then use the results of the validation based on the training data to select the best variable window search function based on the precision and recall metrics by NEON site. If a NEON site does not have training data the best overall variable window function will be used.

```{r}
# set seed to replicate results
set.seed(444)
# randomly assign plots as training:
lidar_df <-
  lidar_df %>% 
  dplyr::ungroup() %>% 
  # generate rand
  dplyr::mutate(rand = runif(n=dplyr::n())) %>% 
  # count training/eval by site and flag new training
  dplyr::group_by(siteID) %>% 
  dplyr::mutate(
    training_plots = sum(ifelse(data_type=="training",1,0))
    , evaluation_ann_plots = sum(ifelse(has_image_annotation & data_type=="evaluation",1,0))
    , evaluation_fld_plots = sum(ifelse(has_field_stems & data_type=="evaluation",1,0))
  ) %>% 
  # sample img annotated data
  dplyr::arrange(siteID, has_image_annotation, rand) %>% 
  dplyr::group_by(siteID,has_image_annotation) %>% 
  dplyr::mutate(
    is_new_image_annotation_training = dplyr::case_when(
      training_plots == 0 & 
        evaluation_ann_plots>=5 & 
        has_image_annotation == T &
        dplyr::row_number()<=2 ~ T
      , training_plots == 0 & 
        evaluation_ann_plots>=4 & 
        has_image_annotation == T &
        dplyr::row_number()<=1 ~ T
      , T ~ F
    )
  ) %>% 
  # sample fld stems data
  dplyr::arrange(siteID, has_field_stems, rand) %>% 
  dplyr::group_by(siteID,has_field_stems) %>% 
  dplyr::mutate(
    is_new_field_stems_training = dplyr::case_when(
      evaluation_fld_plots>=5 & 
        has_field_stems == T &
        dplyr::row_number()<=2 ~ T
      , evaluation_fld_plots>=4 & 
        has_field_stems == T &
        dplyr::row_number()<=1 ~ T
      , T ~ F
    )
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(-c(rand,tot,conifer,tidyselect::ends_with("_plots")))

```

check our new table of training/evaluation data by NEON site after we augmented the training data 

```{r}
lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    n = dplyr::n()
    , training_plots = sum(ifelse(data_type=="training",1,0))
    # validation type
    , ann_plots = sum(ifelse(has_image_annotation,1,0))
    , fld_plots = sum(ifelse(has_field_stems,1,0))
    # training/eval
    , training_ann_plots = sum(ifelse(has_image_annotation & (is_new_image_annotation_training | data_type=="training"),1,0))
    , training_fld_plots = sum(ifelse(has_field_stems & (is_new_field_stems_training | data_type=="training"),1,0))
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    # training/eval
    eval_ann_plots = ann_plots-training_ann_plots
    , eval_fld_plots = fld_plots-training_fld_plots
  ) %>% 
  dplyr::arrange(desc(n)) %>% 
  dplyr::select(-c(n,ann_plots,fld_plots)) %>% 
  dplyr::relocate(
    c(tidyselect::ends_with("ann_plots"),tidyselect::ends_with("fld_plots"))
    ,.after = dplyr::last_col()
  ) %>% 
  kableExtra::kbl(
    caption = "NEON sites with conifers and lidar plots using augmented training data"
    , col.names = c(
      "site","original<br>training plots"
      ,"training plots","evaluation plots"
      ,"training plots","evaluation plots"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::add_header_above(
    c(" "=2, "image annotated crowns" = 2, "field collected stems" = 2)
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Height threshold for "canopy" trees{#ht_filter}

We'll process the point cloud and get a tree list using our `cloud2trees::cloud2trees()` method with all default settings except for the ITD window function (see section below). However, an important consideration in the evaluation of our method is that the Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) benchmark was developed specifically for "canopy" trees and the field-collected stems evaluation data only includes >10 cm DBH trees:

>NEON field crews sample all trees within a plot that are greater than 10cm DBH, regardless of whether the tree crown can be seen in the remote sensing image data. While understory tree detection is an important area of future work, the scope of this benchmark is focused on crowns in the canopy that are visible from above. (p. 10)

In order to align our point cloud-based algorithm with the evaluation data inclusion of only "canopy" trees, we'll identify the shortest live tree in the field-collected stems evaluation data by NEON site. We'll use this value to filter our `cloud2trees::cloud2trees()` tree list in post-processing. This filtering process is analogous to setting a diameter threshold during a forest inventory (e.g. >10 cm DBH), meaning only trees with a DBH of 10 cm, for example, or greater are tallied and measured which a common practice because it focuses on trees that are potentially merchantable.

In working with the `NeonTreeEvaluation::field` data we'll use the filters found in `clean_field_data()` from [`NeonTreeEvaluation`](https://github.com/weecology/NeonTreeEvaluation_package/blob/724bdb0a9a3ea6433450f62c69a8fadce9ccdb66/R/evaluate_field_stems.R) as an internal function

```{r}
# set the percentile to determine the shortest live tree in the field-collected stems evaluation data
# using a percentile helps to avoid outlier measurements that might occur if we used a minimum value
percentile_for_ht <- 0.05
# get non-na heights from neon field measured trees
neon_field_heights <-
  NeonTreeEvaluation::field %>% 
  # filters found in `clean_field_data()`
  clean_field_data() %>% 
  dplyr::filter(!is.na(height) & height>3) %>% 
  # and finally, filter for our conifer sites only
  dplyr::inner_join(
    conifer_sites %>% 
      dplyr::filter(pct_conifer>min_conifer_pct)
  )
# get 5th tile ht by site
neon_site_heights <- 
  neon_field_heights %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    site_prcntl_ht = quantile(floor(height), probs = percentile_for_ht, na.rm = T)
  ) %>% 
  dplyr::ungroup()
```

look at the summary of height data across all sites and plots

```{r}
summary(neon_field_heights$height)
```

what does this look like for each NEON site?

```{r}
neon_field_heights %>% 
  dplyr::inner_join(neon_site_heights, by = "siteID") %>% 
  ggplot2::ggplot(ggplot2::aes(x = height, group = siteID)) +
    ggplot2::geom_density(color = "gold",fill = "gold", alpha = 0.7, lwd = 1.2) +
    ggplot2::geom_vline(aes(xintercept = site_prcntl_ht), linetype = "dashed") +
    ggplot2::facet_wrap(facets = dplyr::vars(siteID), ncol = 6, scales = "free") +
    ggplot2::scale_x_continuous(breaks = scales::breaks_extended(6)) +
    ggplot2::labs(
      x = "height (m)", y = ""
      , subtitle = paste0(
        "field-collected heights of \"canopy\" trees in conifer NEON sites with "
        , scales::number(percentile_for_ht*100, accuracy = 1)
        , "th percentile by site"
      )
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.y = ggplot2::element_blank()
      , axis.ticks.y = ggplot2::element_blank()
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    )
```

what is the overall `r scales::number(percentile_for_ht*100, accuracy = 1)`th percentile to use for minimum height?

```{r}
(neon_min_ht <- quantile(neon_field_heights$height, probs = percentile_for_ht) %>% floor())
```

let's attach this to our data frame of lidar data files

```{r}
lidar_df <- lidar_df %>% 
  dplyr::left_join(neon_site_heights, by = "siteID") %>% 
  dplyr::mutate(
   site_prcntl_ht = dplyr::coalesce(site_prcntl_ht,neon_min_ht)
   , plot_name_sans_yr = dplyr::case_when(
     stringr::str_ends(plot_name,"_20\\d{2}$") ~ stringr::str_match(plot_name,"(\\w+)_\\d+")[,2]
     , T ~ plot_name
   )
  ) %>% 
  # create a count by plot name sans year for use later ;/
  dplyr::arrange(plot_name, plot_name_sans_yr, has_field_stems) %>% 
  dplyr::group_by(plot_name_sans_yr, has_field_stems) %>% 
  dplyr::mutate(plot_name_sans_yr_grp = dplyr::row_number()) %>% 
  dplyr::group_by(plot_name_sans_yr) %>% 
  dplyr::mutate(plot_has_multiple_years = max(plot_name_sans_yr_grp)>1) %>% 
  dplyr::ungroup()
# dplyr::glimpse(lidar_df)
# save
readr::write_csv(lidar_df, "../data/NeonTreeEvaluation_lidar_df.csv", append = F, progress = F)
```

### Example validation process{#ex_valid}

now that we have our lidar data list and post-processing height filter that we can test our point cloud-based tree detection and crown segmentation process against, let's walk through the validation for a single point cloud

we'll test with a single evaluation point cloud

```{r}
lidar_df_row <- lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::mutate(
    rn = ifelse(
      data_type=="evaluation" & has_image_annotation & has_field_stems
      ,dplyr::row_number()
      ,NA
  )) %>% 
  dplyr::filter(!is.na(rn)) %>% 
  dplyr::slice(1) %>% 
  dplyr::pull(rn)
# lidar_df$f_path[lidar_df_row] %>% lidR::readLAS() %>% lidR::st_crs()
```

#### View the point cloud

This step isn't necessary for validation, but let's see what this point cloud data looks like. We can plot the point cloud with and color by the point height

```{r,echo=FALSE,message=FALSE,warning=FALSE}
# # ## if want to get current rgl parameters
# par3d()$zoom
# par3d()$FOV
# par3d()$userMatrix %>% c()
# par3d()$windowRect
## set up for printing rgl graphics
r3dDefaults <- rgl::r3dDefaults
m  <- structure(c(
0.978620589,0.029807273,-0.203502536,0.000000000,-0.205546334,0.106896326,-0.972791851,0.000000000,-0.007242568
,0.993823111,0.110737726,0.000000000,0.000000000,0.000000000,0.000000000,1.000000000
), .Dim = c(4L, 4L))
r3dDefaults$FOV <- 30
r3dDefaults$userMatrix <- m
r3dDefaults$zoom <- 0.67
r3dDefaults$windowRect <- c(0,23,1536,864)
rgl::setupKnitr(autoprint = TRUE)
```

```{r, rgl = TRUE, results='asis',fig.show='asis'}
lidar_df$f_path[lidar_df_row] %>% 
  lidR::readLAS() %>% 
  lidR::plot(
    color = "Z", breaks = "quantile", bg = "white", legend = T
   , pal = harrypotter::hp(n=50, house = "gryffindor")
  )
```

there are trees in there for sure (and conifer trees by the looks of it)

let's look at the RGB imagery and image annotated crowns (notice that the `NeonTreeEvaluation` commands rely on the deprecated `raster` package :/ )

```{r}
# read rgb
rgb_temp <- lidar_df$plot_name[lidar_df_row] %>%
  NeonTreeEvaluation::get_data(type = "rgb") %>% 
  raster::stack()

# read image annotated crown data and make polygons
polys_temp <-
  lidar_df$plot_name[lidar_df_row] %>%
  NeonTreeEvaluation::get_data(type = "annotations") %>% 
  NeonTreeEvaluation::xml_parse() 
polys_temp <- NeonTreeEvaluation::boxes_to_spatial_polygons(polys_temp,rgb_temp)

# plot
terra::plotRGB(rgb_temp %>% terra::rast())
terra::plot(
  polys_temp %>% terra::vect()
  , col = NA, border = "red"
  , lwd = 2 , add = TRUE
)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

```{r, include=FALSE, eval=FALSE}
###### this terra + sf doesn't work because NeonTreeEvaluation::xml_parse returns data in local coordinates
###### .... would need to add these to the coordinates in the rgb raster :/

# function to convert numbers to bbox polygon
bbox_poly <- function(xmin,ymin,xmax,ymax) {
  # points
  lower_left_pt <- sf::st_point(c(xmin,ymin))
  upper_right_pt <- sf::st_point(c(xmax,ymax))
  # to bbox polygon
  sf::st_sfc(lower_left_pt, upper_right_pt) %>% 
    sf::st_bbox() %>% 
    sf::st_as_sfc() 
}
# read rgb
rgb_temp <- lidar_df$plot_name[lidar_df_row] %>%
  NeonTreeEvaluation::get_data(type = "rgb") %>% 
  terra::rast()

# read image annotated crown data and make polygons
# polys_temp <- 
  lidar_df$plot_name[lidar_df_row] %>%
  NeonTreeEvaluation::get_data(type = "annotations") %>% 
  NeonTreeEvaluation::xml_parse() %>% 
  dplyr::mutate(id = dplyr::row_number()) %>% 
  dplyr::select(id,xmin,ymin,xmax,ymax) %>% 
  dplyr::rowwise(id) %>% 
  dplyr::mutate(geometry = bbox_poly(xmin,ymin,xmax,ymax)) %>% 
  dplyr::ungroup() %>% 
  sf::st_set_geometry("geometry") %>% 
  sf::st_set_crs(xxxxxx)
# plot
terra::plotRGB(rgb_temp, axes = T)
terra::plot(
  polys_temp %>% terra::vect() %>% terra::set.crs(terra::crs(rgb_temp))
  , border = "red"
  , add = TRUE
)
```

#### `cloud2trees::cloud2trees()`

for our example, we'll use all default settings in `cloud2trees::cloud2trees()` to get a point cloud-detected tree list

```{r, warning=FALSE, message=FALSE, results=F}
ans <- cloud2trees::cloud2trees(
  input_las_dir = lidar_df$f_path[lidar_df_row]
  , output_dir = tempdir()
)
# filter the tree list based on the height threshold
ans$crowns_sf <- ans$crowns_sf %>% 
  dplyr::filter(tree_height_m >= lidar_df$site_prcntl_ht[lidar_df_row])
```

quick check of our heights

```{r}
ans$crowns_sf$tree_height_m %>% summary()
```

#### Format extracted tree polygons

we need to format our extracted trees for `NeonTreeEvaluation` evaluation and submission

first, we'll simplify multipolygon crowns

```{r}
ans$crowns_sf <- cloud2trees::simplify_multipolygon_crowns(ans$crowns_sf)
```

check out our extracted trees

```{r}
ggplot2::ggplot() +
  ggplot2::geom_tile(
    data = ans$chm_rast %>%
      terra::as.data.frame(xy=T) %>%
      dplyr::rename(f=3)
    , mapping = ggplot2::aes(x = x, y = y, fill = f)
    , na.rm = T
  ) +
  harrypotter::scale_fill_hp(
    option = "gryffindor"
    , breaks = scales::breaks_extended(n=10)
  ) +
  ggplot2::geom_sf(
    data = ans$crowns_sf
    , fill = NA, color = "gray44", lwd = 1.2
  ) +
  ggplot2::scale_x_continuous(expand = c(0, 0)) +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "", y = "", fill = "CHM (m)") +
  ggplot2::theme_light() +
  ggplot2::theme(axis.text = ggplot2::element_blank())
```

we'll reserve judgement and let the data talk

format the data for `NeonTreeEvaluation` submission and evaluation

```{r}
return_sf <-
  ans$crowns_sf %>% 
  sf::st_set_geometry("geometry") %>% 
  dplyr::rowwise("treeID") %>% 
  dplyr::mutate(
    xmin = sf::st_bbox(geometry)[1]
    , ymin = sf::st_bbox(geometry)[2]
    , xmax = sf::st_bbox(geometry)[3]
    , ymax = sf::st_bbox(geometry)[4]
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    label = "Tree"
    , plot_name = lidar_df$plot_name[lidar_df_row]
  ) %>% 
  dplyr::select(xmin,xmax,ymin,ymax,label,plot_name) %>% 
  sf::st_set_crs(NA)
# what?
return_sf %>% dplyr::glimpse()
```

does this match the submission polygon data from the `NeonTreeEvaluation` package?

```{r}
NeonTreeEvaluation::submission_polygons %>% dplyr::glimpse()
```

yes, except for the "score" column which I'm pretty sure is an artifact from after evaluation?

#### Test evaluation

We compared our tree detection and crown delineation results to the three types of evaluation data (i.e. “ground truth” data) presented by Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)): field-collected stems, image-annotated crowns, and field-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective, and field-annotated crowns combine the benefits of both but are highly resource-intensive.

##### Scores for image-annotated crowns

The main data source are image-annotated crowns, in which a single observer annotated visible trees in 200 40m x 40m images from across the United States. Get the benchmark score image-annotated "ground truth" data.

*in testing, including the `sf` polygon data did not work...switching to the bbox method with `sf::st_drop_geometry()`*

```{r}
rslt_img_annttd_crwns <- NeonTreeEvaluation::evaluate_image_crowns(
  predictions = return_sf %>% sf::st_drop_geometry()
  , show = T
  , summarize = T
)
```

in the plot (if there is a plot), "red" boxes are crowns our point cloud-based method extracted and "black" are the image annotated crowns

it looks like the overlay is generally the same but we are still extracting trees that may not be considered "canopy" trees ;\

what is in the return from `NeonTreeEvaluation::evaluate_image_crowns()` ?

```{r}
rslt_img_annttd_crwns %>% names()
```

`overall`: must be across all NEON sites, plots, and trees included for evaluation

```{r}
rslt_img_annttd_crwns$overall
```

`by_site`: must be across plots, and trees included for evaluation in a NEON sites

```{r}
rslt_img_annttd_crwns$by_site
```

`plot_level`: must be across trees included for evaluation in a NEON site, plot combination

```{r}
rslt_img_annttd_crwns$plot_level
```

count_error is the number of predicted trees minus the number of "ground truth" trees but in graphical form so we'll skip it

##### Scores for field-annotated crowns

The second data source is a small number of field-annotated crowns from two geographic sites. These crowns were drawn on a tablet while physically standing in the field, thereby reducing the uncertainty in crown segmentation.

*not all plots have field-annotated crowns* 

`NeonTreeEvaluation::evaluate_field_crowns()` returns an error if "No plot names matching the field crown data, see list_field_crowns for paths to RGB field crown imagery."... so, we'll have to capture errors in our checks?

```{r}
# safe it
safe_evaluate_field_crowns <- purrr::safely(NeonTreeEvaluation::evaluate_field_crowns)
# test it
rslt_fld_crwns <- safe_evaluate_field_crowns(
  predictions = return_sf %>% sf::st_drop_geometry()
  , show = T
  , summarize = T
)
```

did it do it?

```{r}
rslt_fld_crwns$error
```

nope!

##### Scores for field-collected stems

The third data source is the NEON Woody Vegetation Structure Dataset. Each tree stem is represented by a single point. This data has been filtered to represent overstory trees visible in the remote sensing imagery.

*not all plots have field-collected stems* 

`NeonTreeEvaluation::evaluate_field_stems()` returns an error if "No submitted plot_names with matching field stem data, see list_field_stems()"... so, we'll have to capture errors in our checks?

```{r}
# safe it
safe_evaluate_field_stems <- purrr::safely(NeonTreeEvaluation::evaluate_field_stems)
# test it
rslt_fld_stems <- safe_evaluate_field_stems(
  predictions = return_sf %>% sf::st_drop_geometry()
  , show = T
  , summarize = T
)
```

did it do it?

```{r}
rslt_fld_stems$error
```

nope!

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(ans, lidar_df_row, min_conifer_pct)
remove(rslt_img_annttd_crwns,rslt_fld_crwns,rslt_fld_stems,conifer_sites,conifer_spp,neon_field_heights,neon_site_heights,return_sf)
gc()
```

### Training validation process

Now that we have been through the validation process for a [single sample plot](#ex_valid) and [defined different variable window functions](#itd_more) for use in ITD, we'll create a function to detect trees and delineate tree crowns using our `cloud2trees::cloud2trees()` method to apply over each lidar data set.

We'll extract trees on each training data set using all `r length(my_ws_functions)` variable window functions, compare our point cloud-detected trees to the image annotated crowns (i.e. ground truth), and store the best variable window function by NEON site for use in validation on the evaluation lidar data. If a NEON site does not have training data, the best variable window function across NEON sites will be used.

#### Summary of training lidar data

Let's get a quick summary of the lidar data available for training. Note, we expect our augmented training data sets to be sized around 40m x 40m while the original training data sets will cover a larger area.

```{r}
training_lidar_sum_temp <-
  lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::filter(data_type=="training" | is_new_image_annotation_training | is_new_field_stems_training) %>% 
  dplyr::pull(f_path) %>% 
  purrr::map(function(x){
    ctg <- lidR::readLAScatalog(x)
    ctg@data %>% 
      dplyr::summarise(
        num_points = sum(Number.of.point.records)
        , area_m2 = sum(as.numeric(sf::st_area(.)))
      ) %>% 
      dplyr::mutate(points_per_m2 = num_points/area_m2) %>% 
      sf::st_drop_geometry() %>% 
      dplyr::mutate(
        plot_name = x %>% basename() %>% stringr::str_remove_all("\\.(laz|las)$")    
      ) %>% 
      dplyr::relocate(plot_name)
  }) %>% 
  dplyr::bind_rows()
# check it
training_lidar_sum_temp %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::select(plot_name,siteID) 
    , by = "plot_name"
  ) %>% 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric),~scales::comma(.x,accuracy=.1))) %>% 
  dplyr::relocate(siteID) %>% 
  dplyr::arrange(siteID,plot_name) %>% 
  kableExtra::kbl(
    caption = "summary of training lidar data"
    , digits = 1
    , col.names = c(
      "site","plot","# points","area m<sup>2</sup>", "points<br>per m<sup>2</sup>"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "622px",fixed_thead = T) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")
```

note the out-of-box training plots are much larger -- that's why we didn't augment the training set for these sites -- and check out the spread in point density

do the 2019 "OSBS" point clouds overlap?

```{r}
ggplot2::ggplot() +
  ggplot2::geom_sf(
    data = lidR::readLAScatalog(
        lidar_df %>% 
          dplyr::filter(plot_name == "2019_OSBS_5_405000_3287000_image_crop") %>% 
          dplyr::pull(f_path)
      ) %>% 
      purrr::pluck("data") %>% 
      dplyr::mutate(plot_name = basename(filename) %>% stringr::str_remove_all("\\.(laz|las)$"))
    , ggplot2::aes(color = plot_name)
    , fill = NA, lwd = 2
  ) +
  ggplot2::geom_sf(
    data = lidR::readLAScatalog(
        lidar_df %>% 
          dplyr::filter(plot_name == "2019_OSBS_5_405000_3287000_image_crop2") %>% 
          dplyr::pull(f_path)
      ) %>% 
      purrr::pluck("data") %>% 
      dplyr::mutate(plot_name = basename(filename) %>% stringr::str_remove_all("\\.(laz|las)$"))
    , ggplot2::aes(color = plot_name)
    , fill = NA, lwd = 2
  ) +
  ggplot2::scale_color_manual(values = c("navy","gold")) +
  ggplot2::labs(color="") +
  ggplot2::theme_light() +
  ggplot2::theme(legend.position = "top",axis.text = ggplot2::element_text(size=6)) +
  ggplot2::guides(col = ggplot2::guide_legend(ncol = 1))

```

no

let's look at the RGB imagery and image annotated crowns (notice that the `NeonTreeEvaluation` commands rely on the deprecated `raster` package :\)

```{r}
neontree_plot_fn <- function(plot_name,lwd=2) {
  # read rgb
  rgb_temp <- 
    NeonTreeEvaluation::get_data(plot_name = plot_name, type = "rgb") %>% 
    raster::stack()
  
  # read image annotated crown data and make polygons
  polys_temp <-
    NeonTreeEvaluation::get_data(plot_name = plot_name, type = "annotations") %>% 
    NeonTreeEvaluation::xml_parse() 
  polys_temp <- NeonTreeEvaluation::boxes_to_spatial_polygons(polys_temp,rgb_temp)
  
  # plot
  terra::plotRGB(rgb_temp %>% terra::rast(), main = plot_name)
  terra::plot(
    polys_temp %>% terra::vect()
    , col = NA, border = "red"
    , lwd = lwd , add = TRUE
  )
}
# 2019_OSBS_5_405000_3287000_image_crop
neontree_plot_fn("2019_OSBS_5_405000_3287000_image_crop")
# 2019_OSBS_5_405000_3287000_image_crop2
neontree_plot_fn("2019_OSBS_5_405000_3287000_image_crop2")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Function to extract trees{#xtract}

let's create a function to extract trees and format for evaluation using our `lidar_df` data which includes a data frame of file paths with the appropriate plot name

```{r}
cloud2trees_for_eval <- function(lidar_df_row, lidar_df, ws) {
  # message
  message(paste0("doing the work for ...... ", lidar_df$plot_name[lidar_df_row]))
  # run c2t
  qc2t <- purrr::safely(cloud2trees::cloud2trees)
  ans <- qc2t(
    input_las_dir = lidar_df$f_path[lidar_df_row]
    , output_dir = tempdir()
    , ws = ws
  )
  if(!is.null(ans$error)){return(NULL)}
  ans <- ans$result
  # simp
  ans$crowns_sf <- cloud2trees::simplify_multipolygon_crowns(ans$crowns_sf)
  # return
  return_sf <-
    ans$crowns_sf %>% 
    sf::st_set_geometry("geometry") %>% 
    dplyr::rowwise("treeID") %>% 
    dplyr::mutate(
      xmin = sf::st_bbox(geometry)[1]
      , ymin = sf::st_bbox(geometry)[2]
      , xmax = sf::st_bbox(geometry)[3]
      , ymax = sf::st_bbox(geometry)[4]
    ) %>% 
    dplyr::ungroup() %>% 
    dplyr::mutate(
      label = "Tree"
      , plot_name = lidar_df$plot_name[lidar_df_row]
    ) %>% 
    dplyr::select(xmin,xmax,ymin,ymax,label,plot_name,tree_height_m) %>% 
    sf::st_set_crs(NA)
  # give the workers a rest
  Sys.sleep(2)
  return(return_sf)
}
# function to apply cloud2trees_for_eval over each ws
ws_fn_eval <- function(lidar_df, lidar_df_row, ws_fn_list) {
  1:length(ws_fn_list) %>% 
    purrr::map(function(x){
      ans <- cloud2trees_for_eval(
        lidar_df_row = lidar_df_row
        , lidar_df = lidar_df
        , ws = ws_fn_list[[x]]
      )
      ### some of these ws functions may not extract any trees
      ### account for this
      if(is.null(ans) || !inherits(ans,"data.frame")){
        return(NULL)
      }else{
        return(
          ans %>% 
            dplyr::mutate(ws_fn_nm = ws_fn_list[x] %>% names() %>% as.character())
        )
      }
    }) %>% 
    dplyr::bind_rows()
}
```

#### Extract trees for training data

Now we'll extract trees from the training lidar data using each variable window function

```{r, results=FALSE, warning=FALSE, message=FALSE}
# where should we save the file?
submission_fn <- "../data/NeonTreeEvaluation_training_submission.gpkg"
# if we don't already have the data, run it
if(!file.exists(submission_fn)){
# if(T){ ### !!!!!!! take out when it gets real
  # just get training
  training_df_temp <- lidar_df %>% 
    dplyr::filter(data_type == "training" | is_new_image_annotation_training | is_new_field_stems_training)
  # get trees for each training plot and ws fn combination
  training_submission <- 
    1:nrow(training_df_temp) %>% 
    purrr::map(\(x) 
      ws_fn_eval(
        lidar_df = training_df_temp 
        , lidar_df_row = x
        , ws_fn_list = my_ws_functions
      )
    ) %>% 
    dplyr::bind_rows()
  # save it 
  sf::st_write(training_submission, submission_fn, append = F)
}else{
  training_submission <- sf::st_read(submission_fn, quiet = T)
}
```

what did we get?

```{r}
training_submission %>% dplyr::glimpse()
```

now we need to filter our tree list based on the [height threshold for "canopy" trees](#ht_filter)

```{r}
training_submission <-
  training_submission %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(
        data_type=="training" | is_new_image_annotation_training | is_new_field_stems_training
      ) %>% 
      dplyr::select(plot_name,site_prcntl_ht)
    , by = "plot_name"
  ) %>% 
  dplyr::filter(tree_height_m>=site_prcntl_ht) %>% 
  dplyr::select(-c(tree_height_m,site_prcntl_ht))
```

```{r, include=FALSE, eval=FALSE}
training_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::count(plot_name,ws_fn_nm) %>% 
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric),~scales::comma(.x,accuracy=1))) %>% 
  kableExtra::kbl(
    caption = "Count of lidar-detected trees by plot name and variable window function"
    , digits = 0
    , col.names = c(
      "plot"
      , "variable<br>window<br>function"
      , "# trees"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")
```

tabulate detected "canopy" trees by plot and variable window function and plot it

```{r}
training_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::count(plot_name,ws_fn_nm) %>% 
  ggplot2::ggplot(ggplot2::aes(x = n, y = plot_name, color = ws_fn_nm)) +
    ggplot2::geom_point(size = 5) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::scale_x_log10(labels=scales::comma) +
    ggplot2::labs(
      x = "# trees", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Training data: number of detected canopy trees"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.y = ggplot2::element_text(size=8)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Training: validation against ground truth

We'll compare our aerial point cloud-based tree detection and crown delineation results to two types of evaluation data (i.e. “ground truth” data) presented by Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)): field-collected stems and image-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective.

Precision quantifies the proportion of correctly identified tree crowns among all detected crowns (how many of the trees it found were real trees). Recall measures the proportion of correctly identified crowns relative to the total image-annotated tree crowns (how many of the actual trees it managed to find).

`NeonTreeEvaluation::evaluate_image_crowns()` is used to evaluate our data against the image-annotated crowns ground truth. This function returns precision and recall per image (i.e. "plot"). `NeonTreeEvaluation::evaluate_field_stems()` is used to evaluate our data against the field collected stems ground truth. This function returns recall -- the proportion of field stems that fall within a single predicted crown -- per image (i.e. "plot"). We will apply both evaluation methods over each variable window function tested for each plot marked as training data.

##### Scores for image-annotated crowns

Get the benchmark score compared to image-annotated "ground truth" data.

*in testing, including the `sf` polygon data did not work...switching to the bbox method with `sf::st_drop_geometry()`*

```{r, include=FALSE,eval=FALSE}
NeonTreeEvaluation::evaluate_image_crowns(
    predictions = training_submission %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(ws_fn_nm == "log_fn") %>% 
      dplyr::filter(plot_name=="YELL_048_2018") %>% 
      dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
    , show = F
    , summarize = T
  )

NeonTreeEvaluation::image_crowns(
  predictions = training_submission %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(ws_fn_nm == "log_fn") %>% 
      dplyr::filter(plot_name=="YELL_048_2018") %>% 
      dplyr::select(xmin,ymin,xmax,ymax,label,plot_name) %>% 
    dplyr::group_by(plot_name)
  , project_boxes = F
  , show = F
)

NeonTreeEvaluation::load_ground_truth(plot_name="YELL_048_2018", show = FALSE)

# Load xml of annotations
  (siteID <- stringr::str_match("YELL_048_2018", "(\\w+)_")[, 2])

  (path_to_xml <- NeonTreeEvaluation::get_data("YELL_048_2018","annotations"))

  if (!file.exists(path_to_xml)) {
    message(paste("There are no annotations for file", path_to_xml, "skipping..."))
    return(NULL)
  }
  xmls <- xml_parse(path_to_xml)

  # load rgb
  path_to_rgb <- get_data("YELL_048_2018","rgb")

  if (file.exists(path_to_rgb)) {
    rgb <- raster::stack(path_to_rgb)
  } else {
    warning(path_to_rgb, "does not exist in current benchmark, skipping")
    return(NULL)
  }

  # Read RGB image as projected raster

  # View one plot's annotations as polygons, project into UTM
  # copy project utm zone (epsg), xml has no native projection metadata
  ground_truth <- boxes_to_spatial_polygons(boxes = xmls, raster_object = rgb, project_boxes = TRUE)

  if (show) {
    raster::plotRGB(rgb)
    raster::plot(ground_truth, add = TRUE)
  }
  return(ground_truth)

```

```{r, results=F, message=FALSE}
# where should we save the file?
training_eval_temp <- "../data/NeonTreeEvaluation_training_evaluate_image_crowns.csv"
# if we don't already have the data, run it
if(!file.exists(training_eval_temp)){
# if(T){
  # filter for only those with this type of validation data
  submission_temp <- training_submission %>% 
    ## !!!!!!!!!!!! do these separate b/c memory crash :/
    ## hate that this is manual but i'm.....tired
    dplyr::filter(
      !plot_name %in% c("2018_NIWO_2_450000_4426000_image_crop","2018_TEAK_3_315000_4094000_image_crop")
    ) %>% 
    dplyr::inner_join(
      lidar_df %>% 
        sf::st_drop_geometry() %>% 
        dplyr::filter(has_image_annotation==T & (data_type == "training" | is_new_image_annotation_training)) %>% 
        dplyr::distinct(plot_name)
      , by = "plot_name"
    )
  
  # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported
  # would need to dig through NeonTreeEvaluation, fix, and pull request...
  safe_evaluate_image_crowns <- purrr::safely(NeonTreeEvaluation::evaluate_image_crowns)
  # validate
  training_img_annttd_crwns <-
    submission_temp$ws_fn_nm %>% 
    unique() %>% 
    purrr::map(function(x){
      ans <- safe_evaluate_image_crowns(
        predictions = submission_temp %>% 
          sf::st_drop_geometry() %>% 
          dplyr::filter(ws_fn_nm == x) %>% 
          dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
        , show = F
        , summarize = T
      )
      if(!is.null(ans$error)){
        return(NULL)
      }else{
        return(
          ans$result %>% 
            purrr::pluck("plot_level") %>% 
            dplyr::ungroup() %>% 
            dplyr::mutate(ws_fn_nm = x)
        ) 
      }
    }) %>% 
    dplyr::bind_rows()
  # save
  readr::write_csv(training_img_annttd_crwns, file = training_eval_temp)
  #################################
  # now for the big ones so we don't crash memory
  ## hate that this is manual but i'm.....tired
  #################################
  # filter for only those with this type of validation data
  submission_temp <- training_submission %>% 
    ## !!!!!!!!!!!! do these separate
    dplyr::filter(plot_name %in% c("2018_NIWO_2_450000_4426000_image_crop")) %>% 
    dplyr::inner_join(
      lidar_df %>% 
        sf::st_drop_geometry() %>% 
        dplyr::filter(has_image_annotation==T) %>% 
        dplyr::distinct(plot_name)
      , by = "plot_name"
    )
  # validate
  training_img_annttd_crwns_temp <-
    submission_temp$ws_fn_nm %>% 
    unique() %>% 
    purrr::map(function(x){
      ans <- safe_evaluate_image_crowns(
        predictions = submission_temp %>% 
          sf::st_drop_geometry() %>% 
          dplyr::filter(ws_fn_nm == x) %>% 
          dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
        , show = F
        , summarize = T
      )
      if(!is.null(ans$error)){
        return(NULL)
      }else{
        return(
          ans$result %>% 
            purrr::pluck("plot_level") %>% 
            dplyr::ungroup() %>% 
            dplyr::mutate(ws_fn_nm = x)
        ) 
      }
    }) %>% 
    dplyr::bind_rows()
  # add it
  training_img_annttd_crwns <- training_img_annttd_crwns %>% 
    dplyr::bind_rows(training_img_annttd_crwns_temp)
  # save
  readr::write_csv(training_img_annttd_crwns, file = training_eval_temp)
  #################################
  # now for the big ones so we don't crash memory
  ## hate that this is manual but i'm.....tired
  #################################
  # filter for only those with this type of validation data
  submission_temp <- training_submission %>% 
    ## !!!!!!!!!!!! do these separate
    dplyr::filter(plot_name %in% c("2018_TEAK_3_315000_4094000_image_crop")) %>% 
    dplyr::inner_join(
      lidar_df %>% 
        sf::st_drop_geometry() %>% 
        dplyr::filter(has_image_annotation==T) %>% 
        dplyr::distinct(plot_name)
      , by = "plot_name"
    )
  # validate
  training_img_annttd_crwns_temp <-
    submission_temp$ws_fn_nm %>% 
    unique() %>% 
    purrr::map(function(x){
      ans <- safe_evaluate_image_crowns(
        predictions = submission_temp %>% 
          sf::st_drop_geometry() %>% 
          dplyr::filter(ws_fn_nm == x) %>% 
          dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
        , show = F
        , summarize = T
      )
      if(!is.null(ans$error)){
        return(NULL)
      }else{
        return(
          ans$result %>% 
            purrr::pluck("plot_level") %>% 
            dplyr::ungroup() %>% 
            dplyr::mutate(ws_fn_nm = x)
        ) 
      }
    }) %>% 
    dplyr::bind_rows()
  # add it
  training_img_annttd_crwns <- training_img_annttd_crwns %>% 
    dplyr::bind_rows(training_img_annttd_crwns_temp)
  # save
  readr::write_csv(training_img_annttd_crwns, file = training_eval_temp)
}else{
  training_img_annttd_crwns <- readr::read_csv(training_eval_temp, progress = F, show_col_types = F)
}
# training_img_annttd_crwns %>% dplyr::glimpse()
```

our return data should include precision and recall values for each plot and variable window function that resulted in trees extracted (some window functions might not have extracted any trees) with image annotated crown evaluation data...unless the `NeonTreeEvaluation::evaluate_image_crowns()` resulted in an error

count of the variable window functions which resulted in trees extracted and were successfully evaluated using image annotated crown ground truth data by plot

```{r}
training_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(
        has_image_annotation==T & 
        (data_type=="training" | is_new_image_annotation_training)
      ) %>% 
      dplyr::distinct(plot_name)
    , by = "plot_name"
  ) %>% 
  dplyr::count(plot_name,ws_fn_nm) %>% 
  dplyr::left_join(
    training_img_annttd_crwns
    , by = dplyr::join_by(plot_name,ws_fn_nm)
  ) %>% 
  dplyr::group_by(plot_name) %>% 
  dplyr::summarize(ws_trees = dplyr::n(), ws_eval = sum(ifelse(is.na(precision),0,1))) %>% 
  dplyr::arrange(plot_name) %>% 
  kableExtra::kbl(
    caption = "Count of variable window functions tested and successfully evaluated using image annotated crowns"
    , digits = 0
    , col.names = c(
      "plot","window functions<br>with trees","window functions<br>successfully evaluated"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

plot precision and recall by plot

```{r}
training_img_annttd_crwns %>% 
  tidyr::pivot_longer(cols = c(precision,recall)) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = plot_name, color = ws_fn_nm)) +
    # ggplot2::geom_point(size = 4) +
    ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::labs(
      x = "", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Training data: validation against image annotated crowns ground truth"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.y = ggplot2::element_text(size=8)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    )
```

we'll quickly compare the variable window functions by averaging across sites and making a "quadrant" plot whereby we overlay the median value to differentiate the "best" window size by the two dimensions (precision, recall)...the upper-right quadrant is the "best" of the variable window functions tested based on these training plots evaluated against image annotated crowns

```{r}
agg_pr_temp <- training_img_annttd_crwns %>% 
  dplyr::group_by(ws_fn_nm) %>% 
  dplyr::summarise(dplyr::across(dplyr::where(is.numeric),mean))
agg_pr_temp %>% 
  ggplot2::ggplot(ggplot2::aes(x=precision,y=recall,color=ws_fn_nm)) +
  ggplot2::geom_hline(yintercept = median(agg_pr_temp$recall),linetype="dashed") +
  ggplot2::geom_vline(xintercept = median(agg_pr_temp$precision),linetype="dashed") +
  ggplot2::geom_point(size = 6) +
  ggplot2::scale_color_manual(values = pal_ws) +
  ggplot2::scale_x_continuous(limits = c(0,0.5)) +
  ggplot2::scale_y_continuous(limits = c(0,0.5)) +
  ggplot2::labs(
    color = "variable\nwindow\nfunction"
    , subtitle = "Training data: validation against image annotated crowns ground truth"
    , caption = "*values are averaged across sites and plots"
  ) +
  ggplot2::theme_light() +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

##### Scores for field-collected stems

```{r, include=FALSE, eval=FALSE}
xxx <- NeonTreeEvaluation::evaluate_field_stems(
  training_submission %>% 
    dplyr::inner_join(
      lidar_df %>% 
        sf::st_drop_geometry() %>% 
        dplyr::filter(has_field_stems==T & is_new_field_stems_training) %>% 
        dplyr::distinct(plot_name)
      , by = "plot_name"
    ) %>% 
    dplyr::filter(plot_name=="ABBY_073_2018" & ws_fn_nm == "lin_fn") %>% 
    sf::st_drop_geometry() %>% 
    dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
  , show = T
  , summarize = T
)
xxx
```

```{r, results=F, message=FALSE}
# where should we save the file?
training_eval_temp <- "../data/NeonTreeEvaluation_training_evaluate_field_stems.csv"
# if we don't already have the data, run it
if(!file.exists(training_eval_temp)){
# if(T){
  # filter for only those with this type of validation data
  submission_temp <- training_submission %>% 
    dplyr::inner_join(
      lidar_df %>% 
        sf::st_drop_geometry() %>% 
        dplyr::filter(has_field_stems==T & (data_type == "training" | is_new_field_stems_training)) %>% 
        dplyr::distinct(plot_name)
      , by = "plot_name"
    )
  # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported
  # would need to dig through NeonTreeEvaluation, fix, and pull request...
  safe_evaluate_field_stems <- purrr::safely(NeonTreeEvaluation::evaluate_field_stems)
  # validate
  training_fld_cllctd_stems <-
    submission_temp$ws_fn_nm %>% 
    unique() %>% 
    purrr::map(function(x){
      ans <- safe_evaluate_field_stems(
        predictions = submission_temp %>% 
          # sf::st_drop_geometry() %>% ### polygons work here !
          dplyr::filter(ws_fn_nm == x) %>% 
          dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
        , show = F
        , summarize = T
      )
      if(!is.null(ans$error)){
        return(NULL)
      }else{
        return(
          ans$result %>% 
            purrr::pluck("plot_level") %>% 
            dplyr::ungroup() %>% 
            dplyr::mutate(ws_fn_nm = x)
        ) 
      }
    }) %>% 
    dplyr::bind_rows()
  # save
  readr::write_csv(training_fld_cllctd_stems, file = training_eval_temp)
}else{
  training_fld_cllctd_stems <- readr::read_csv(training_eval_temp, progress = F, show_col_types = F)
}
# training_fld_cllctd_stems %>% dplyr::glimpse()
```

our return data should include recall values for each plot and variable window function that resulted in trees extracted (some window functions might not have extracted any trees) with image annotated crown evaluation data...unless the `NeonTreeEvaluation::evaluate_field_stems()` resulted in an error

count of the variable window functions which resulted in trees extracted and were successfully evaluated using image annotated crown ground truth data by plot

```{r}
training_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(
        has_field_stems==T & 
        (data_type=="training" | is_new_field_stems_training)
      ) %>% 
      dplyr::distinct(plot_name,plot_name_sans_yr)
    , by = "plot_name"
  ) %>% 
  dplyr::count(plot_name,plot_name_sans_yr,ws_fn_nm) %>% 
  dplyr::left_join(
    training_fld_cllctd_stems %>% 
      dplyr::select(plot_name,ws_fn_nm,recall)
    , by = dplyr::join_by(plot_name_sans_yr==plot_name,ws_fn_nm)
  ) %>% 
  dplyr::group_by(plot_name) %>% 
  dplyr::summarize(ws_trees = dplyr::n(), ws_eval = sum(ifelse(is.na(recall),0,1))) %>% 
  dplyr::arrange(plot_name) %>% 
  kableExtra::kbl(
    caption = "Count of variable window functions tested and successfully evaluated using field collected stems"
    , digits = 0
    , col.names = c(
      "plot","window functions<br>with trees","window functions<br>successfully evaluated"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

plot recall by plot

```{r}
training_fld_cllctd_stems %>% 
  tidyr::pivot_longer(cols = c(n,recall)) %>% 
  dplyr::filter(name=="recall") %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = plot_name, color = ws_fn_nm)) +
    # ggplot2::geom_point(size = 4) +
    ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::labs(
      x = "", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Training data: validation against field collected stems ground truth"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Best variable window function

We are going to use the training data to determine the "best" ITD window function to use for a given NEON site since the `cloud2trees` package [strongly recommends](https://github.com/georgewoolsey/cloud2trees?tab=readme-ov-file#individual-tree-detection-itd-tuning) that "a different window size function is defined for each region of your study area with significantly different forest structure". We'll get the precision and recall for each plot and variable window function combination and then select the window function that results in the best tree detection by NEON site. We'll also average across sites to select the best variable window function overall which we'll use for NEON sites that did not have training data.

We will determine the "best" variable window function by calculating the F-score which incorporates true positive, commission, and omission rates to determine how well the aerial point cloud-detected trees match the ground truth trees. As a measure of predictive performance, the highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero.

$$
\textrm{F-score} = 2 \times \frac{\bigl(precision \times recall \bigr)}{\bigl(precision + recall \bigr)}
$$

let's combine our evaluation results, collapse across plot to calculate an average precision and recall for each NEON site and variable window function combination, calculate F-score, choose the highest F-score acheived by a variable window function for each NEON site and overall by averaging over each site.

```{r}
# combine eval results
training_eval_combined_temp <- training_img_annttd_crwns %>% 
  dplyr::inner_join(
    lidar_df %>% sf::st_drop_geometry() %>% dplyr::distinct(plot_name,siteID)
    , by = "plot_name"
  ) %>% 
  dplyr::select(siteID,plot_name,ws_fn_nm,recall,precision) %>% 
  dplyr::mutate(eval_type = "img_annttd_crwns") %>% 
  dplyr::bind_rows(
    training_fld_cllctd_stems %>% 
      dplyr::select(siteID,plot_name,ws_fn_nm,recall) %>% 
      dplyr::mutate(eval_type = "fld_cllctd_stems")
  )
# overall eval results
training_eval_overall <-
  training_eval_combined_temp %>% 
  dplyr::group_by(ws_fn_nm) %>% 
  dplyr::summarise(
    precision = mean(precision, na.rm = T)
    , recall = mean(recall, na.rm = T)
    , n_fld_cllctd_stems = sum(ifelse(eval_type == "fld_cllctd_stems",1,0))
    , n_img_annttd_crwns = sum(ifelse(eval_type == "img_annttd_crwns",1,0))
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    dplyr::across(.cols = c(precision,recall), ~ifelse(is.nan(.x),as.numeric(NA),.x))
    , f_score = dplyr::case_when(
      is.na(precision) | is.na(recall) ~ as.numeric(NA)
      , (precision+recall) == 0 ~ 0
      , T ~ 2 * ( (precision*recall)/(precision+recall) )
    )
  ) %>% 
  dplyr::arrange(desc(f_score),desc(precision),desc(recall),desc(n_fld_cllctd_stems),desc(n_img_annttd_crwns)) %>% 
  dplyr::mutate(is_best_fn = dplyr::row_number()==1)
# save it
readr::write_csv(training_eval_overall, "../data/NeonTreeEvaluation_training_eval_overall.csv", append = F, progress = F)
# site eval results
training_eval_by_site <-
  training_eval_combined_temp %>% 
  dplyr::group_by(siteID,ws_fn_nm) %>% 
  dplyr::summarise(
    precision = mean(precision, na.rm = T)
    , recall = mean(recall, na.rm = T)
    , n_fld_cllctd_stems = sum(ifelse(eval_type == "fld_cllctd_stems",1,0))
    , n_img_annttd_crwns = sum(ifelse(eval_type == "img_annttd_crwns",1,0))
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    dplyr::across(.cols = c(precision,recall), ~ifelse(is.nan(.x),as.numeric(NA),.x))
    , f_score = dplyr::case_when(
      is.na(precision) | is.na(recall) ~ as.numeric(NA)
      , (precision+recall) == 0 ~ 0
      , T ~ 2 * ( (precision*recall)/(precision+recall) )
    )
  )
# save it after cleaning
```

##### Best variable window function by NEON site

we collapsed across plot to calculate an average precision and recall for each NEON site and variable window function combination

```{r}
training_eval_by_site %>% 
  tidyr::pivot_longer(cols = c(precision,recall,f_score)) %>% 
  dplyr::mutate(
    name = ifelse(name=="f_score","F-score",name) %>% forcats::fct_rev()
    , siteID = forcats::fct_rev(siteID)
  ) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = ws_fn_nm)) +
    # ggplot2::geom_point(size = 4) +
    ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::labs(
      x = "", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Training data: overall validation against ground truth by NEON site"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.x = ggplot2::element_text(size=7)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    )
```

Note that not every NEON site has a precision value filled (and no F-score). These NEON sites [did not have enough data](#trn_dta) to use for training data to be evaluated against the image annotated crown ground truth data. For these sites, we'll select the variable window function that resulted in the best recall as determined by comparing the training data to the field collected stems ground truth data.

Ties across window functions will be broken by selecting the window function that resulted in the fewest trees extracted from the point cloud since, for the sites with recall only, the window function able to best detect trees by using fewer trees will be more precise

```{r}
trees_site_fn_temp <-
  training_submission %>% 
  dplyr::mutate(area = sf::st_area(.)) %>% 
  sf::st_drop_geometry() %>% 
  dplyr::group_by(plot_name,ws_fn_nm) %>% 
  dplyr::summarise(trees = dplyr::n(), area = sum(area, na.rm = T)) %>%
  dplyr::ungroup() %>% 
  dplyr::inner_join(
    lidar_df %>% sf::st_drop_geometry() %>% dplyr::distinct(plot_name,siteID)
    , by = "plot_name"
  ) %>% 
  dplyr::group_by(siteID,ws_fn_nm) %>% 
  dplyr::summarise(
    plots = dplyr::n()
    , trees = mean(trees,na.rm = T)
    , area = mean(area,na.rm = T)) %>%
  dplyr::ungroup()
# trees_site_fn_temp %>% dplyr::glimpse()
```

let's look at the trees detected by NEON site and variable window function

```{r}
trees_site_fn_temp %>% 
  tidyr::pivot_longer(cols = c(trees,area)) %>% 
  dplyr::mutate(
    siteID = forcats::fct_rev(siteID)
  ) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = ws_fn_nm)) +
    # ggplot2::geom_point(size = 4) +
    ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) +
    ggplot2::facet_grid(cols = dplyr::vars(name), scales = "free_x") +
    ggplot2::scale_x_log10() +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::labs(
      x = "", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Training data: trees detected and area covered by tree crowns"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
      , axis.text.x = ggplot2::element_blank()
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    )
```

There are some variable window functions that resulted in many more trees extracted than others. If we only have recall to judge by, these functions will perform better than others which may actually be more suitable simply because there are more trees (more of the plot area is covered so it is more likely that trees will be "detected" with no penalty for including more trees). To put some checks in-place to detect these instances, we'll exclude functions from consideration to be selected if recall is the only evaluation metric and the number of trees detected is more than two standard deviations from the mean number of trees detected in a plot at a site. We'll apply the same constraints for the area covered by tree crowns as well to catch cases when there may be few trees detected but the individual crown area is very large so it covers more area even with fewer trees.

```{r}
trees_site_fn_temp <- trees_site_fn_temp %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::mutate(
    mean_trees = mean(trees,na.rm = T), sd_trees = sd(trees,na.rm = T)
    , mean_area = mean(area,na.rm = T), sd_area = sd(area,na.rm = T)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    exclude_these_fns = 
      ( abs(trees-mean_trees)>(2*sd_trees) ) | 
      ( abs(area-mean_area)>(2*sd_area) )
  )
```

pick the best window function

```{r}
training_eval_by_site <- training_eval_by_site %>% 
  # attach trees
  dplyr::left_join(
    trees_site_fn_temp %>% 
      dplyr::select(-plots)
    , by = dplyr::join_by(siteID,ws_fn_nm)
  ) %>% 
  dplyr::arrange(siteID,desc(f_score),desc(precision),exclude_these_fns,desc(recall),area,trees) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::mutate(is_best_fn = dplyr::row_number()==1) %>% 
  dplyr::ungroup()
# save it
readr::write_csv(training_eval_by_site, "../data/NeonTreeEvaluation_training_eval_by_site.csv", append = F, progress = F)
```

show the best window function by site 

```{r}
training_eval_by_site %>% 
  tidyr::pivot_longer(cols = c(precision,recall,f_score)) %>% 
  dplyr::mutate(
    name = ifelse(name=="f_score","F-score",name) %>% forcats::fct_rev()
    , siteID = forcats::fct_rev(siteID)
  ) %>% 
  dplyr::arrange(is_best_fn) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = ws_fn_nm)) +
    # ggplot2::geom_point(size = 4) +
    ggplot2::geom_jitter(
      ggplot2::aes(shape = is_best_fn)
      ,height = 0.12
      , width = 0
      , size = 5
    ) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::scale_shape_manual(values = c(20,15)) +
    ggplot2::labs(
      x = "", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Best variable window function by NEON site based on training data evaluation"
      , caption = "*a square indicates the best function"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.x = ggplot2::element_text(size=7)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
      , shape = "none"
    )
```

table of the best function by site

```{r}
training_eval_by_site %>% 
  dplyr::filter(is_best_fn) %>% 
  dplyr::select(siteID,ws_fn_nm,precision,recall,f_score) %>% 
  dplyr::arrange(siteID) %>% 
kableExtra::kbl(
    caption = "Best variable window function by NEON site based on training data evaluation"
    , digits = 2
    , col.names = c(
      "site","variable<br>window<br>function"
      ,"precision","recall","F-score"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

##### Best variable window function overall

to select the best overall variable window function to apply to NEON sites that lacked training data, we collapsed across all training sites and plots to calculate an average precision and recall at the variable window function level

we can look at the full results table

```{r}
training_eval_overall %>% 
  dplyr::select(-c(is_best_fn)) %>% 
  dplyr::relocate(tidyselect::starts_with("n_"), .after = dplyr::last_col()) %>% 
  kableExtra::kbl(
    caption = "Best variable window function overall based on training data evaluation"
    , digits = 2
    , col.names = c(
      "variable<br>window<br>function"
      ,"precision","recall","F-score"
      ,"plots<br>with ground truth<br>field collected stems"
      ,"plots<br>with ground truth<br>image ann. crowns"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

keep in mind that these are average values across plots and sites so the actual metric values matter little...it's the relative value that we care about

we can view this "quadrant" plot whereby we overlay the median value to differentiate the "best" window size by the two dimensions (precision, recall)...the upper-right quadrant is the "best" of the variable window functions tested based on these training plots evaluated against image annotated crowns and field collected stems ground truth data

```{r}
training_eval_overall %>% 
  ggplot2::ggplot(ggplot2::aes(x=precision,y=recall,color=ws_fn_nm)) +
  ggplot2::geom_hline(yintercept = median(training_eval_overall$recall),linetype="dashed") +
  ggplot2::geom_vline(xintercept = median(training_eval_overall$precision),linetype="dashed") +
  ggplot2::geom_point(ggplot2::aes(shape=is_best_fn),size = 6) +
  ggplot2::scale_color_manual(values = pal_ws) +
  ggplot2::scale_shape_manual(values = c(20,15)) +
  ggplot2::scale_x_continuous(limits = c(0,0.4)) +
  ggplot2::scale_y_continuous(limits = c(0,0.7)) +
  ggplot2::labs(
    color = "variable\nwindow\nfunction"
    , subtitle = "Best variable window function overall based on training data evaluation"
    , caption = "*a square indicates the best function"
  ) +
  ggplot2::theme_light() +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    , shape = "none"
  )
```

now we'll attach the best variable window function to our processing data

```{r}
lidar_df <- lidar_df %>%
  dplyr::left_join(
    training_eval_by_site %>%
      dplyr::filter(is_best_fn) %>% 
      dplyr::select(siteID,ws_fn_nm)
    , by = "siteID"
  ) %>% 
  dplyr::mutate(
    ws_fn_nm = dplyr::coalesce(
      ws_fn_nm
      , training_eval_overall %>%
        dplyr::filter(is_best_fn) %>% 
        dplyr::pull(ws_fn_nm)
    )
  )
# save
readr::write_csv(lidar_df, "../data/NeonTreeEvaluation_lidar_df.csv", append = F, progress = F)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(
  training_submission, training_eval_by_site
  , training_eval_overall, training_fld_cllctd_stems
  , training_img_annttd_crwns
  , submission_fn
  , neontree_plot_fn, clean_field_data
  , m, r3dDefaults
)
gc()
```

### Evaluation validation process

We can finally process the evaluation data using all that we have gleaned in our analysis with the training data

#### Extract trees for evaluation data

as a reminder, here's how many evaluation plots we have by NEON site

```{r}
lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::filter(
    data_type == "evaluation" &
      !is_new_image_annotation_training &
      !is_new_field_stems_training
  ) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    n = dplyr::n()
    , evaluation_ann_plots = sum(ifelse(has_image_annotation,1,0))
    , evaluation_fld_plots = sum(ifelse(has_field_stems,1,0))
  ) %>% 
  kableExtra::kbl(
    caption = "NEON sites with conifers and lidar plots for evaluation"
    , col.names = c(
      "site","unique<br>evaluation plots"
      ,"evaluation plots<br>with ground truth<br>image ann. crowns"
      ,"evaluation plots<br>with ground truth<br>field collected stems"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

Now we'll extract trees from the evaluation lidar data using the best window function for a site based on the training data

```{r, results=FALSE, warning=FALSE, message=FALSE}
# where should we save the file?
submission_fn_temp <- "../data/NeonTreeEvaluation_evaluation_submission.gpkg"
# if we don't already have the data, run it
if(!file.exists(submission_fn_temp)){
# if(T){ ### !!!!!!! take out when it gets real
  # just get evaluation
  evaluation_df_temp <- lidar_df %>% 
    dplyr::filter(
      data_type == "evaluation" &
        !is_new_image_annotation_training &
        !is_new_field_stems_training
    )
  
  # get trees for each evaluation plot and ws fn combination
  evaluation_submission <- 
    1:nrow(evaluation_df_temp) %>%
    purrr::map(function(x){ 
      # get ws fn
      my_fn_nm <- evaluation_df_temp %>% dplyr::slice(x) %>% dplyr::pull(ws_fn_nm)
      # run it
      ans <- cloud2trees_for_eval(
        lidar_df_row = x
        , lidar_df = evaluation_df_temp
        , ws = my_ws_functions[[my_fn_nm]]
      )
      ### some of these ws functions may not extract any trees
      ### account for this
      if(is.null(ans) || !inherits(ans,"data.frame")){
        return(NULL)
      }else{
        return(
          ans # %>% dplyr::mutate(ws_fn_nm = my_fn_nm) # uncomment for testing
        )
      }
    }) %>% 
    dplyr::bind_rows()
  # save it 
  sf::st_write(evaluation_submission, submission_fn_temp, append = F)
}else{
  evaluation_submission <- sf::st_read(submission_fn_temp, quiet = T)
}
```

now we need to filter our tree list based on the [height threshold for "canopy" trees](#ht_filter)

```{r}
evaluation_submission <-
  evaluation_submission %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::select(plot_name,site_prcntl_ht)
    , by = "plot_name"
  ) %>% 
  dplyr::filter(tree_height_m>=site_prcntl_ht) %>% 
  dplyr::select(-c(tree_height_m,site_prcntl_ht))
```

what did we get?

```{r}
evaluation_submission %>% dplyr::glimpse()
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Evaluation: validation against ground truth

We'll compare our aerial point cloud-based tree detection and crown delineation results to two types of evaluation data (i.e. “ground truth” data) presented by Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)): field-collected stems and image-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective.

##### Scores for image-annotated crowns

```{r, results=F, message=FALSE}
# where should we save the file?
evaluation_eval_temp <- "../data/NeonTreeEvaluation_evaluation_evaluate_image_crowns.csv"
# if we don't already have the data, run it
if(!file.exists(evaluation_eval_temp)){
# if(T){
  # filter for only those with this type of validation data
  submission_temp <- evaluation_submission %>% 
    dplyr::inner_join(
      lidar_df %>% 
        sf::st_drop_geometry() %>% 
        dplyr::filter(has_image_annotation) %>% 
        dplyr::distinct(plot_name)
      , by = "plot_name"
    )
  
  # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported
  # would need to dig through NeonTreeEvaluation, fix, and pull request...
  safe_evaluate_image_crowns <- purrr::safely(NeonTreeEvaluation::evaluate_image_crowns)
  # validate
  evaluation_img_annttd_crwns <- safe_evaluate_image_crowns(
    predictions = submission_temp %>% 
      sf::st_drop_geometry() %>% 
      dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
    , show = F
    , summarize = T
  )
  
  if(!is.null(evaluation_img_annttd_crwns$error)){
    stop(evaluation_img_annttd_crwns$error)
  }else{
    evaluation_img_annttd_crwns <- 
      evaluation_img_annttd_crwns$result %>% 
        purrr::pluck("plot_level") %>% 
        dplyr::ungroup()
  }
  # calculate f-score
  evaluation_img_annttd_crwns <- evaluation_img_annttd_crwns %>% 
    dplyr::mutate(
      dplyr::across(.cols = c(precision,recall), ~ifelse(is.nan(.x),as.numeric(NA),.x))
      , f_score = dplyr::case_when(
        is.na(precision) | is.na(recall) ~ as.numeric(NA)
        , (precision+recall) == 0 ~ 0
        , T ~ 2 * ( (precision*recall)/(precision+recall) )
      )
    )
  # save
  readr::write_csv(evaluation_img_annttd_crwns, file = evaluation_eval_temp)
}else{
  evaluation_img_annttd_crwns <- readr::read_csv(evaluation_eval_temp, progress = F, show_col_types = F)
}
# evaluation_img_annttd_crwns %>% dplyr::glimpse()
```

our return data should include precision and recall values for each plot that had trees extracted (some evaluation plots might not have extracted any trees) with image annotated crown evaluation data...unless the `NeonTreeEvaluation::evaluate_image_crowns()` resulted in an error

count of the plots which resulted in trees extracted and were successfully evaluated using image annotated crown ground truth data

```{r}
evaluation_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(has_image_annotation) %>% 
      dplyr::distinct(plot_name,siteID)
    , by = "plot_name"
  ) %>% 
  dplyr::count(siteID,plot_name) %>% 
  dplyr::left_join(
    evaluation_img_annttd_crwns
    , by = "plot_name"
  ) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarize(plots = dplyr::n(), plots_eval = sum(ifelse(is.na(f_score),0,1))) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(pct = scales::percent(plots_eval/plots,accuracy = 1)) %>% 
  dplyr::arrange(siteID) %>% 
  kableExtra::kbl(
    caption = "Count of plots with trees detected that were successfully evaluated using image annotated crowns"
    , digits = 0
    , col.names = c(
      "site","# plots<br>with trees","# plots<br>successfully evaluated"
      ,"% plots<br>successfully evaluated"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

we'll dig into these metrics in the [next section](#valid_sum); for now, let's review some figures comparing the image-annotated crowns versus the bounding box of our delineated crown polygons

*in the RGB plots: "red" boxes are crowns from our point cloud-based method extracted and "black" are the image annotated crowns*

```{r, message=FALSE, results=F, warning=FALSE,fig.show='asis'}
set.seed(888)
evaluation_img_annttd_crwns %>% 
  dplyr::ungroup() %>% 
  dplyr::slice_sample(n=3) %>% 
  dplyr::pull(plot_name) %>% 
  purrr::map(\(x) NeonTreeEvaluation::image_crowns(
    predictions = evaluation_submission %>% 
      dplyr::inner_join(
        lidar_df %>% 
          sf::st_drop_geometry() %>% 
          dplyr::filter(has_image_annotation) %>% 
          dplyr::distinct(plot_name)
        , by = "plot_name"
      ) %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(plot_name==x)
    , show = T
    , project_boxes = F
  ))
```

*in the RGB plots: "red" boxes are crowns from our point cloud-based method extracted and "black" are the image annotated crowns*

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

##### Prep for field-collected stems evaluation

In testing, it was discovered that some plots have lidar data collections from different years (e.g. "NIWO_040_2019" and "NIWO_040_2020"). Unfortunately, when lidar-detected trees from these different data are passed to the `NeonTreeEvaluation::evaluate_field_stems()` function from the same plot, only a single recall score is returned at the plot level with the year removed (e.g. "NIWO_040").

If the lidar data is truly different, then we would prefer to obtain evaluated recall for each unique, yearly data set even if it is compared against the same ground truth data. Different lidar data just presents more opportunities for us to test our methodologies. Let's ensure that these are different lidar data sets by loading in a few.

```{r}
lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::filter(plot_has_multiple_years & has_field_stems) %>% 
  dplyr::select(plot_name, plot_name_sans_yr, plot_name_sans_yr_grp) %>% 
  dplyr::slice_head(n=10) %>% 
  kableExtra::kbl(caption="examples of plots with lidar data collections from different years") %>% 
  kableExtra::kable_styling()
```

let's load in the lidar data for one of these plots with lidar data collections from different years and look at the lidar data summary 

```{r}
# sample a plot
ex_plot_temp <- lidar_df %>% 
  sf::st_drop_geometry() %>% 
  dplyr::filter(plot_has_multiple_years) %>% 
  dplyr::slice_sample(n=1) %>% 
  dplyr::pull(plot_name_sans_yr)
# load in the lidar for all years from this plot
las_list_temp <-
  1:nrow( lidar_df %>% dplyr::filter(plot_name_sans_yr==ex_plot_temp) ) %>% 
  purrr::map(function(x){
    lidR::readLAS(
      files =
        lidar_df %>% 
        dplyr::filter(plot_name_sans_yr==ex_plot_temp) %>% 
        dplyr::slice(x) %>% 
        dplyr::pull(f_path)
    )
  })
# check it
las_list_temp
```

notice that even though these point clouds are from the same plot (see the "extent"), the underlying data is different (see the "points" and "memory")

Since the lidar data is different we will obtain evaluated recall for each unique, yearly data set even if it is compared against the same ground truth data

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

##### Scores for field-collected stems

```{r, results=F, message=FALSE}
# where should we save the file?
evaluation_eval_temp <- "../data/NeonTreeEvaluation_evaluation_evaluate_field_stems.csv"
# if we don't already have the data, run it
if(!file.exists(evaluation_eval_temp)){
# if(T){
  # filter for only those with this type of validation data
  submission_temp <- evaluation_submission %>% 
    dplyr::inner_join(
      lidar_df %>% 
        sf::st_drop_geometry() %>% 
        dplyr::filter(has_field_stems) %>% 
        dplyr::distinct(plot_name,plot_name_sans_yr,plot_name_sans_yr_grp)
      , by = "plot_name"
    )
  
  # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported
  # would need to dig through NeonTreeEvaluation, fix, and pull request...
  safe_evaluate_field_stems <- purrr::safely(NeonTreeEvaluation::evaluate_field_stems)
  # validate
  evaluation_fld_cllctd_stems <-
    submission_temp$plot_name_sans_yr_grp %>% 
    unique() %>% 
    purrr::map(function(x){
      ans <- safe_evaluate_field_stems(
        predictions = submission_temp %>% 
          # sf::st_drop_geometry() %>% ### polygons work here !
          dplyr::filter(plot_name_sans_yr_grp == x) %>% 
          dplyr::select(xmin,ymin,xmax,ymax,label,plot_name)
        , show = F
        , summarize = T
      )
      if(!is.null(ans$error)){
        return(NULL)
      }else{
        return(
          ans$result %>% 
            purrr::pluck("plot_level") %>% 
            dplyr::ungroup() %>% 
            dplyr::mutate(plot_name_sans_yr_grp = x)
        ) 
      }
    }) %>% 
    dplyr::bind_rows() %>%
    # attach the full plot name
    dplyr::inner_join(
      submission_temp %>% 
        sf::st_drop_geometry() %>% 
        dplyr::distinct(plot_name,plot_name_sans_yr,plot_name_sans_yr_grp) %>% 
        dplyr::rename(plot_name_orig=plot_name)
      , by = dplyr::join_by(plot_name==plot_name_sans_yr,plot_name_sans_yr_grp)
    ) %>% 
    dplyr::mutate(plot_name = plot_name_orig) %>% 
    dplyr::select(-c(plot_name_orig,plot_name_sans_yr_grp))
  
  # evaluation_fld_cllctd_stems %>% dplyr::glimpse()
  # identical( 
  #   evaluation_fld_cllctd_stems %>% nrow() 
  #   , evaluation_fld_cllctd_stems %>% dplyr::distinct(plot_name) %>% nrow() 
  # )
  # save
  readr::write_csv(evaluation_fld_cllctd_stems, file = evaluation_eval_temp)
}else{
  evaluation_fld_cllctd_stems <- readr::read_csv(evaluation_eval_temp, progress = F, show_col_types = F)
}
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

our return data should include recall values for each plot and variable window function that resulted in trees extracted (some window functions might not have extracted any trees) with image annotated crown evaluation data...unless the `NeonTreeEvaluation::evaluate_field_stems()` resulted in an error

count of the plots which resulted in trees extracted and were successfully evaluated using field collected stems ground truth data

```{r}
evaluation_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(has_field_stems) %>% 
      dplyr::distinct(siteID,plot_name)
    , by = "plot_name"
  ) %>% 
  dplyr::count(siteID,plot_name) %>% 
  dplyr::left_join(
    evaluation_fld_cllctd_stems %>% dplyr::select(-siteID)
    , by = "plot_name"
  ) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarize(plots = dplyr::n(), plots_eval = sum(ifelse(is.na(recall),0,1))) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(pct = scales::percent(plots_eval/plots,accuracy = 1)) %>% 
  dplyr::arrange(siteID) %>% 
  kableExtra::kbl(
    caption = "Count of plots with trees detected that were successfully evaluated using field collected stems"
    , digits = 0
    , col.names = c(
      "site","# plots<br>with trees","# plots<br>successfully evaluated"
      ,"% plots<br>successfully evaluated"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

```{r, include=FALSE,eval=FALSE}
evaluation_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::inner_join(
    lidar_df %>% 
      sf::st_drop_geometry() %>% 
      dplyr::filter(has_field_stems) %>% 
      dplyr::distinct(siteID,plot_name)
    , by = "plot_name"
  ) %>% 
  dplyr::count(siteID,plot_name) %>% 
  dplyr::left_join(
    evaluation_fld_cllctd_stems %>% dplyr::select(-siteID)
    , by = "plot_name"
  ) %>% 
  dplyr::filter(is.na(recall))
```

we'll dig into these metrics in the [next section](#valid_sum); for now, let's review some figures comparing the field collected stem point locations versus our delineated crown polygons

*in the RGB plots: "red" outlines are crowns from our point cloud-based method extracted while "blue" points are the field collected stems not detected and "yellow" points are the field collected stems properly identified*

```{r, message=FALSE, results=F, warning=FALSE,fig.show='asis'}
set.seed(88)
evaluation_fld_cllctd_stems %>% 
  dplyr::ungroup() %>% 
  dplyr::slice_sample(n=3) %>% 
  dplyr::pull(plot_name) %>% 
  purrr::map(\(x) NeonTreeEvaluation::evaluate_field_stems(
    predictions = evaluation_submission %>% 
      dplyr::inner_join(
        lidar_df %>% 
          sf::st_drop_geometry() %>% 
          dplyr::filter(has_field_stems) %>% 
          dplyr::distinct(plot_name)
        , by = "plot_name"
      ) %>% 
      dplyr::filter(plot_name==x)
    , show = T
  ))
```

*in the RGB plots: "red" outlines are crowns from our point cloud-based method extracted while "blue" points are the field collected stems not detected and "yellow" points are the field collected stems properly identified*

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Summary of validation against ground truth{#valid_sum}

Let's take a deep dive into the evaluation of our aerial point cloud-based tree detection methodology based on comparison to the Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) benchmark data

#### Overall

let's look at the distribution of scores by pooling all plots across NEON sites

##### Image-annotated crowns

distribution of plot-level scores

```{r}
evaluation_img_annttd_crwns %>% 
  tidyr::pivot_longer(cols = c(precision,recall,f_score)) %>% 
  dplyr::mutate(
    name = ifelse(name=="f_score","F-score",name) %>% forcats::fct_rev()
  ) %>% 
  dplyr::group_by(name) %>% 
  dplyr::mutate(med_val = median(value,na.rm=T)) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, color = name, fill = name)) +
    ggplot2::geom_density(alpha = 0.7, lwd = 1.2) +
    ggplot2::geom_vline(aes(xintercept = med_val), linetype = "dashed") +
    ggplot2::geom_rug() +
    ggplot2::facet_wrap(facets = vars(name), scales = "free") +
    ggplot2::scale_x_continuous(breaks = scales::breaks_extended(6)) +
    ggplot2::scale_color_viridis_d(begin=0.3,end=0.7) +
    ggplot2::scale_fill_viridis_d(begin=0.3,end=0.7) +
    ggplot2::labs(
      x = "", y = ""
      , subtitle = "Overall validation against image annotated crowns ground truth"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      legend.position = "none"
      , axis.text.x = ggplot2::element_text(size=7)
      , axis.text.y = ggplot2::element_blank()
      , axis.ticks.y = ggplot2::element_blank()
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    )
```

summary statistics

```{r}
evaluation_img_annttd_crwns %>% 
  dplyr::ungroup() %>% 
  dplyr::rename(fscore=f_score) %>% 
  dplyr::summarise(
    dplyr::across(
      c(precision,recall,fscore)
      , .fns = list(mean = mean, median = median, sd = sd, min = min, max = max)
    )
    , n = dplyr::n()
  ) %>% 
  tidyr::pivot_longer(cols = -c(n)) %>% 
  dplyr::mutate(
    agg = stringr::word(name,-1,sep = "_")
    , metric = stringr::str_remove_all(name, paste0("_",agg))
  ) %>% 
  dplyr::select(-name) %>% 
  dplyr::mutate(
    value = scales::comma(value,accuracy=0.01)
  ) %>% 
  tidyr::pivot_wider(names_from = agg, values_from = value) %>% 
  dplyr::mutate(
    unit_lab = paste0(
      "Overall<br>("
      , scales::comma(n,accuracy=1)
      ," plots)"
    )
    , range = paste0(min, "—", max)
  ) %>% 
  dplyr::relocate(unit_lab) %>% 
  dplyr::mutate(
    metric = factor(
      metric
      , ordered = T
      , levels = c(
          "recall"
          , "precision"
          , "fscore"
        )
      , labels = c(
          "recall"
          , "precision"
          , "F-score"
        )
    )
  ) %>% 
  dplyr::select(-c(n,min,max)) %>% 
  dplyr::arrange(unit_lab,metric) %>% 
  kableExtra::kbl(
    caption = "Overall validation against image annotated crowns ground truth"
    , col.names = c(
      "Score level", "Metric"
      , "Mean", "Median"
      , "Std Dev", "Range"
    )
    , escape = F
    # , digits = 2
  ) %>% 
  kableExtra::kable_styling(font_size = 13) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")
```

###### Comparison with other methods

There are overall scores from other methods in the benchmark

| Author                | Precision | Recall | Cite/Code                              |
|-----------------------|-----------|--------|------------------------------------------|
| Weinstein et al. 2020 | 0.66 | 0.79 | https://deepforest.readthedocs.io/|
| Silva et al. 2016     | 0.34| 0.47 |[`lidR::silva2016()`](https://rdrr.io/cran/lidR/man/its_silva2016.html)|
| `cloud2trees` (present study)     | `r scales::comma(mean(evaluation_img_annttd_crwns$precision),accuracy = 0.01)`| `r scales::comma(mean(evaluation_img_annttd_crwns$recall),accuracy = 0.01)` |[cloud2trees package](https://github.com/georgewoolsey/cloud2trees)|

Our scores are in-line with the scores obtained by [`lidR::silva2016()`](https://rdrr.io/cran/lidR/man/its_silva2016.html) but it is unclear which plots and NEON sites were evaluated with this methodology

Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) describe the DeepForest model:

>DeepForest is a RGB deep learning model that predicts canopy crown bounding boxes. The prebuilt model in DeepForest was trained with the training data described above, but did not use or overlap spatially with any evaluation data in this benchmark. Following the best practices for computational biology benchmarking described in, we emphasize that the DeepForest algorithm was designed in conjunction with these evaluation data and it is therefore not surprising that it performs well, with image-annotated boxes and field-annotated crown polygons both at approximately 70% accuracy. (p. 14)

##### Field-collected stems

distribution of plot-level scores

```{r}
evaluation_fld_cllctd_stems %>% 
  tidyr::pivot_longer(cols = c(recall,n)) %>% 
  dplyr::filter(name!="n") %>% 
  dplyr::mutate(
    name = ifelse(name=="n","# actual trees",name) %>% forcats::fct_rev()
  ) %>% 
  dplyr::group_by(name) %>% 
  dplyr::mutate(med_val = median(value,na.rm=T)) %>% 
  dplyr::ungroup() %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, color = name, fill = name)) +
    ggplot2::geom_density(alpha = 0.7, lwd = 1.2) +
    ggplot2::geom_vline(aes(xintercept = med_val), linetype = "dashed") +
    ggplot2::geom_rug() +
    ggplot2::facet_wrap(facets = vars(name), scales = "free") +
    ggplot2::scale_x_continuous(breaks = scales::breaks_extended(6)) +
    ggplot2::scale_color_manual(values=c(viridis::viridis(n=3,begin=0.3,end=0.7)[1],"gray")) +
    ggplot2::scale_fill_manual(values=c(viridis::viridis(n=3,begin=0.3,end=0.7)[1],"gray")) +
    ggplot2::labs(
      x = "", y = ""
      , subtitle = "Overall validation against field-collected stems ground truth"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      legend.position = "none"
      , axis.text.x = ggplot2::element_text(size=7)
      , axis.text.y = ggplot2::element_blank()
      , axis.ticks.y = ggplot2::element_blank()
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    )
```

summary statistics

```{r}
evaluation_fld_cllctd_stems %>% 
  dplyr::ungroup() %>% 
  dplyr::summarise(
    dplyr::across(
      c(recall)
      , .fns = list(mean = mean, median = median, sd = sd, min = min, max = max)
    )
    , n = dplyr::n()
  ) %>% 
  tidyr::pivot_longer(cols = -c(n)) %>% 
  dplyr::mutate(
    agg = stringr::word(name,-1,sep = "_")
    , metric = stringr::str_remove_all(name, paste0("_",agg))
  ) %>% 
  dplyr::select(-name) %>% 
  dplyr::mutate(
    value = scales::comma(value,accuracy=0.01)
  ) %>% 
  tidyr::pivot_wider(names_from = agg, values_from = value) %>% 
  dplyr::mutate(
    unit_lab = paste0(
      "Overall<br>("
      , scales::comma(n,accuracy=1)
      ," plots)"
    )
    , range = paste0(min, "—", max)
  ) %>% 
  dplyr::relocate(unit_lab) %>% 
  dplyr::select(-c(n,min,max)) %>% 
  dplyr::arrange(unit_lab,metric) %>% 
  kableExtra::kbl(
    caption = "Overall validation against field-collected stems ground truth"
    , col.names = c(
      "Score level", "Metric"
      , "Mean", "Median"
      , "Std Dev", "Range"
    )
    , escape = F
    # , digits = 2
  ) %>% 
  kableExtra::kable_styling(font_size = 13) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")

```

###### Comparison to other methods

There are overall scores from other methods in the benchmark

| Author                |Recall | Cite/Code                              |
|-----------------------|--------|------------------------------------------|
| Weinstein et al. 2020 | 0.74 | https://deepforest.readthedocs.io/|
| `cloud2trees` (present study) | `r scales::comma(mean(evaluation_fld_cllctd_stems$recall),accuracy = 0.01)` | [cloud2trees package](https://github.com/georgewoolsey/cloud2trees)|

Our scores are in-line with the scores obtained by the DeepForest model which is promising given that "the DeepForest algorithm was designed in conjunction with these evaluation data" presented by Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6), p. 14)

#### NEON site

let's look at the distribution of scores at the NEON site level

##### Image-annotated crowns

distribution of plot-level scores

```{r}
evaluation_img_annttd_crwns %>% 
  dplyr::inner_join(lidar_df %>% sf::st_drop_geometry() %>% dplyr::distinct(plot_name,siteID),by="plot_name") %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::mutate(siteID = paste0(siteID, "\n(plots=", scales::comma(dplyr::n(),accuracy=1),")")) %>% 
  dplyr::ungroup() %>% 
  tidyr::pivot_longer(cols = c(precision,recall,f_score)) %>% 
  dplyr::mutate(
    name = ifelse(name=="f_score","F-score",name) %>% forcats::fct_rev()
    , siteID = forcats::fct_rev(siteID)
  ) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = name)) +
    ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) +
    ggplot2::geom_boxplot(
      width=0.3,staplewidth=0.4,lwd=0.7
      ,fill=NA,color = "gray9"
      , outliers = F
    ) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_viridis_d(begin=0.3,end=0.7) +
    ggplot2::scale_x_continuous(breaks = scales::breaks_extended(6)) +
    ggplot2::labs(
      x = "", y = "", color = ""
      , subtitle = "NEON site-level validation against image annotated crowns ground truth"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      legend.position = "none"
      , axis.text.x = ggplot2::element_text(size=7)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    )
```

summary statistics

```{r}
evaluation_img_annttd_crwns %>% 
  dplyr::inner_join(lidar_df %>% sf::st_drop_geometry() %>% dplyr::distinct(plot_name,siteID),by="plot_name") %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::rename(fscore=f_score) %>% 
  dplyr::summarise(
    dplyr::across(
      c(precision,recall,fscore)
      , .fns = list(mean = mean, median = median, sd = sd, min = min, max = max)
    )
    , n = dplyr::n()
  ) %>% 
  tidyr::pivot_longer(cols = -c(siteID,n)) %>% 
  dplyr::mutate(
    agg = stringr::word(name,-1,sep = "_")
    , metric = stringr::str_remove_all(name, paste0("_",agg))
  ) %>% 
  dplyr::select(-name) %>% 
  dplyr::mutate(
    value = scales::comma(value,accuracy=0.01)
  ) %>% 
  tidyr::pivot_wider(names_from = agg, values_from = value) %>% 
  dplyr::mutate(
    unit_lab = paste0(
      siteID
      , "<br>("
      , scales::comma(n,accuracy=1)
      ," plots)"
    )
    , range = paste0(min, "—", max)
  ) %>% 
  dplyr::relocate(unit_lab) %>% 
  dplyr::select(-c(siteID)) %>% 
  dplyr::mutate(
    metric = factor(
      metric
      , ordered = T
      , levels = c(
          "recall"
          , "precision"
          , "fscore"
        )
      , labels = c(
          "recall"
          , "precision"
          , "F-score"
        )
    )
  ) %>% 
  dplyr::select(-c(n,min,max)) %>% 
  dplyr::arrange(unit_lab,metric) %>% 
  kableExtra::kbl(
    caption = "NEON site-level validation against image annotated crowns ground truth"
    , col.names = c(
      "site", "Metric"
      , "Mean", "Median"
      , "Std Dev", "Range"
    )
    , escape = F
    # , digits = 2
  ) %>% 
  kableExtra::kable_styling(font_size = 13) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")
```

where are these NEON sites that we were able to evaluate using the image annotated crowns ground truth data

```{r}
lidar_df %>% 
  dplyr::inner_join(
    evaluation_img_annttd_crwns %>% dplyr::distinct(plot_name)
  ) %>% 
  dplyr::count(siteID) %>%
  dplyr::rename(plots_evaluated=n) %>% 
  mapview::mapview(
    cex = "plots_evaluated"
    , label = "siteID"
    , legend = F
    # , layer.name = "# plots evaluated"
    , col.regions = "firebrick"
  )
```

##### Field-collected stems

distribution of plot-level scores

```{r}
evaluation_fld_cllctd_stems %>% 
  dplyr::select(-siteID) %>% 
  dplyr::inner_join(
    lidar_df %>% sf::st_drop_geometry() %>% dplyr::distinct(plot_name,siteID),by="plot_name"
  ) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::mutate(siteID = paste0(siteID, "\n(plots=", scales::comma(dplyr::n(),accuracy=1),")")) %>% 
  dplyr::ungroup() %>% 
  tidyr::pivot_longer(cols = c(recall,n)) %>% 
  dplyr::filter(name!="n") %>% 
  dplyr::mutate(
    name = ifelse(name=="n","# actual trees",name) %>% forcats::fct_rev()
    , siteID = forcats::fct_rev(siteID)
  ) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = name)) +
    ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) +
    ggplot2::geom_boxplot(
      width=0.3,staplewidth=0.4,lwd=0.7
      ,fill=NA,color = "gray9"
      , outliers = F
    ) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_manual(values=c(viridis::viridis(n=3,begin=0.3,end=0.7)[1],"gray")) +
    ggplot2::scale_x_continuous(breaks = scales::breaks_extended(6)) +
    ggplot2::labs(
      x = "", y = "", color = ""
      , subtitle = "NEON site-level validation against field-collected stems ground truth"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      legend.position = "none"
      , axis.text.x = ggplot2::element_text(size=7)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    )
```

summary statistics

```{r}
evaluation_fld_cllctd_stems %>% 
  dplyr::select(-siteID) %>% 
  dplyr::inner_join(
    lidar_df %>% sf::st_drop_geometry() %>% dplyr::distinct(plot_name,siteID),by="plot_name"
  ) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::summarise(
    dplyr::across(
      c(recall)
      , .fns = list(mean = mean, median = median, sd = sd, min = min, max = max)
    )
    , n = dplyr::n()
  ) %>% 
  tidyr::pivot_longer(cols = -c(siteID,n)) %>% 
  dplyr::mutate(
    agg = stringr::word(name,-1,sep = "_")
    , metric = stringr::str_remove_all(name, paste0("_",agg))
  ) %>% 
  dplyr::select(-name) %>% 
  dplyr::mutate(
    value = scales::comma(value,accuracy=0.01)
  ) %>% 
  tidyr::pivot_wider(names_from = agg, values_from = value) %>% 
  dplyr::mutate(
    unit_lab = paste0(
      siteID
      , "<br>("
      , scales::comma(n,accuracy=1)
      ," plots)"
    )
    , range = paste0(min, "—", max)
  ) %>% 
  dplyr::relocate(unit_lab) %>% 
  dplyr::select(-c(siteID,n,min,max)) %>% 
  dplyr::arrange(unit_lab,metric) %>% 
  kableExtra::kbl(
    caption = "NEON site-level validation against field collected-stems ground truth"
    , col.names = c(
      "site", "Metric"
      , "Mean", "Median"
      , "Std Dev", "Range"
    )
    , escape = F
    # , digits = 2
  ) %>% 
  kableExtra::kable_styling(font_size = 13) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top")
```

the recall values for our lidar-based tree list are much better using the field collected stems for validation than using the image annotated crowns...this might this suggest that our "canopy" filter is aligned for field stems (which we based the filter on) but not for crowns visible from aerial photography

where are these NEON sites that we were able to evaluate using the field-collected stems ground truth data

```{r}
lidar_df %>% 
  dplyr::inner_join(
    evaluation_fld_cllctd_stems %>% dplyr::distinct(plot_name)
  ) %>% 
  dplyr::count(siteID) %>%
  dplyr::rename(plots_evaluated=n) %>% 
  mapview::mapview(
    cex = "plots_evaluated"
    , label = "siteID"
    , legend = F
    # , layer.name = "# plots evaluated"
    , col.regions = "steelblue2"
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(
  evaluation_fld_cllctd_stems, evaluation_img_annttd_crwns
  , evaluation_submission, lidar_df
  , safe_evaluate_field_crowns, safe_evaluate_field_stems
  , percentile_for_ht, neon_min_ht
)
gc()
```

## FOR-instance benchmark

Puliti et al. [(2023)](https://doi.org/10.48550/arXiv.2309.01279) also provide the benchmarking dataset "FOR-instance" for dense airborne laser scanning data. The dataset comprises five curated UAV-based laser scanning data collections from diverse global locations, representing various forest types. The data was collected over five sites in Norway, Czech Republic, Austria, Australia, and New Zealand and covers mature forests for the following forest types: boreal (42% of the number of trees) and temperate coniferous (5%), temperate mixed deciduous (29%), coniferous plantation (13%), and dry sclerophyll forests (11%). We will also consider this data for validation of our methods.

This benchmark data is described (p. 10): 

>The annotated collections were then split into development (i.e. dev) and test sub-sets in order to create two separate sets of data for the following scopes:

> **Dev data**: these data can be used for developing new methods. The dev data can be further split into training and validation by the user to create a validation dataset for hyperparameter tuning, overfitting detection, and model selection.

> **Test data**: these data aim to be used for the final evaluation of a method (i.e., either preexisting or developed using the training data). These datasets should strictly not be used in any step of model training, and to avoid any information leak, the user should limit the number of
times the test data is used.

70% of the annotated area was selected for development (i.e. "training") and 30% for testing (i.e. "evaluation"). We downloaded this data from the online archive at [https://zenodo.org/records/8287792](https://zenodo.org/records/8287792) 

### lidar data in **FOR-instance**

FOR-instance includes the following five forest types described by Puliti et al. [(2023, p. 5)](https://doi.org/10.48550/arXiv.2309.01279):

1) Coniferous dominated boreal forest (42% of the annotated trees): these are found in Norway in the **NIBIO** collection and are composed of mature forests dominated by Norway spruce (*Picea abies*) and with few Scots pine (*Pinus Sylvestris*) and deciduous species (mainly *Betula* sp.). In specific, the **NIBIO** collection is mainly dominated by Norway spruce. Further details on the **NIBIO** forest structures can be found in the study by Puliti et al. (2022).
2) Coniferous dominated temperate forest (5% of the trees): these forests correspond to three plots in the **CULS** collection and consist of a productive even-aged Scots pine forest stand found in sandy areas of the Czech north of Prague, that are generally more productive than the mire forest in boreal conditions. Further details regarding forest characteristics can be found in Kuzelka et al. (2020).
3) Deciduous alluvial forests (29% of the annotated trees): this forest type is found in the **TU_WIEN** collection and consists of a Natura 2000 site characterized by complex vegetation structures (woody debris, lying and standing deadwood, dense understory) a large diversity of tree species and considerable variation in canopy layering and tree size (DBH up to 1.3 m). Further details on the specific forest characteristics can be found in Wieser et al. (2017).
4) Native dry sclerophyll eucalypt forest (11% of the annotated trees): This forest type is found in the **RMIT** collection with a dominant canopy species of White Peppermint (*Eucalyptus pulchella*). Trees of this species are found within the plot at a range of ages and heights (from 5 m to 17 m). The study site was subject to planned burning 3 years before the drone laser scanning acquisition. The understory at time of capture consisted of low shrubs (up to 2 m in height) and native grasses.
5) Coniferous non-native temperate (13% of the annotated trees): This type is found in the **SCION** collection and consists of pure radiata pine (*Pinus radiata*) of 18-20 years of age and is representative of a typical productive plantation forest (approx. 930 trees per hectare) in New Zealand. The forest was planted on a 2 m × 5 m grid and remained unthinned, and no silvicultural interventions except for regular mowing of the blackberry understory between the rows. The vertical forest structure is single-layered, and due to the branchiness of radiata pine, the canopy is composed of a dense layer of intertwined and overlapping crowns.

we will want to limit our evaluation to only sites with conifer trees -- **NIBIO**, **CULS**, and **SCION** -- since `cloud2trees` implements methods developed specifically to quantify conifer forest structure that may not be appropriate for other forest types

let's load the FOR-instance point cloud data tracking file. If you have been with us from the beginning, this is the `lidar_df` file that we [generated above](#neon_lidar) for the Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) benchmark

```{r}
fi_lidar_df <- readr::read_csv("../data/FORinstance_dataset/data_split_metadata.csv", show_col_types = F, progress = F)
# nice! we'll structure this in the same way as the `lidar_df` we have been working with above
fi_lidar_df <-
  fi_lidar_df %>% 
  dplyr::mutate(
    f_path = file.path( normalizePath("../data/FORinstance_dataset"), path )
    , siteID = folder
    , plotID = 
      stringr::str_extract(path, pattern = "plot_\\d+") %>% 
      stringr::str_remove("plot_") %>% 
      readr::parse_number()
    , plot_name = paste0(
      siteID
      , "_"
      , f_path %>% basename() %>% stringr::str_remove_all("\\.(laz|las)$")
    )
    , data_type = dplyr::case_when(
      split == "dev" ~ "training"
      , split == "test" ~ "evaluation"
    ) %>% as.factor()
  ) %>% 
  # we need to figure out where the tree level data resides
  dplyr::left_join(
    list.files(
        "../data/FORinstance_dataset"
        , pattern = "tree_data_.*\\.csv$"
        , full.names = T, recursive = T
      ) %>% 
      normalizePath() %>% 
      dplyr::as_tibble() %>% 
      dplyr::rename(tree_data = 1) %>% 
      dplyr::mutate(siteID = dirname(tree_data) %>% word(-1,sep = "/")) %>% 
      dplyr::group_by(siteID) %>% 
      dplyr::filter(dplyr::row_number()==1) %>% 
      dplyr::ungroup()
    , by = "siteID"
  ) %>% 
  # filter for the conifer sites
  dplyr::filter(siteID %in% c("NIBIO","CULS","SCION") )
# what is this?
fi_lidar_df %>% dplyr::glimpse()
```

looks good. let's load in an example file and look at what it has

```{r}
las_temp <-
  fi_lidar_df %>% 
  dplyr::filter(data_type=="training") %>% 
  dplyr::slice(17) %>% 
  dplyr::pull(f_path) %>% 
  lidR::readLAS()
# what?
las_temp
```

that is a dense point cloud compared to the ALS data we used in the Weinstein et al. ([2021](https://scholar.google.com/scholar?cluster=4986448711981898434&hl=en&as_sdt=0,6)) benchmark

Let's see if the point cloud data has attributes attached. Puliti et al. [(2023)](https://doi.org/10.48550/arXiv.2309.01279) note that:

>The tree unique identifier was stored as an additional treeID column in the .las files. The annotation was performed for all trees where the stem was visible in the point cloud. Hence, small trees in the understory were assigned to a low vegetation class (see step 2 semantic annotation). Trees outside the plot boundaries were not annotated but retained and assigned to the out-points class (see step 2 semantic annotation). In addition to any other point that did not represent a tree (e.g. ground), these points were assigned a treeID = 0. (p. 8)

```{r}
las_temp %>% 
  purrr::pluck("data") %>% 
  dplyr::glimpse()
```

look at that `treeID` attribute, super slick. this particular plot has a small area, let's plot point cloud and color the points by `treeID`

```{r,echo=FALSE,message=FALSE,warning=FALSE}
# ## if want to get current rgl parameters
# par3d()$zoom
# par3d()$FOV
# par3d()$userMatrix %>% c()
# par3d()$windowRect
## set up for printing rgl graphics
r3dDefaults <- rgl::r3dDefaults
m  <- structure(c(
  1.0000000,0.0000000,0.0000000,0.0000000,0.0000000
  ,0.3420201,-0.9396926,0.0000000,0.0000000,0.9396926
  ,0.3420201,0.0000000,0.0000000,0.0000000,0.0000000,1.0000000
), .Dim = c(4L, 4L))
r3dDefaults$FOV <- 30
r3dDefaults$userMatrix <- m
r3dDefaults$zoom <- 1
r3dDefaults$windowRect <- c(371,31,1139,872)
rgl::setupKnitr(autoprint = TRUE)
```

```{r, rgl = TRUE}
las_temp %>% 
  lidR::filter_poi(!is.na(treeID) & treeID != 0) %>% 
  lidR::plot(color = "treeID", bg = "white", legend = F)
```

those sure look like trees \\ (^_^) /

one challenge we will face is how to deal with trees that are in the data but not marked as trees because they are out of the plot

```{r, rgl = TRUE}
las_temp %>% 
  lidR::add_attribute(
    ifelse(las_temp$treeID==0,0,1)
    , "is_tree"
  ) %>% 
  lidR::filter_poi(!is.na(treeID)) %>% 
  lidR::plot(color = "is_tree", bg = "white", legend = F)
```

these non-plot trees shown in blue will be classified with the Classification code of "3" for "out-points"

```{r}
las_temp %>% 
  purrr::pluck("data") %>% 
  dplyr::count(Classification)
```

| Classification    | Class Name            |
|-------------------|-----------------------|
| 0           | Unclassified |
| 1           | Low-vegetation |
| 2           | Terrain |
| 3           | Out-points |
| 4           | Stem |
| 5           | Live branches |
| 6           | Woody branches |

we will use the Classification code of "3" for "out-points" to mask our `cloud2trees` results in post-processing so that they don't count against us for precision.

Since the individual tree segmentation from Puliti et al. [(2023)](https://doi.org/10.48550/arXiv.2309.01279) resides in the point cloud data only, we will need to transform the 3D point cloud into 2D representations of the tree crown. Let's make a quick function to do this so that we can compare tree crown polygons from the benchmark to our `cloud2trees` tree crown polygons. This will allow us to perform "instance segmentation evaluation" whereby we assess the ability of our method to correctly identify and delineate individual trees.

we'll want to use the FOR-instance point cloud as input and return: 1) tree crown polygons; 2) polygons of areas outside of the plot to mask

```{r, include=FALSE,eval=FALSE}
stdtreemetrics_new = function(x, y, z){
  metrics <- list(
    max_z = max(z),
    min_z = min(z),
    npoints = length(x)
  )
  return(metrics)
}

las_temp %>% 
  lidR::filter_poi(!is.na(treeID) & treeID != 0) %>% 
  lidR::crown_metrics(func = ~stdtreemetrics_new(X, Y, Z), attribute = "treeID", geom = "concave")


ggplot() + 
  geom_sf(
    data = las_temp %>% 
      lidR::filter_poi(Classification == 3) %>% 
      lidR::pixel_metrics(fun = max(!treeID), res = 0.1) %>% 
      terra::as.polygons(aggregate = T, na.rm = T, crs = lidR::crs(las_temp)) %>% 
      sf::st_as_sf()
    , color = "black"
    , fill = "gray22"
  ) +
  geom_sf(
    data = las_temp %>% 
      lidR::filter_poi(!is.na(treeID) & treeID != 0) %>% 
      lidR::crown_metrics(func = .stdtreemetrics, attribute = "treeID", geom = "concave") # yesssss
    , fill=NA, color = "red",lwd=0.9
  ) + 
  geom_sf(
    data = las_temp %>% 
      lidR::filter_poi(!is.na(treeID) & treeID != 0) %>% 
      lidR::crown_metrics(func = .stdtreemetrics, attribute = "treeID", geom = "convex") # nooooo
    , fill=NA, color = "blue",lwd=0.8
  ) +
  geom_sf(
    data = sf::st_difference(
      las_temp %>% 
      lidR::filter_poi(!is.na(treeID) & treeID != 0) %>% 
      lidR::crown_metrics(func = .stdtreemetrics, attribute = "treeID", geom = "concave")
      , las_temp %>% 
      lidR::filter_poi(Classification == 3) %>% 
      lidR::pixel_metrics(fun = max(!treeID), res = 0.1) %>% 
      terra::as.polygons(aggregate = T, na.rm = T, crs = lidR::crs(las_temp)) %>% 
      sf::st_as_sf()
    )
    , fill=NA, color = "cyan",lwd=0.5
  )
```

```{r}
# .stdtreemetrics from lidR does not return min(z)
stdtreemetrics_new = function(x, y, z){
  metrics <- list(
    max_z = max(z),
    min_z = min(z),
    npoints = length(x)
  )
  return(metrics)
}
# pass a las, return some polygons with metrics
fi_las_to_poly <- function(las,res=0.25,smooth=T) {
  if(!inherits(las,"LAS")){stop("must be LAS class")}
  # get the mask using out of plot points these areas will be removed from both
  # the ground truth crowns and our estimated crowns
  mask <- las %>% 
    lidR::filter_poi(Classification == 3) %>% 
    lidR::pixel_metrics(fun = max(!treeID), res = res)
  if(smooth){
    mask <- terra::focal(
      mask
      , w = 3
      , fun = "max"
      , na.rm = T
      # na.policy Must be one of:
        # "all" (compute for all cells)
        # , "only" (only for cells that are NA)
        # , or "omit" (skip cells that are NA).
      , na.policy = "only"
    )
  }
  # raster mask to polygons
   mask <- mask %>% 
    terra::as.polygons(aggregate = T, na.rm = T, crs = lidR::crs(las)) %>% 
    sf::st_as_sf()
  # tree crown polygons
  crowns <- 
    las %>% 
    lidR::filter_poi(!is.na(treeID) & treeID != 0) %>% 
    # lidR::crown_metrics(func = .stdtreemetrics, attribute = "treeID", geom = "concave") %>% 
    lidR::crown_metrics(func = ~stdtreemetrics_new(X, Y, Z), attribute = "treeID", geom = "concave") %>% 
    dplyr::ungroup() %>% 
    dplyr::mutate(gt_tree_height = max_z - min_z) %>% 
    # sf::st_difference(mask) %>% # to be consistent we
    dplyr::select(treeID,gt_tree_height)
  # return
  return(list(
    crowns = crowns
    , mask = mask
  ))
}
```

let's see if this function gets us 1) tree crown polygons; 2) polygons of areas outside of the plot to mask

```{r}
polys_temp <- fi_las_to_poly(las = las_temp,res=0.1,smooth = F)
# plot it
ggplot2::ggplot() +
  ggplot2::geom_sf(data = polys_temp$mask, mapping = ggplot2::aes(fill = "mask"), color = NA,alpha=0.9) +
  ggplot2::geom_sf(data = polys_temp$crowns, mapping = ggplot2::aes(fill = "crowns"), color = NA,alpha=0.9) +
  ggplot2::scale_fill_manual(values = c("forestgreen","gray11"), name = "") +
  ggplot2::theme_void()
```

Neat. In a real-world forest canopy tree crowns overlap, for example, where shorter trees are partially or entirely under the crown of taller trees. We want to ensure that we are allowing for this overlap because it represents reality. This will penalize the 2D crown-delineation process implemented in `cloud2trees` during evaluation since it does not capture this overlap. There are overlapping crowns in this plot and our 2D crown polygon representation that we'll use for instance segmentation evaluation appropriately reflects reality (darker green areas show individual tree crowns overlapping)

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Example validation process{#ex_valid2}

Puliti et al. [(2023, p. 14)](https://doi.org/10.48550/arXiv.2309.01279) provide guidance on performing "instance segmentation evaluation" whereby we assess the ability of our method to correctly identify and delineate individual trees

>The first step in performing any instance segmentation evaluation is to match the point cloud reference and predicted instance IDs. To do this, it is necessary to define whether a prediction is correct. Here we adopt a method Wielgosz et al. (2023) proposed for matching tree instance IDs from two point clouds with reference and predicted instance IDs, respectively. Given a reference and a predicted point cloud with two separate sets of instances, the tree instances are iteratively matched in descending order from the tallest to the shortest trees by using the following algorithm:

a) Find the tallest tree in the reference data;
b) Find the tree in the predicted instances that have the largest intersection over union (IoU) with the tree selected in the previous step;
c) if the IoU is <0.5: the predicted tree is considered an error, and thus no reference instance ID is available;
d) if the IoU is ≥0.5: the tree is considered a correct match, and assign reference instance ID label to the predicted tree;
e) Add to collection (dictionary) of predicted tree instances with IDs matching the reference instance IDs.

Following the initial matching of reference and predicted instance IDs, the evaluation can be done on the tree level to evaluate detection, omission, and commission rates (which can be used to calculate precision, recall, and the F-score metric)

#### `cloud2trees::cloud2trees()`

for our example, we'll use all default settings in `cloud2trees::cloud2trees()` to get a point cloud-detected tree list

```{r, warning=FALSE, message=FALSE, results=F}
lidar_df_row_temp <-
  fi_lidar_df %>% 
  dplyr::mutate(
    rn = ifelse(
      data_type=="training"
      ,dplyr::row_number()
      ,NA
  )) %>% 
  dplyr::filter(!is.na(rn)) %>% 
  dplyr::slice(2) %>% 
  dplyr::pull(rn)
```

look at the point cloud

```{r, rgl=T}
fi_lidar_df %>% 
  dplyr::slice(lidar_df_row_temp) %>% 
  dplyr::pull(f_path) %>% 
  lidR::readLAS() %>% 
  lidR::filter_poi(!is.na(treeID) & treeID != 0) %>% 
  lidR::plot(color = "treeID", bg = "white", legend = F)
```

extract trees using our `cloud2trees::cloud2trees()` process

```{r, warning=FALSE, message=FALSE, results=F}
# c2t
ans <- cloud2trees::cloud2trees(
  input_las_dir = fi_lidar_df$f_path[lidar_df_row_temp]
  , output_dir = tempdir()
)
# simplify multipolygon crowns
ans_crowns <- cloud2trees::simplify_multipolygon_crowns(ans$crowns_sf)
```

check out our extracted trees on the CHM

```{r}
ggplot2::ggplot() +
  ggplot2::geom_tile(
    data = ans$chm_rast %>%
      terra::as.data.frame(xy=T) %>%
      dplyr::rename(f=3)
    , mapping = ggplot2::aes(x = x, y = y, fill = f)
    , na.rm = T
  ) +
  harrypotter::scale_fill_hp(
    option = "gryffindor"
    , breaks = scales::breaks_extended(n=10)
  ) +
  ggplot2::geom_sf(
    data = ans_crowns
    , fill = NA, color = "gray44", lwd = 1.2
  ) +
  ggplot2::labs(x = "", y = "", fill = "CHM (m)") +
  ggplot2::theme_void() +
  ggplot2::theme(axis.text = ggplot2::element_blank())
```

#### Comparison with ground truth

check out the crown polygons from the ground truth data and we'll match the resolution to the `cloud2trees::cloud2trees()` setting

```{r}
ground_truth_temp <- fi_lidar_df %>% 
  dplyr::slice(lidar_df_row_temp) %>% 
  dplyr::pull(f_path) %>% 
  lidR::readLAS() %>% 
  fi_las_to_poly(res = 0.25,smooth = T)
# plot it
ggplot2::ggplot() +
  ggplot2::geom_sf(data = ground_truth_temp$mask, mapping = ggplot2::aes(fill = "mask"), color = NA,alpha=0.9) +
  ggplot2::geom_sf(data = ground_truth_temp$crowns, mapping = ggplot2::aes(fill = "crowns"), color = NA,alpha=0.9) +
  ggplot2::scale_fill_manual(values = c("forestgreen","gray11"), name = "") +
  ggplot2::theme_void()
```

we'll implement a rule to remove the entire crown from our predicted data if >= 70% of it's area is masked

```{r}
# remove trees with >70% overlap with mask
ans_crowns_no_mask <-
  ans_crowns %>% 
  dplyr::left_join(
    # calculate area within mask
    ans_crowns %>% 
      dplyr::select(treeID) %>% 
      sf::st_intersection(ground_truth_temp$mask) %>% 
      dplyr::mutate(mask_area = sf::st_area(.) %>% as.numeric()) %>% 
      sf::st_drop_geometry() %>% 
      dplyr::select(treeID,mask_area)
    , by = "treeID"
  ) %>% 
  dplyr::filter(
    ( dplyr::coalesce(mask_area,0)/as.numeric(sf::st_area(.)) ) < 0.7
  ) %>% 
  dplyr::select(-mask_area)
```

here are the predicted trees we are left to evaluate compared to the ground truth and the mask

```{r}
# plot it
ggplot2::ggplot() +
  ggplot2::geom_sf(data = ground_truth_temp$mask, mapping = ggplot2::aes(fill = "mask"), color = NA,alpha=0.9) +
  ggplot2::geom_sf(data = ground_truth_temp$crowns, mapping = ggplot2::aes(fill = "crowns"), color = NA,alpha=0.9) +
  ggplot2::geom_sf(
    data = ans_crowns %>% 
      dplyr::anti_join(
        ans_crowns_no_mask %>% 
          sf::st_drop_geometry() %>% 
          dplyr::select(treeID)
        , by = "treeID"
      )
    , mapping = ggplot2::aes(fill = "prediction\nremoved")
    , alpha = 0
    , color = "plum", lwd = 1
  ) +
  ggplot2::geom_sf(
    data = ans_crowns_no_mask
    , mapping = ggplot2::aes(fill = "prediction")
    , alpha = 0
    , color = "purple", lwd = 1
  ) +
  ggplot2::scale_fill_manual(values = c("forestgreen","gray11","purple","plum"), name = "") +
  ggplot2::theme_void()
```

#### Prediction vs ground truth matching

Given a reference and a predicted point cloud with two separate sets of instances, the tree instances are iteratively matched in descending order from the tallest to the shortest trees by using the following algorithm:

a) Find the tallest tree in the reference data;
b) Find the tree in the predicted instances that have the largest intersection over union (IoU) with the tree selected in the previous step;
c) if the IoU is <0.5: the predicted tree is considered an error, and thus no reference instance ID is available;
d) if the IoU is ≥0.5: the tree is considered a correct match, and assign reference instance ID label to the predicted tree;
e) Add to collection (dictionary) of predicted tree instances with IDs matching the reference instance IDs.

to find the tallest tree in the data we need to bring in the tree attributes data

```{r}
ground_truth_temp$crowns <- ground_truth_temp$crowns %>% 
  # add in tree attribute data
  dplyr::left_join(
    # read tree data
    fi_lidar_df %>% 
      dplyr::slice(lidar_df_row_temp) %>% 
      dplyr::pull(tree_data) %>% 
      readr::read_csv(show_col_types = F, progress = F) %>% 
      # filter for just the current plot
      dplyr::filter(
        plotID == fi_lidar_df %>% 
          dplyr::slice(lidar_df_row_temp) %>% 
          dplyr::pull(plotID)
      ) %>% 
      dplyr::rename_with(.fn = tolower, .cols = -c(plotID,treeID))
    , by = "treeID"
  )
# what tree attributes are there?
ground_truth_temp$crowns %>% dplyr::glimpse()
```

looks like there is only DBH so we'll use the `gt_tree_height` data calculated from the point cloud using our `fi_las_to_poly()` for the matching algorithm above. We calculated ground truth tree height in the For-Instance benchmark (`gt_tree_height`) as the difference between the highest and lowest point for each tree instance following Wielgosz et al. ([2024](https://doi.org/10.1016/j.rse.2024.114367)).

first, we'll define a function to attempt a match with a single ground truth tree

```{r}
ground_truth_single_tree_match <- function(gt_tree, predictions, min_iou_pct = 0.5) {
  if( !inherits(gt_tree,"sf") || nrow(gt_tree)!=1 ){stop("ground truth tree must be spatial `sf` data with a single record")}
  if( !inherits(predictions,"sf") ){stop("predictions must be spatial `sf` data")}
  if( !(names(gt_tree) %>% stringr::str_equal("treeID") %>% any()) ){stop("ground truth tree data must contain `treeID` column")}
  if( names(predictions) %>% stringr::str_equal("treeID") %>% any() ){
    predictions <- predictions %>% dplyr::rename(treeID_pred = treeID)
  }
  if( !(names(predictions) %>% stringr::str_equal("treeID_pred") %>% any()) ){
    stop("predictions tree data must contain `treeID` or `treeID_pred` column")
  }
  
  # intersection
    i_temp <-
      sf::st_intersection(predictions,gt_tree) %>% 
      dplyr::mutate(i_area = sf::st_area(.) %>% as.numeric())
    if(nrow(i_temp)==0){return(NULL)}
  # union
    u_temp <-
      i_temp %>% 
      sf::st_drop_geometry() %>% 
      dplyr::inner_join(predictions %>% sf::st_set_geometry("geom1"), by = "treeID_pred") %>% 
      dplyr::inner_join(gt_tree %>% sf::st_set_geometry("geom2") %>% dplyr::select(treeID), by = "treeID") %>% 
      dplyr::rowwise() %>% 
      dplyr::mutate(
        u_area = sf::st_union(geom1, geom2) %>% sf::st_area() %>% as.numeric()
      ) %>% 
      dplyr::ungroup() %>% 
      dplyr::select(-c(geom1,geom2)) %>% 
      sf::st_drop_geometry() %>% 
      dplyr::mutate(
        iou = dplyr::case_when(
          is.na(u_area) | is.nan(u_area) ~ NA
          , T ~ dplyr::coalesce(i_area,0)/u_area
        )
      ) %>% 
      dplyr::filter(!is.na(iou) & iou>=min_iou_pct)
    if(nrow(u_temp)==0){return(NULL)}
  # return
  return(
    u_temp %>% 
      # pick the highest iou
      dplyr::arrange(desc(iou)) %>% 
      dplyr::slice(1) %>% 
      # column clean
      dplyr::select(
        treeID, i_area, u_area, iou
        , base::setdiff( 
          names(predictions)
          , c("geometry", "geom", "treeID", "i_area", "u_area", "iou") 
        )
      )
  )
}
```

test for a single ground truth tree

```{r}
ground_truth_single_tree_match(
  gt_tree = ground_truth_temp$crowns %>% dplyr::arrange(desc(gt_tree_height)) %>% dplyr::slice(2)
  , predictions = ans_crowns_no_mask %>% dplyr::select(treeID)
)
```

now we need to make a function that takes reference and predicted tree crown polygons, matches the trees iteratively allowing for only a single match from the predictions (i.e. one prediction can only go to one ground truth tree), and returns all trees labeled as true positive (correctly matched with a prediction), commission (predictions which do not match a ground truth tree; false positive), or omission (ground truth trees for which no predictions match; false negative)

```{r}
ground_truth_crown_match <- function(
  # ground_truth should be sorted already
  ground_truth
  # predictions just needs treeID
  , predictions
  , min_iou_pct = 0.5
  , calculate_height_diff = F
) {
  # checks
  if( !inherits(ground_truth,"sf") || nrow(ground_truth)==0 ){stop("ground truth must be spatial `sf` data")}
  if( !inherits(predictions,"sf") ){stop("predictions must be spatial `sf` data")}
  if( !(names(ground_truth) %>% stringr::str_equal("treeID") %>% any()) ){
    stop("ground truth tree data must contain `treeID` column")
  }
  if(calculate_height_diff && !(names(ground_truth) %>% stringr::str_equal("gt_tree_height") %>% any()) ){
    stop("ground truth tree data must contain `gt_tree_height` column if `calculate_height_diff=T`")
  }
  if(calculate_height_diff && !(names(predictions) %>% stringr::str_equal("tree_height_m") %>% any()) ){
    stop("ground truth tree data must contain `tree_height_m` column if `calculate_height_diff=T`")
  }
  if( !(names(predictions) %>% stringr::str_equal("treeID") %>% any()) ){
    stop("predictions tree data must contain `treeID` column")
  }else{
    predictions <- predictions %>% dplyr::rename(treeID_pred = treeID)
  }
  
  # set up a blank data frame
  return_df <-
    dplyr::tibble(
      i_area = as.numeric(NA)
      , u_area = as.numeric(NA)
      , iou = as.numeric(NA)
    ) %>% 
    dplyr::bind_cols(
      ground_truth %>% 
        sf::st_drop_geometry() %>% 
        dplyr::select(treeID) %>% 
        dplyr::slice(0)
    ) %>% 
    dplyr::bind_cols(
      predictions %>% 
        sf::st_drop_geometry() %>% 
        dplyr::select(treeID_pred) %>% 
        dplyr::slice(0)
    ) %>% 
    dplyr::relocate(tidyselect::starts_with("treeID"))
  
  # save names to select
  nms_temp <- names(return_df)
  
  # start with tallest tree and match to get true positives
    for (i in 1:nrow(ground_truth)) {
      match_temp <- ground_truth_single_tree_match(
        gt_tree = ground_truth %>% dplyr::slice(i)
        , predictions = predictions %>% dplyr::select(treeID_pred) %>% dplyr::anti_join(return_df,by="treeID_pred")
        , min_iou_pct = min_iou_pct
      )
      # add to return
      if(!is.null(match_temp)){
        return_df <- return_df %>% 
          dplyr::bind_rows(match_temp %>% dplyr::select(nms_temp))
      }
      match_temp <- NULL
    }
  # label tps
  return_df <- return_df %>% 
    dplyr::mutate(match_grp = "true positive")
  # add omissions
  return_df <-
    return_df %>% 
    dplyr::bind_rows(
      ground_truth %>% 
        sf::st_drop_geometry() %>% 
        dplyr::anti_join(return_df,by="treeID") %>% 
        dplyr::select(treeID) %>% 
        dplyr::mutate(match_grp = "omission")
    )
  # add commissions
  return_df <-
    return_df %>% 
    dplyr::bind_rows(
      predictions %>% 
        sf::st_drop_geometry() %>% 
        dplyr::anti_join(return_df,by="treeID_pred") %>% 
        dplyr::select(treeID_pred) %>% 
        dplyr::mutate(match_grp = "commission")
    )
  # make match_grp factor
  return_df <- return_df %>% 
    dplyr::mutate(
      match_grp = factor(
        match_grp
        , ordered = T
        , levels = c(
          "true positive"
          , "commission"
          , "omission"
        )
      ) %>% forcats::fct_rev()
    )
  
  # calculate height difference for matched trees
  if(calculate_height_diff){
    return_df <- return_df %>% 
      dplyr::left_join(
        ground_truth %>% 
          sf::st_drop_geometry() %>% 
          dplyr::select(treeID,gt_tree_height)
        , by = "treeID"
      ) %>% 
      dplyr::left_join(
        predictions %>% 
          sf::st_drop_geometry() %>% 
          dplyr::select(treeID_pred,tree_height_m)
        , by="treeID_pred"
      ) %>% 
      dplyr::mutate(height_diff = gt_tree_height-tree_height_m)
  }
  # return
  if(nrow(return_df)==0){
    warning("no records found for ground truth to predition matching")
    return(NULL)
  }else{
    return(return_df)
  }
  
}
```

test the tree matching process on the full data

```{r}
ground_truth_crown_match_ans <-
  ground_truth_crown_match(
    # ground_truth should be sorted already
    ground_truth = ground_truth_temp$crowns %>% dplyr::arrange(desc(gt_tree_height))
    # predictions just needs treeID
    , predictions = ans_crowns_no_mask %>% dplyr::select(treeID,tree_height_m)
    , min_iou_pct = 0.5
    , calculate_height_diff = T
  )
```

how did our predictions do for this particular example?

```{r}
# what did we get?
ground_truth_crown_match_ans %>% 
  dplyr::count(match_grp) %>% 
  dplyr::mutate(pct = (n/sum(n)) %>% scales::percent(accuracy=0.1))
```

let's look at that spatially

```{r}
pal_match_grp = c(
  "omission"=viridis::cividis(3)[1]
  , "commission"=viridis::cividis(3)[2]
  , "true positive"=viridis::cividis(3)[3]
)
# plot it
ggplot2::ggplot() +
  ggplot2::geom_sf(
    data = 
      ground_truth_temp$crowns %>% 
        dplyr::left_join(
          ground_truth_crown_match_ans %>% 
            dplyr::select(treeID,match_grp)
          , by = "treeID"
        )
    , mapping = ggplot2::aes(fill = match_grp)
    , color = NA ,alpha=0.7
  ) + 
  ggplot2::geom_sf(
    data =
      ans_crowns_no_mask %>%
        dplyr::left_join(
          ground_truth_crown_match_ans %>%
            dplyr::select(treeID_pred,match_grp)
          , by = dplyr::join_by(treeID==treeID_pred)
        )
    , mapping = ggplot2::aes(fill = match_grp, color = match_grp)
    , alpha = 0
    , lwd = 1.2
  ) +
  ggplot2::scale_fill_manual(values = pal_match_grp, name = "") +
  ggplot2::scale_color_manual(values = pal_match_grp, name = "") +
  ggplot2::theme_void() +
  ggplot2::guides(
    fill = ggplot2::guide_legend(override.aes = list(color = c(NA,NA,viridis::cividis(3)[2])))
    , color = "none"
  )
```

Note the predicted crowns classified as commission errors that encompass multiple ground truth tree crowns which are counted as omission errors. Even though the predicted crowns cover at least on ground truth crowns, these instances are classified as such due to the criteria that "if the IoU is <0.5: the predicted tree is considered an error" and thus no ground truth instance is matched

let's quickly look at the IoU values on the true positive trees

```{r}
ground_truth_crown_match_ans %>% dplyr::select(iou) %>% summary()
```

Lastly, we'll make a function to aggregate our plot-level scores to evaluate omission rate (false negative rate or miss rate), commission rate (false positive rate), precision, recall (detection rate), and the F-score metric. As a reminder, true positive ($TP$) trees correctly match ground truth trees with a prediction, commission tree predictions do not match a ground truth tree (false positive; $FP$), and omissions are ground truth trees for which no predictions match (false negative; $FN$)

$$\textrm{omission rate} = \frac{FN}{TP+FN}$$

$$\textrm{commission rate} = \frac{FP}{TP+FP}$$

$$\textrm{precision} = \frac{TP}{TP+FP}$$

$$\textrm{recall} = \frac{TP}{TP+FN}$$

$$
\textrm{F-score} = 2 \times \frac{\bigl(precision \times recall \bigr)}{\bigl(precision + recall \bigr)}
$$

we'll also automatically calculate root mean square error (RMSE) for the tree height in meters above ground if the we detect the `height_diff` column. Inclusion of tree height RMSE represents a complementary measure to the tree detection rates that captures the ability of the model to properly extract the tree tops. 

tree height RMSE is calculated as 

$$
\textrm{RMSE} = \sqrt{ \frac{  \sum_{i=1}^{N} (y_{i} - \hat{y_{i}})^{2}}{N}}
$$

Where $N$ is equal to the total number of trees either in a FOR-instance collection or in the whole dataset, $y_i$ is the reference height value and $\hat{y_i}$ is the predicted tree height value of tree $i$ in meters above ground (m).

```{r}
agg_ground_truth_crown_match <- function(ground_truth_crown_match_ans) {
  if(nrow(ground_truth_crown_match_ans)==0){return(NULL)}
  if( !(names(ground_truth_crown_match_ans) %>% stringr::str_equal("match_grp") %>% any()) ){stop("ground_truth_crown_match_ans must contain `match_grp` column")}
  # count by match group
  agg <- ground_truth_crown_match_ans %>% 
    dplyr::count(match_grp) %>% 
    dplyr::mutate(
      match_grp = dplyr::case_match(
        match_grp
        , "true positive"~"tp"
        , "commission"~"fp"
        , "omission"~"fn"
      )
    )
  # true positive, false positive, false negative rates
  return_df <- dplyr::tibble(match_grp = c("tp","fp","fn")) %>% 
    dplyr::left_join(agg, by = "match_grp") %>% 
    dplyr::mutate(dplyr::across(.cols = c(n), .fn = ~dplyr::coalesce(.x,0))) %>% 
    tidyr::pivot_wider(
      names_from = match_grp
      , values_from = c(n)
      , names_glue = "{match_grp}_{.value}"
    )
  # rates, precision, recall, f-score
  return_df <- return_df %>% 
    dplyr::mutate(
      omission_rate = dplyr::case_when(
        (tp_n+fn_n) == 0  ~ as.numeric(NA)
        , T ~ fn_n/(tp_n+fn_n)
      ) # False Negative Rate or Miss Rate
      , commission_rate = dplyr::case_when(
        (tp_n+fp_n) == 0  ~ as.numeric(NA)
        , T ~ fp_n/(tp_n+fp_n)
      ) # False Positive Rate
      , precision = dplyr::case_when(
        (tp_n+fp_n) == 0  ~ as.numeric(NA)
        , T ~ tp_n/(tp_n+fp_n)
      )
      , recall = dplyr::case_when(
        (tp_n+fn_n) == 0  ~ as.numeric(NA)
        , T ~ tp_n/(tp_n+fn_n)
      )
      , f_score = dplyr::case_when(
        is.na(precision) | is.na(recall) ~ as.numeric(NA)
        , (precision+recall) == 0 ~ 0
        , T ~ 2 * ( (precision*recall)/(precision+recall) )
      )    
    )
  # height rmse
  if( names(ground_truth_crown_match_ans) %>% stringr::str_equal("height_diff") %>% any() ){
    return_df <- return_df %>% 
      dplyr::bind_cols(
        ground_truth_crown_match_ans %>% 
        dplyr::ungroup() %>% 
        dplyr::summarise(
          height_diff_sq = sum(height_diff^2, na.rm = T)
          , n = sum(ifelse(!is.na(height_diff),1,0))
        ) %>% 
        dplyr::mutate(
          rmse_height_m = dplyr::case_when(
            dplyr::coalesce(n,0)==0 ~ as.numeric(NA)
            , T ~ sqrt(height_diff_sq/n)
          )
        ) %>% 
        dplyr::select(rmse_height_m)
      )
  }
  # return
  return(return_df)
}
```

let's look at our score summary for this plot

```{r}
agg_ground_truth_crown_match(ground_truth_crown_match_ans)
```

Finally, let's compile this process into a function

```{r}
fi_evaluate <- function(
  predictions
  , las_f_path
  , remove_mask_overlap_pct = 0.7
  , min_iou_pct = 0.5
  , calculate_height_diff = F
  , force_same_crs = F
) {
  # checks, no balances
  if(!file.exists(las_f_path)){stop(paste0("could not locate point cloud file at\n      ",las_f_path))}
  if( !(names(predictions) %>% stringr::str_equal("treeID") %>% any()) ){
    stop("predictions tree data must contain `treeID` or `treeID` column")
  }
  # get ground truth polygons
  ground_truth_temp <- lidR::readLAS(las_f_path) %>% 
    fi_las_to_poly(res = 0.25,smooth = T) # hard-coded
  
  # force same crs
  if(force_same_crs){
    predictions <- predictions %>% sf::st_set_crs(sf::st_crs(ground_truth_temp$mask))
  }
  
  # remove predictions trees with >70% overlap with mask
  ans_crowns_no_mask <-
    predictions %>% 
    dplyr::left_join(
      # calculate area within mask
      predictions %>% 
        dplyr::select(treeID) %>% 
        sf::st_intersection(ground_truth_temp$mask) %>% 
        dplyr::mutate(mask_area = sf::st_area(.) %>% as.numeric()) %>% 
        sf::st_drop_geometry() %>% 
        dplyr::select(treeID,mask_area)
      , by = "treeID"
    ) %>% 
    dplyr::filter(
      ( dplyr::coalesce(mask_area,0)/as.numeric(sf::st_area(.)) ) < remove_mask_overlap_pct
    ) %>% 
    dplyr::select(-mask_area)
  
  # tree matching process on the full data
  ground_truth_crown_match_ans <- ground_truth_crown_match(
    # ground_truth should be sorted already
    ground_truth = ground_truth_temp$crowns %>% 
      dplyr::arrange(desc(gt_tree_height)) # column calculated in fi_las_to_poly()
    , predictions = ans_crowns_no_mask
    , min_iou_pct = min_iou_pct
    , calculate_height_diff = calculate_height_diff
  )
  
  # plot-level score
  plot_score <- agg_ground_truth_crown_match(ground_truth_crown_match_ans)
  
  # return
  return(list(
    plot_score = plot_score
    , ground_truth_crown_match = ground_truth_crown_match_ans
    , ground_truth_crowns = ground_truth_temp$crowns
  ))
}
```

test it

```{r}
fi_evaluate_ans_temp <- fi_evaluate(
  predictions = ans_crowns
  , las_f_path = fi_lidar_df %>% dplyr::slice(lidar_df_row_temp) %>% dplyr::pull(f_path)
  , calculate_height_diff = T
  , force_same_crs = T
)
# fi_evaluate_ans_temp
fi_evaluate_ans_temp$plot_score
```

plot the test

```{r}
# plot it
ggplot2::ggplot() +
  ggplot2::geom_sf(
    data = 
      fi_evaluate_ans_temp$ground_truth_crowns %>% 
        dplyr::left_join(
          fi_evaluate_ans_temp$ground_truth_crown_match %>% 
            dplyr::select(treeID,match_grp)
          , by = "treeID"
        )
    , mapping = ggplot2::aes(fill = match_grp)
    , color = NA ,alpha=0.7
  ) + 
  ggplot2::geom_sf(
    data =
      ans_crowns %>% 
        dplyr::inner_join(
          fi_evaluate_ans_temp$ground_truth_crown_match %>% 
            dplyr::select(treeID_pred,match_grp)
          , by = dplyr::join_by(treeID==treeID_pred)
        ) %>% 
        sf::st_set_crs(sf::st_crs(fi_evaluate_ans_temp$ground_truth_crowns))
    , mapping = ggplot2::aes(fill = match_grp, color = match_grp)
    , alpha = 0
    , lwd = 1.2
  ) +
  ggplot2::scale_fill_manual(values = pal_match_grp, name = "") +
  ggplot2::scale_color_manual(values = pal_match_grp, name = "") +
  ggplot2::theme_void() +
  ggplot2::guides(
    fill = ggplot2::guide_legend(override.aes = list(color = c(NA,NA,viridis::cividis(3)[2])))
    , color = "none"
  )
```

works

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(m,r3dDefaults,ans_crowns,ans_crowns_no_mask,ans,ground_truth_crown_match_ans)
gc()
```

### Training validation process

Now that we have been through the validation process for a [single sample plot](#ex_valid2) and [defined different variable window functions](#itd_more) for use in ITD, we'll detect trees and delineate tree crowns using our `cloud2trees::cloud2trees()` [method](#xtract) to apply over each training lidar data set.

We'll extract trees on each training data set using all `r length(my_ws_functions)` [variable window functions](#itd_more), compare our point cloud-detected trees to the ground truth trees, and store the best variable window function by study site for use in validation on the evaluation lidar data.

#### Extract trees for training data

extract trees from the training lidar data using each variable window function

```{r, results=FALSE, warning=FALSE, message=FALSE}
# where should we save the file?
submission_fn_temp <- "../data/ForInstance_training_submission.gpkg"
# if we don't already have the data, run it
if(!file.exists(submission_fn_temp)){
# if(T){ ### !!!!!!! take out when it gets real
  # just get training
  training_df_temp <- fi_lidar_df %>% 
    dplyr::filter(data_type == "training")
  # get trees for each training plot and ws fn combination
  fi_training_submission <- 
    1:nrow(training_df_temp) %>% 
    purrr::map(\(x) 
      ws_fn_eval(
        lidar_df = training_df_temp 
        , lidar_df_row = x
        , ws_fn_list = my_ws_functions
      )
    ) %>% 
    dplyr::bind_rows()
  # generate treeID
  fi_training_submission <- fi_training_submission %>% 
    dplyr::group_by(plot_name,ws_fn_nm) %>% 
    dplyr::mutate(treeID = dplyr::row_number()) %>% 
    dplyr::ungroup()
  # save it 
  sf::st_write(fi_training_submission, submission_fn_temp, append = F)
}else{
  fi_training_submission <- sf::st_read(submission_fn_temp, quiet = T)
}
```

what did we get?

```{r}
fi_training_submission %>% dplyr::glimpse()
```

tabulate detected trees by plot and variable window function and plot it

```{r}
fi_training_submission %>% 
  sf::st_drop_geometry() %>% 
  dplyr::count(plot_name,ws_fn_nm) %>% 
  ggplot2::ggplot(ggplot2::aes(x = n, y = plot_name, color = ws_fn_nm)) +
    ggplot2::geom_point(size = 5) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::scale_x_log10(labels=scales::comma) +
    ggplot2::labs(
      x = "# trees", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Training data: number of detected canopy trees"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.y = ggplot2::element_text(size=8)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

#### Training: validation against ground truth

We'll compare our aerial point cloud-based tree detection and crown delineation results to the ground truth using the steps we outlined in the [example above](#ex_valid2).

```{r, warning=FALSE,message=FALSE,results='hide'}
# where should we save the file?
training_eval_temp <- "../data/ForInstance_training_evaluate.csv"
training_eval_m_temp <- "../data/ForInstance_training_evaluate_crown_match.csv"
training_eval_c_temp <- "../data/ForInstance_training_evaluate_gt_crowns.gpkg"
# if we don't already have the data, run it
if(
  !file.exists(training_eval_temp) ||
  !file.exists(training_eval_m_temp) ||
  !file.exists(training_eval_c_temp)
){
# if(T){
  # get unique plot_name,ws_fn_nm
  combs_temp <-
    fi_training_submission %>% 
    sf::st_drop_geometry() %>% 
    dplyr::distinct(plot_name,ws_fn_nm)
  
  # safe it
  safe_fi_evaluate <- purrr::safely(fi_evaluate)
  
  # set up empty sets
  training_plot_score_df <- NULL
  training_ground_truth_crown_match_df <- NULL
  training_ground_truth_crowns_df <- NULL
  
  # we'll just loop this validation
  for (i in 1:nrow(combs_temp)) {
    # validate
    fi_evaluate_ans_temp <- safe_fi_evaluate(
      predictions = 
        fi_training_submission %>% 
        dplyr::filter(
          plot_name == combs_temp$plot_name[i]
          & ws_fn_nm == combs_temp$ws_fn_nm[i]
        )
      , las_f_path = fi_lidar_df %>% dplyr::filter(plot_name==combs_temp$plot_name[i]) %>% dplyr::pull(f_path)
      , remove_mask_overlap_pct = 0.7
      , min_iou_pct = 0.5
      , calculate_height_diff = T
      , force_same_crs = T
    )
    # populate results if available
    if(is.null(fi_evaluate_ans_temp$error)){
      # training_plot_score_df
      training_plot_score_df <- training_plot_score_df %>% 
        dplyr::bind_rows(
          fi_evaluate_ans_temp$result$plot_score %>% 
            dplyr::mutate(
              plot_name = combs_temp$plot_name[i]
              , ws_fn_nm = combs_temp$ws_fn_nm[i]
            )
        )
      # training_ground_truth_crown_match_df
      training_ground_truth_crown_match_df <- training_ground_truth_crown_match_df %>% 
        dplyr::bind_rows(
          fi_evaluate_ans_temp$result$ground_truth_crown_match %>% 
            dplyr::mutate(
              plot_name = combs_temp$plot_name[i]
              , ws_fn_nm = combs_temp$ws_fn_nm[i]
            )
        )
      # training_ground_truth_crowns_df
      epsg_temp <- fi_evaluate_ans_temp$result$ground_truth_crowns %>% 
        sf::st_crs(parameters=T) %>% 
        purrr::pluck("epsg")
      training_ground_truth_crowns_df <- training_ground_truth_crowns_df %>% 
        dplyr::bind_rows(
          fi_evaluate_ans_temp$result$ground_truth_crowns %>% 
            dplyr::mutate(
              plot_name = combs_temp$plot_name[i]
              , ws_fn_nm = combs_temp$ws_fn_nm[i]
              , epsg = ifelse(identical(epsg_temp,character(0)),as.numeric(NA),as.numeric(epsg_temp))
            ) %>% 
            sf::st_set_crs(NA)
        )
    }
    fi_evaluate_ans_temp <- NULL
  }
  
  # save
  readr::write_csv(training_plot_score_df, file = training_eval_temp)
  readr::write_csv(training_ground_truth_crown_match_df, file = training_eval_m_temp)
  sf::st_write(training_ground_truth_crowns_df, training_eval_c_temp, append = F)
}else{
  training_plot_score_df <- readr::read_csv(training_eval_temp, progress = F, show_col_types = F)
  training_ground_truth_crown_match_df <- readr::read_csv(training_eval_m_temp, progress = F, show_col_types = F)
  training_ground_truth_crowns_df <- sf::st_read(training_eval_c_temp, quiet = T)
}
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

plot precision, recall, and F-score by plot

```{r}
training_plot_score_df %>% 
  tidyr::pivot_longer(cols = c(precision,recall,f_score)) %>% 
  dplyr::mutate(
    name = ifelse(name=="f_score","F-score",name) %>% forcats::fct_rev()
    , plot_name = plot_name %>% forcats::fct_rev()
  ) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = plot_name, color = ws_fn_nm)) +
    # ggplot2::geom_point(size = 4) +
    ggplot2::geom_jitter(height = 0.12, width = 0, size = 3) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::labs(
      x = "", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Training data: validation against image annotated crowns ground truth"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.y = ggplot2::element_text(size=8)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    )
```

we'll quickly compare the variable window functions by averaging across sites and making a "quadrant" plot whereby we overlay the median value to differentiate the "best" window size by the two dimensions (precision, recall)...the upper-right quadrant is the "best" of the variable window functions tested based on these training plots evaluated against image annotated crowns

```{r}
agg_pr_temp <- training_plot_score_df %>% 
  dplyr::group_by(ws_fn_nm) %>% 
  dplyr::summarise(dplyr::across(dplyr::where(is.numeric),mean))
agg_pr_temp %>% 
  ggplot2::ggplot(ggplot2::aes(x=precision,y=recall,color=ws_fn_nm)) +
  ggplot2::geom_hline(yintercept = median(agg_pr_temp$recall,na.rm = T),linetype="dashed") +
  ggplot2::geom_vline(xintercept = median(agg_pr_temp$precision,na.rm = T),linetype="dashed") +
  ggplot2::geom_point(size = 6) +
  ggplot2::scale_color_manual(values = pal_ws) +
  # ggplot2::scale_x_continuous(limits = c(0,0.5)) +
  # ggplot2::scale_y_continuous(limits = c(0,0.5)) +
  ggplot2::labs(
    color = "variable\nwindow\nfunction"
    , subtitle = "Training data: validation against image annotated crowns ground truth"
    , caption = "*values are averaged across sites and plots"
  ) +
  ggplot2::theme_light() +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

##### Best variable window function by site

we collapsed across plot to calculate an average precision, recall, and F-score for each site and variable window function combination

As a reminder, we limited our evaluation to only sites with conifer trees: **NIBIO**, **CULS**, and **SCION**

* **NIBIO**: Coniferous dominated boreal forest study site in Norway composed of mature forests dominated by Norway spruce (*Picea abies*) and with few Scots pine (*Pinus Sylvestris*) and deciduous species (mainly *Betula* sp.).
* **CULS**: Coniferous dominated temperate forest study site in the Czech north of Prague with forested plots consisting of productive even-aged Scots pine forest
* **SCION**: Coniferous non-native temperate study site in New Zealand consisting of pure Monterey pine (*Pinus radiata*) of 18-20 years of age and is representative of a typical productive plantation forest

first, we'll collapse across plot to calculate the site-level metrics for each variable window function 

```{r}
# site eval results
fi_training_eval_by_site <-
  training_plot_score_df %>% 
  dplyr::inner_join(
    fi_lidar_df %>% dplyr::distinct(plot_name,siteID)
    , by="plot_name"
  ) %>% 
  dplyr::group_by(siteID,ws_fn_nm) %>% 
  dplyr::summarise(
    precision = mean(precision, na.rm = T)
    , recall = mean(recall, na.rm = T)
    , omission_rate = mean(omission_rate, na.rm = T)
    , commission_rate = mean(commission_rate, na.rm = T)
    , rmse_height_m = mean(rmse_height_m, na.rm = T)
    , rmse_height_m_weighted_mean = sum(rmse_height_m*tp_n, na.rm = T)/sum(tp_n, na.rm = T)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    dplyr::across(
      .cols = c(precision,recall,tidyselect::ends_with("_rate"),tidyselect::starts_with("rmse_"))
      , ~ifelse(is.nan(.x),as.numeric(NA),.x)
    )
    , f_score = dplyr::case_when(
      is.na(precision) | is.na(recall) ~ as.numeric(NA)
      , (precision+recall) == 0 ~ 0
      , T ~ 2 * ( (precision*recall)/(precision+recall) )
    )
  ) %>% 
  dplyr::arrange(siteID,desc(f_score),desc(precision),desc(recall),omission_rate,commission_rate) %>% 
  dplyr::group_by(siteID) %>% 
  dplyr::mutate(is_best_fn = dplyr::row_number()==1) %>% 
  dplyr::ungroup()
# save it after cleaning
readr::write_csv(fi_training_eval_by_site, "../data/ForInstance_training_eval_by_site.csv", append = F, progress = F)
```

show the best window function by site 

```{r}
fi_training_eval_by_site %>% 
  tidyr::pivot_longer(cols = c(precision,recall,f_score)) %>% 
  dplyr::mutate(
    name = ifelse(name=="f_score","F-score",name) %>% forcats::fct_rev()
    , siteID = forcats::fct_rev(siteID)
  ) %>% 
  dplyr::arrange(is_best_fn) %>% 
  ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = ws_fn_nm)) +
    # ggplot2::geom_point(size = 4) +
    ggplot2::geom_jitter(
      ggplot2::aes(shape = is_best_fn)
      ,height = 0.12
      , width = 0
      , size = 5
    ) +
    ggplot2::facet_grid(cols = dplyr::vars(name)) +
    ggplot2::scale_color_manual(values = pal_ws) +
    ggplot2::scale_shape_manual(values = c(20,15)) +
    ggplot2::scale_x_continuous(limits = c(NA,1.01)) +
    ggplot2::labs(
      x = "", y = ""
      , color = "variable\nwindow\nfunction"
      , subtitle = "Best variable window function by study site based on training data evaluation"
      , caption = "*a square indicates the best function"
    ) +
    ggplot2::theme_light() +
    ggplot2::theme(
      axis.text.x = ggplot2::element_text(size=7)
      , panel.grid.major.y = ggplot2::element_line(color="gray22")
      , strip.text = ggplot2::element_text(color = "black", size = 10)
    ) +
    ggplot2::guides(
      color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
      , shape = "none"
    )
```

table of the best function by site

```{r}
fi_training_eval_by_site %>% 
  dplyr::filter(is_best_fn) %>% 
  dplyr::select(siteID,ws_fn_nm, omission_rate,commission_rate,recall,precision,f_score) %>% 
  dplyr::arrange(siteID) %>% 
kableExtra::kbl(
    caption = "Best variable window function by study site based on training data evaluation"
    , digits = 2
    , col.names = c(
      "site","variable<br>window<br>function"
      , "omission<br>rate"
      , "commission<br>rate"
      ,"recall","precision","F-score"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

##### Best variable window function overall

to select the best overall variable window function to apply to NEON sites that lacked training data, we collapsed across all training sites and plots to calculate an average precision and recall at the variable window function level

we can look at the full results table

```{r}
# overall eval results
training_eval_overall <-
  training_eval_combined_temp %>% 
  dplyr::group_by(ws_fn_nm) %>% 
  dplyr::summarise(
    precision = mean(precision, na.rm = T)
    , recall = mean(recall, na.rm = T)
    , n_fld_cllctd_stems = sum(ifelse(eval_type == "fld_cllctd_stems",1,0))
    , n_img_annttd_crwns = sum(ifelse(eval_type == "img_annttd_crwns",1,0))
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    dplyr::across(.cols = c(precision,recall), ~ifelse(is.nan(.x),as.numeric(NA),.x))
    , f_score = dplyr::case_when(
      is.na(precision) | is.na(recall) ~ as.numeric(NA)
      , (precision+recall) == 0 ~ 0
      , T ~ 2 * ( (precision*recall)/(precision+recall) )
    )
  ) %>% 
  dplyr::arrange(desc(f_score),desc(precision),desc(recall),desc(n_fld_cllctd_stems),desc(n_img_annttd_crwns)) %>% 
  dplyr::mutate(is_best_fn = dplyr::row_number()==1)
# save it
readr::write_csv(training_eval_overall, "../data/NeonTreeEvaluation_training_eval_overall.csv", append = F, progress = F)
```

```{r}
training_eval_overall %>% 
  dplyr::select(-c(is_best_fn)) %>% 
  dplyr::relocate(tidyselect::starts_with("n_"), .after = dplyr::last_col()) %>% 
  kableExtra::kbl(
    caption = "Best variable window function overall based on training data evaluation"
    , digits = 2
    , col.names = c(
      "variable<br>window<br>function"
      ,"precision","recall","F-score"
      ,"plots<br>with ground truth<br>field collected stems"
      ,"plots<br>with ground truth<br>image ann. crowns"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling()
```

keep in mind that these are average values across plots and sites so the actual metric values matter little...it's the relative value that we care about

we can view this "quadrant" plot whereby we overlay the median value to differentiate the "best" window size by the two dimensions (precision, recall)...the upper-right quadrant is the "best" of the variable window functions tested based on these training plots evaluated against image annotated crowns and field collected stems ground truth data

```{r}
training_eval_overall %>% 
  ggplot2::ggplot(ggplot2::aes(x=precision,y=recall,color=ws_fn_nm)) +
  ggplot2::geom_hline(yintercept = median(training_eval_overall$recall),linetype="dashed") +
  ggplot2::geom_vline(xintercept = median(training_eval_overall$precision),linetype="dashed") +
  ggplot2::geom_point(ggplot2::aes(shape=is_best_fn),size = 6) +
  ggplot2::scale_color_manual(values = pal_ws) +
  ggplot2::scale_shape_manual(values = c(20,15)) +
  ggplot2::scale_x_continuous(limits = c(0,0.4)) +
  ggplot2::scale_y_continuous(limits = c(0,0.7)) +
  ggplot2::labs(
    color = "variable\nwindow\nfunction"
    , subtitle = "Best variable window function overall based on training data evaluation"
    , caption = "*a square indicates the best function"
  ) +
  ggplot2::theme_light() +
  ggplot2::guides(
    color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6))
    , shape = "none"
  )
```

now we'll attach the best variable window function to our processing data

```{r}
lidar_df <- lidar_df %>%
  dplyr::left_join(
    training_eval_by_site %>%
      dplyr::filter(is_best_fn) %>% 
      dplyr::select(siteID,ws_fn_nm)
    , by = "siteID"
  ) %>% 
  dplyr::mutate(
    ws_fn_nm = dplyr::coalesce(
      ws_fn_nm
      , training_eval_overall %>%
        dplyr::filter(is_best_fn) %>% 
        dplyr::pull(ws_fn_nm)
    )
  )
# save
readr::write_csv(lidar_df, "../data/NeonTreeEvaluation_lidar_df.csv", append = F, progress = F)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(
  training_submission, training_eval_by_site
  , training_eval_overall, training_fld_cllctd_stems
  , training_img_annttd_crwns
  , submission_fn
  , neontree_plot_fn, clean_field_data
  , m, r3dDefaults
)
gc()
```