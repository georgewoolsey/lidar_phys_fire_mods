[["index.html", "Aerial LiDAR for Fire Model Inputs Section 1 Introduction 1.1 Objective 1.2 Data", " Aerial LiDAR for Fire Model Inputs George Woolsey 11 March, 2025 Section 1 Introduction Code in support of “Using aerial LiDAR data for object-based physical fire modeling in conifer forests of the southwestern US” 1.1 Objective The objective of this study is to demonstrate the use of aerial LiDAR data to create inputs for physics-based fire models in frequent-fire forests of the southwestern United States. We review the methods used to extract tree location, species, and physical form from aerial LiDAR data. We evaluate this canopy crown detection methodology using a benchmark data set created to standardize evaluation metrics (Weinstein et al. 2021). We explain how to format this data for seamless integration with two commonly used object-based physical fire modeling tools. We demonstrate the end-to-end process using a case study from the southwestern United States. 1.2 Data Lidar data from the Mogollon Rim area of the Coconino National Forest about 20 km north of Payson, Arizona, USA was acquired from the USGS LidarExplorer. The aerial lidar data was collected between August 2013 and October 2014 under lidar project “AZ_USFS_3DEP_Processing_2019_D20” (project ID: 195122). "],["processing.html", "Section 2 Point Cloud Processing 2.1 Lidar Data Location 2.2 Individial Tree Detection Tuning: itd_tuning() 2.3 Point Cloud Tree Extraction: cloud2trees() 2.4 DBH Modeling: trees_dbh() 2.5 HMD Modeling: trees_hmd() 2.6 CBH Modeling: trees_cbh() 2.7 Forest Type: trees_type() 2.8 Combine data 2.9 Crown Biomass: trees_biomass() 2.10 Data Export", " Section 2 Point Cloud Processing In this section we’ll process the raw point cloud data using the cloud2trees R package developed to provide accessible routines for processing point cloud data collected by airborne lidar or generated using UAS imagery and photogrammetry (e.g. structure from motion). The cloud2trees package can be installed by following the directions listed in the README file on GitHub. If one is still experiencing difficulties installing the package, see the example.R file which details how to install the package using a virgin R instance. ## remotes helps us get packages hosted on github install.packages(&quot;remotes&quot;) ## get cloud2trees remotes::install_github(repo = &quot;georgewoolsey/cloud2trees&quot;, upgrade = F) Load the standard libraries we use to do work # bread-and-butter library(tidyverse) # the tidyverse library(viridis) # viridis colors library(harrypotter) # hp colors library(RColorBrewer) # brewer colors library(scales) # work with number and plot scales library(latex2exp) # visualization library(mapview) # interactive html maps library(kableExtra) # tables library(patchwork) # combine plots # spatial analysis library(terra) # raster library(sf) # simple features library(lidR) # lidar data library(cloud2trees) # the cloud2trees 2.1 Lidar Data Location Let’s check out the lidar data we got from the Mogollon Rim area of the Coconino National Forest about 20 km north of Payson, Arizona, USA using the USGS LidarExplorer. # directory with the downloaded .las|.laz files f &lt;- &quot;e:/lidar_phys_fire_mods/data/mogollon_rim_az_lidar/&quot; # is there data? list.files(f, pattern = &quot;.*\\\\.(laz|las)$&quot;) %&gt;% length() ## [1] 42 # what files are in here? list.files(f, pattern = &quot;.*\\\\.(laz|las)$&quot;)[1:3] ## [1] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3808.laz&quot; ## [2] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3809.laz&quot; ## [3] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3810.laz&quot; what information does lidR read from the catalog? ctg_temp &lt;- lidR::readLAScatalog(f) ctg_temp ## class : LAScatalog (v1.4 format 6) ## extent : 467000, 473000, 3808000, 3815000 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83(2011) / UTM zone 12N + NAVD88 height - Geoid18 (m) ## area : 42 km² ## points : 571.95 million points ## density : 13.6 points/m² ## density : 11.3 pulses/m² ## num. files : 42 that’s a lot of points…can an ordinary laptop handle it? we’ll find out. We’ll plot our point cloud data tiles real quick to orient ourselves ctg_temp %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% mapview::mapview(popup = F, layer.name = &quot;point cloud tile&quot;) 2.2 Individial Tree Detection Tuning: itd_tuning() The cloud2trees package performs individual tree detection using lidR::locate_trees() with the lidR::lmf() algorithm. The local maximum filter algorithm allows for a constant window size or a variable window size defined by a function. See the lidR package book section by point cloud processing expert Jean-Romain Roussel for excellent detail on ITD and defining window size. The itd_tuning() function is used to visually assess tree crown delineation results from different window size functions used for the detection of individual trees. itd_tuning() allows users to test different window size functions on a sample of data to determine which function is most suitable for the area being analyzed. The preferred function can then be used in the ws parameter in raster2trees() and cloud2trees(). Let’s run itd_tuning() on our data starting with default window size functions # run itd_tuning() itd_tuning_ans &lt;- cloud2trees::itd_tuning(f) # what did we get? itd_tuning_ans %&gt;% names() ## [1] &quot;plot_samples&quot; &quot;ws_fn_list&quot; check the ws_fn_list return which includes the different window size functions tested # what ws_fn_list itd_tuning_ans$ws_fn_list %&gt;% str() ## List of 3 ## $ lin_fn:function (x) ## $ exp_fn:function (x) ## $ log_fn:function (x) let’s look at the function definition for the linear function (lin_fn) # the linear function itd_tuning_ans$ws_fn_list$lin_fn ## function (x) ## { ## y &lt;- dplyr::case_when(is.na(x) ~ 0.001, x &lt; 0 ~ 0.001, x &lt; ## 2 ~ 1, x &gt; 30 ~ 5, TRUE ~ 0.75 + (x * 0.14)) ## return(y) ## } ## &lt;bytecode: 0x0000024fe27de1c8&gt; ## &lt;environment: 0x0000024fe27d2fc0&gt; let’s plot all of the functions we tested with our call to itd_tuning() using the defaults # shape of the ws functions ggplot() + geom_function(aes(color = &quot;lin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$lin_fn, lwd=1.2) + geom_function(aes(color = &quot;log_fn&quot;),fun=itd_tuning_ans$ws_fn_list$log_fn, lwd=1.2) + geom_function(aes(color = &quot;exp_fn&quot;),fun=itd_tuning_ans$ws_fn_list$exp_fn, lwd=1.2) + xlim(-5,60) + harrypotter::scale_color_hp_d(option = &quot;hermionegranger&quot;) + labs(x = &quot;heights&quot;, y = &quot;ws&quot;, color = &quot;ws function&quot;) + theme_light() now, let’s see how those window size functions impacted individual tree detection by checking the plot_samples return # tuning plot itd_tuning_ans$plot_samples Looking at the first sample, the logarithmic function (log_fn) resulted in too few trees detected in the overstory class. The clearest evidence of this is in the center of the left-hand side of the plot in the first sample. There is a clear “valley” in the CHM which the linear (lin_fn) and exponential (exp_fn) correctly split into two trees but the logarithmic function misses this split. Furthermore, the logarithmic function results in too many tree splits for short trees as can be seen in the second sample plot in the upper-right corner small tree group. The linear and the exponential function are very similar in detecting overstory trees but the linear function perhaps does a better job splitting up clumps of smaller trees. In the second sample plot the linear function does a better job splitting up the short tree group in the upper-right corner compared to the exponential function (there is no way that a tree that short [3-6 m tall] would have such a large crown area as in the exponential function split). If we had one gripe about the linear function, it’s maybe that it results in too many trees in small-tree patches. Let’s define our own custom linear function that slightly increases the window size for shorter trees compared to the default linear function. # custom linear function custom_lin &lt;- function (x){ y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &lt; 2 ~ 1.2 , x &gt; 30 ~ 5 , TRUE ~ 0.9 + (x * 0.139) ) return(y) } # shape of the ws functions ggplot() + geom_function(aes(color = &quot;lin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$lin_fn, lwd=1.2) + geom_function(aes(color = &quot;nonlin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$nonlin_fn, lwd=1.2) + geom_function(aes(color = &quot;custom_lin&quot;),fun=custom_lin, lwd=1.2) + xlim(-5,60) + harrypotter::scale_color_hp_d(option = &quot;hermionegranger&quot;) + labs(x = &quot;heights&quot;, y = &quot;ws&quot;, color = &quot;ws function&quot;) + theme_light() We’ll run another sample test using itd_tuning()with our new function (call it “my_custom_lin” for extra clarity) compared to the default linear and exponential functions. itd_tuning_ans2 &lt;- cloud2trees::itd_tuning( f , ws_fn_list = list( my_custom_lin = custom_lin , lin_fn = itd_tuning_ans$ws_fn_list$lin_fn , exp_fn = itd_tuning_ans$ws_fn_list$exp_fn ) ) now, let’s see how those window size functions impacted individual tree detection by checking the plot_samples return # tuning plot itd_tuning_ans2$plot_samples Our custom linear function (my_custom_lin) strikes a good balance between detection of lower canopy trees (e.g. &lt;10 m in height) without improperly subdividing dominant canopy trees based on the areas sampled. Let’s move forward with our custom linear function in the raster2trees() and cloud2trees() functions. 2.3 Point Cloud Tree Extraction: cloud2trees() The cloud2trees() function combines methods in the cloud2trees package for an all-in-one approach. We’ll call this function without estimating any of the additional tree components (the estimate_* parameters) which we will do separately to show the full process. With all other options turned off, cloud2trees() will: 1) generate a CHM from the point cloud using cloud2raster(); and 2) perform individual tree detection using raster2trees(). cloud2trees_ans &lt;- cloud2trees::cloud2trees( output_dir = &quot;../data&quot; , input_las_dir = f # we defined this above , accuracy_level = 2 , dtm_res_m = 1 , chm_res_m = 0.25 , min_height = 2 , ws = custom_lin # here it is , keep_intrmdt = T # these are turned off by default but we&#39;ll be explicit , estimate_tree_dbh = F , estimate_tree_competition = F , estimate_tree_type = F , estimate_tree_hmd = F , estimate_tree_cbh = F ) we should have a spatial tree list with tree height attached cloud2trees_ans$crowns_sf %&gt;% dplyr::select(treeID, tree_x, tree_y, tree_height_m, crown_area_m2) %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 6 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, &quot;1872834&quot;, &quot;… ## $ tree_x &lt;dbl&gt; 467000.1, 467000.1, 467000.1, 467000.1, 467000.1, 467000… ## $ tree_y &lt;dbl&gt; 3808063, 3808065, 3808067, 3808074, 3808091, 3808124, 38… ## $ tree_height_m &lt;dbl&gt; 2.15, 2.34, 2.02, 2.11, 3.66, 4.37, 2.03, 5.60, 17.72, 2… ## $ crown_area_m2 &lt;dbl&gt; 0.3750, 0.7500, 0.2500, 0.3750, 1.2500, 0.9375, 0.3750, … ## $ geom &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((467000 3808..., MULTIPOLYGO… That’s a lot of trees! Let’s check some out in the central part of our study area overlaid on some satellite imagery. Note, that we do not expect the trees we detected to match with what is visible in the satellite imagery because the collection dates vary, the projections vary, the scales vary, etc, etc. cloud2trees_ans$crowns_sf %&gt;% sf::st_intersection( ctg_temp %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% sf::st_union() %&gt;% sf::st_centroid() %&gt;% sf::st_buffer(80, endCapStyle = &quot;SQUARE&quot;) %&gt;% sf::st_transform(sf::st_crs(cloud2trees_ans$crowns_sf)) ) %&gt;% mapview::mapview( layer.name = &quot;example trees&quot; , color = &quot;white&quot; , lwd = 1 , alpha.regions = 0 , label = FALSE , legend = FALSE , popup = FALSE , map.types = c(&quot;Esri.WorldImagery&quot;) ) Let’s look at the distribution of tree height in our study area # there are always tree heights cloud2trees_ans$treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m)) + ggplot2::geom_density(fill = &quot;navy&quot;, color = &quot;navy&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) …that’s a lot of small trees let’s look at the summary statistics cloud2trees_ans$treetops_sf$tree_height_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.870 4.550 8.236 11.520 69.990 The cloud2trees() function dropped off a lot of additional data in a folder titled “point_cloud_processing_delivery” which is nested where we told the command to write the data (output_dir = \"../data\" parameter setting). Let’s load in the “processed_tracking_data.csv” file to see how long that cloud2trees() process took to run. Run times are, of course, dependent on computer processing and I am working on a laptop typical of a spatial analyst (especially outside of the US Federal Government) running Windows with an Intel i7-10750H 6-core computer processor unit and 32 gigabytes of random-access memory. # load processed_tracking_data.csv processing_data &lt;- readr::read_csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , progress = F , show_col_types = F ) # what? processing_data %&gt;% dplyr::select(1:4) %&gt;% dplyr::glimpse() ## Rows: 1 ## Columns: 4 ## $ number_of_points &lt;dbl&gt; 571949084 ## $ las_area_m2 &lt;dbl&gt; 41999160 ## $ timer_cloud2raster_mins &lt;dbl&gt; 135.4638 ## $ timer_raster2trees_mins &lt;dbl&gt; 93.28212 let’s do some math # total tree extraction time trees_mins_temp &lt;- processing_data$timer_cloud2raster_mins[1] + processing_data$timer_raster2trees_mins[1] # ha ha_temp &lt;- round(processing_data$las_area_m2[1]/10000) # secs per ha rate_temp &lt;- (trees_mins_temp*60) / ha_temp # point density dens_temp &lt;- processing_data$number_of_points[1] / processing_data$las_area_m2[1] Tree extraction over 4,200 hectares took a total of 228.7 minutes at processing rate of 3.27 seconds per hectare on lidar data with a point density of 13.6 points per square meter. 2.4 DBH Modeling: trees_dbh() The trees_dbh() function uses the TreeMap FIA plot data in the area of the tree list to estimate the height-DBH allometry relationship. The height predicting DBH model built from the FIA data is then used to predict DBH based on tree height in the tree list. # where should we save the file? dbh_fnm &lt;- &quot;../data/point_cloud_processing_delivery/dbh_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(dbh_fnm)){ # time it st_temp &lt;- Sys.time() # run it trees_dbh_ans &lt;- cloud2trees::trees_dbh( tree_list = cloud2trees_ans$treetops_sf , outfolder = &quot;../data/point_cloud_processing_delivery&quot; ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_dbh_mins &lt;- mins_temp # save dbh trees_dbh_ans %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = dbh_fnm, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ # dbh data trees_dbh_ans &lt;- readr::read_csv(dbh_fnm, progress = F, show_col_types = F) } Estimating DBH for our tree list of 2.63 M trees over an area of 4,200 hectares took 23.4 minutes at a rate of 0.33 seconds per hectare let’s check the relationship between height and DBH as estimated by the regional allometric relationship trees_dbh_ans %&gt;% dplyr::slice_sample(n=7777) %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, y = dbh_cm)) + ggplot2::geom_point(color = &quot;navy&quot;, alpha = 0.6) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;tree DBH (cm)&quot;) + ggplot2::scale_x_continuous(limits = c(0,NA)) + ggplot2::scale_y_continuous(limits = c(0,NA)) + ggplot2::theme_light() Let’s look at the distribution of tree diameter in our study area trees_dbh_ans %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = dbh_cm)) + ggplot2::geom_density(fill = &quot;brown&quot;, color = &quot;brown&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree DBH (cm)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) …that’s a lot of small trees let’s look at the summary statistics trees_dbh_ans$dbh_cm %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.693 4.872 7.185 15.753 19.908 170.971 cloud2trees::trees_dbh() saved the actual model estimated using the Bayesian modelling package brms which we can load and review dbh_mod_temp &lt;- readRDS(&quot;../data/point_cloud_processing_delivery/regional_dbh_height_model.rds&quot;) # what is this? dbh_mod_temp %&gt;% class() ## [1] &quot;brmsfit&quot; we can draw fit curves with probability bands using the tidybayes package library(tidybayes) # define our height range to predict over dplyr::tibble(tree_height_m = seq(from = 0, to = 50, by = 1)) %&gt;% tidybayes::add_epred_draws(dbh_mod_temp, ndraws = 2000) %&gt;% ggplot2::ggplot(ggplot2::aes(x = tree_height_m)) + tidybayes::stat_lineribbon( ggplot2::aes(y = .epred, color = &quot;estimate&quot;) , .width = c(0.5,0.95) , lwd = 0.6 ) + ggplot2::scale_fill_brewer(palette = &quot;Oranges&quot;) + ggplot2::scale_color_manual(values = c(&quot;gray33&quot;)) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;est. tree DBH (cm)&quot;, color = &quot;&quot;) + ggplot2::scale_x_continuous(limits = c(0,NA), breaks = scales::extended_breaks(n=11)) + ggplot2::scale_y_continuous(limits = c(0,NA), breaks = scales::extended_breaks(n=11)) + ggplot2::theme_light() The probability bands are there in shades of orange but they are so tight to the median estimate that it’s impossible to see them. The model confidence bands are so narrow because the model was trained with lots of FIA measured trees over the broad study area we can check how many FIA measured trees were used to train our model because cloud2trees::trees_dbh() also writes the training data to the disk (that’s neat, but we don’t always need to see how the sausage is made) readr::read_csv( &quot;../data/point_cloud_processing_delivery/regional_dbh_height_model_training_data.csv&quot; , progress = F , show_col_types = F ) %&gt;% dplyr::summarise(training_trees = sum(tree_weight)) %&gt;% dplyr::pull(training_trees) %&gt;% scales::comma(accuracy = 1) ## [1] &quot;2,038,807&quot; that’s how many FIA measured trees were used to train our model 2.5 HMD Modeling: trees_hmd() The trees_hmd() function uses the tree crown polygons we delineated from the point cloud with the columns treeID and tree_height_m to attempt to extract height to maximum crown diameter (HMD) directly from the height normalized point cloud by finding the height of the non-ground point farthest from the tree center (i.e. tree top). HMD refers to the vertical height at which a tree’s crown has its widest horizontal spread. It describes a characteristic of the overall crown shape and structure. Because we have 2.63 M trees, we’ll attempt to extract HMD for a sample and model the rest based on the data we successfully extracted from the point cloud. This is a memory-intensive process, so we’ll clear all objects from our R session and read them back in later after processing. # clean session remove(list = base::setdiff(ls(), c(&quot;processing_data&quot;,&quot;dbh_fnm&quot;))) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 6142647 328.1 92967510 4965.1 69363577 3704.5 ## Vcells 13178173 100.6 613452647 4680.3 650816304 4965.4 # where should we save the file? hmd_fnm &lt;- &quot;../data/point_cloud_processing_delivery/hmd_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(hmd_fnm)){ # sample proportion sample_prop_temp &lt;- 1 # time it st_temp &lt;- Sys.time() # run it trees_hmd_ans &lt;- cloud2trees::trees_hmd( trees_poly = &quot;../data/point_cloud_processing_delivery&quot; , norm_las = &quot;../data/point_cloud_processing_delivery/norm_las/&quot; , tree_sample_prop = sample_prop_temp , force_same_crs = T , estimate_missing_hmd = T ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_hmd_mins &lt;- mins_temp processing_data$sttng_hmd_tree_sample_n &lt;- NA processing_data$sttng_hmd_tree_sample_prop &lt;- sample_prop_temp # save hmd trees_hmd_ans %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = hmd_fnm, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ # hmd data trees_hmd_ans &lt;- readr::read_csv(hmd_fnm, progress = F, show_col_types = F) } HMD extraction took a total of 333.5 minutes at processing rate of 7.61 seconds per 1,000 trees We attempted to extract HMD from 100% of our tree list, let’s see our success rate trees_hmd_ans %&gt;% dplyr::count(is_training_hmd) %&gt;% dplyr::mutate(pct = n/sum(n)) ## # A tibble: 2 × 3 ## is_training_hmd n pct ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 1414752 0.538 ## 2 TRUE 1214743 0.462 all of the records marked as “training data” had HMD successfully extracted from the point cloud and were used to estimate a height-HMD allometry relationship that is spatially informed using the relative tree location let’s look at the training versus the modeled HMD versus height trees_hmd_ans %&gt;% dplyr::slice_sample(n = 11111) %&gt;% dplyr::arrange(desc(is_training_hmd)) %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, y = max_crown_diam_height_m, color=is_training_hmd)) + ggplot2::geom_point() + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;tree HMD (m)&quot;) + ggplot2::scale_y_continuous(breaks = scales::extended_breaks(n=12)) + ggplot2::scale_x_continuous(breaks = scales::extended_breaks(n=14)) + ggplot2::scale_color_viridis_d(option = &quot;turbo&quot;, begin = 0.2, alpha = 0.7, name = &quot;is HMD\\nfrom cloud?&quot;) + ggplot2::theme_light() Let’s look at the distribution of HMD in our study area trees_hmd_ans %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = max_crown_diam_height_m)) + ggplot2::geom_density(fill = &quot;coral&quot;, color = &quot;coral&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree HMD (m)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) and look at the summary statistics of HMD trees_hmd_ans$max_crown_diam_height_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.660 2.080 2.886 3.007 39.490 2.6 CBH Modeling: trees_cbh() The trees_cbh() function uses the tree crown polygons we delineated from the point cloud with the columns treeID and tree_height_m to attempt to extract crown base height (CBH) directly from the height normalized point cloud using the process outlined in Viedma et al. (2024). This is a memory-intensive process, so we’ll clear all objects from our R session and read them back in later after processing. # clean session remove(list = base::setdiff(ls(), c(&quot;processing_data&quot;,&quot;dbh_fnm&quot;,&quot;hmd_fnm&quot;))) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 6163200 329.2 74374008 3972.1 69363577 3704.5 ## Vcells 69250714 528.4 490762118 3744.3 650816304 4965.4 We’ll attempt to extract CBH for a sample and model the rest based on the data we successfully extracted from the point cloud. # where should we save the file? cbh_fnm &lt;- &quot;../data/point_cloud_processing_delivery/cbh_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(cbh_fnm)){ # sample proportion sample_prop_temp &lt;- 0.50 # time it st_temp &lt;- Sys.time() # run it trees_cbh_ans &lt;- cloud2trees::trees_cbh( trees_poly = &quot;../data/point_cloud_processing_delivery&quot; , norm_las = &quot;../data/point_cloud_processing_delivery/norm_las/&quot; , tree_sample_prop = sample_prop_temp , which_cbh = &quot;lowest&quot; , estimate_missing_cbh = TRUE , min_vhp_n = 3 , voxel_grain_size_m = 1 , dist_btwn_bins_m = 1 , min_fuel_layer_ht_m = 1 , lad_pct_gap = 25 , lad_pct_base = 25 , num_jump_steps = 1 , min_lad_pct = 10 , frst_layer_min_ht_m = 1 , force_same_crs = T ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_cbh_mins &lt;- mins_temp processing_data$sttng_cbh_tree_sample_n &lt;- NA processing_data$sttng_cbh_tree_sample_prop &lt;- sample_prop_temp # save cbh trees_cbh_ans %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = cbh_fnm, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ # cbh data trees_cbh_ans &lt;- readr::read_csv(cbh_fnm, progress = F, show_col_types = F) } CBH extraction took a total of 3,387.7 minutes (56.5 hours) at processing rate of 77.30 seconds per 1,000 trees We attempted to extract CBH from 50% of our tree list (1.31 M trees), let’s see our success rate # scales::percent(processing_data$sttng_cbh_tree_sample_prop) trees_cbh_ans %&gt;% dplyr::count(is_training_cbh) %&gt;% dplyr::mutate(pct = n/sum(n)) ## # A tibble: 2 × 3 ## is_training_cbh n pct ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 2291970 0.872 ## 2 TRUE 337525 0.128 That equates to a 25.7% CBH extraction success rate….not a great success ;( This is especially worrisome given the significant time required to attempt CBH extraction from the point cloud. The low success rate is all of the records marked as training data had CBH successfully extracted from the point cloud and were used to estimate a height-CBH allometry relationship that is spatially informed using the relative tree location let’s look at the training versus the modeled CBH versus height trees_cbh_ans %&gt;% dplyr::slice_sample(n = 11111) %&gt;% dplyr::arrange(is_training_cbh) %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, y = tree_cbh_m, color=is_training_cbh)) + ggplot2::geom_point() + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;tree CBH (m)&quot;) + ggplot2::scale_y_continuous(breaks = scales::extended_breaks(n=12)) + ggplot2::scale_x_continuous(breaks = scales::extended_breaks(n=14)) + ggplot2::scale_color_viridis_d(alpha = 0.7, name = &quot;is CBH\\nfrom cloud?&quot;) + ggplot2::theme_light() Let’s look at the distribution of CBH in our study area trees_cbh_ans %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_cbh_m)) + ggplot2::geom_density(fill = &quot;maroon4&quot;, color = &quot;maroon4&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree CBH (m)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) and look at the summary statistics of CBH trees_cbh_ans$tree_cbh_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.500 2.396 3.045 3.668 4.500 28.500 clean our session # clean session remove(list = base::setdiff(ls(), c(&quot;processing_data&quot;,&quot;dbh_fnm&quot;,&quot;hmd_fnm&quot;,&quot;cbh_fnm&quot;))) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 6163278 329.2 59499207 3177.7 69363577 3704.5 ## Vcells 69251331 528.4 392609695 2995.4 650816304 4965.4 2.7 Forest Type: trees_type() We’ll now use trees_type() to attach species information using USDA Forest Inventory and Analysis (FIA) codes. FIA Forest Type Group Code is attached to each tree in the tree list based on the spatial overlap with the Forest Type Groups of the Continental United States data (Wilson 2023). We now need to read back in our full spatial data frame of points to use as the input tree list, let’s just load the tree points # get the data from already run treetops_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_tree_tops.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read(dsn = x, quiet = T) ) %&gt;% dplyr::bind_rows() let’s get the FIA forest type group for our list # where should we save the file? type_fnm &lt;- &quot;../data/point_cloud_processing_delivery/type_data.csv&quot; type_rast_fnm &lt;- &quot;../data/point_cloud_processing_delivery/type_rast.tif&quot; # if we don&#39;t already have the data, run it if(!file.exists(type_fnm)){ # time it st_temp &lt;- Sys.time() # run it trees_type_ans &lt;- cloud2trees::trees_type( tree_list = treetops_sf , study_boundary = sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_union() ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_type_mins &lt;- mins_temp # save type trees_type_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = type_fnm, row.names = F, append = F) # save raster trees_type_ans$foresttype_rast %&gt;% terra::writeRaster(type_rast_fnm, overwrite=T) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ trees_type_ans &lt;- list() # type data trees_type_ans$tree_list &lt;- readr::read_csv(type_fnm, progress = F, show_col_types = F) # raster trees_type_ans$foresttype_rast &lt;- terra::rast(type_rast_fnm) } Let’s look at the FIA Forest Type Group data we extracted for the tree list. trees_type_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(forest_type_group_code, forest_type_group) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::mutate(pct = n/sum(n)) %&gt;% kableExtra::kbl(caption = &quot;Count of trees by FIA Forest Type Group&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 2.1: Count of trees by FIA Forest Type Group forest_type_group_code forest_type_group n pct 220 Ponderosa pine group 2143301 0.82 970 Woodland hardwoods group 239301 0.09 180 Pinyon / juniper group 221663 0.08 200 Douglas-fir group 20484 0.01 260 Fir / spruce / mountain hemlock group 4731 0.00 900 Aspen / birch group 15 0.00 Let’s attach FIA Forest Types Group name to the raster (foresttype_rast) of the area we searched and plot it # load in the forest type data ext_data_temp &lt;- cloud2trees::find_ext_data() foresttype_lookup &lt;- file.path(ext_data_temp$foresttype_dir, &quot;foresttype_lookup.csv&quot;) %&gt;% readr::read_csv(progress = F, show_col_types = F) %&gt;% dplyr::distinct(forest_type_group_code, forest_type_group, hardwood_softwood) # what? foresttype_lookup %&gt;% dplyr::glimpse() ## Rows: 35 ## Columns: 3 ## $ forest_type_group_code &lt;dbl&gt; 100, 120, 140, 150, 160, 170, 180, 200, 220, 24… ## $ forest_type_group &lt;chr&gt; &quot;White / red / jack pine group&quot;, &quot;Spruce / fir … ## $ hardwood_softwood &lt;chr&gt; &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;,… plot the FIA Forest Types Group raster of our study area # study area aoi_temp &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_transform(terra::crs(trees_type_ans$foresttype_rast)) # plot raster r_plt &lt;- trees_type_ans$foresttype_rast %&gt;% terra::crop(aoi_temp %&gt;% sf::st_buffer(100) %&gt;% terra::vect()) %&gt;% terra::as.data.frame(xy=T) %&gt;% dplyr::rename(forest_type_group_code = 3) %&gt;% dplyr::left_join(foresttype_lookup, by = &quot;forest_type_group_code&quot;) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile(mapping = ggplot2::aes(x=x, y=y, fill = forest_type_group)) + ggplot2::labs(fill = &quot;FIA forest\\ntype group&quot;) + harrypotter::scale_fill_hp_d(&quot;lunalovegood&quot;) + ggplot2::theme_void() + ggplot2::theme(legend.position = &quot;top&quot;) + ggplot2::guides(fill = ggplot2::guide_legend(nrow = 2, byrow = T)) # add our study boundary r_plt + ggplot2::geom_sf(data = aoi_temp, color = &quot;white&quot;, fill = NA) 2.8 Combine data before our last tree component calculation, we’ll bring all of our data together because the last component relies on this data # read all of our data back in trees_dbh_temp &lt;- readr::read_csv( dbh_fnm , col_select = c( treeID, tidyselect::contains(&quot;dbh&quot;) , tidyselect::starts_with(&quot;basal_area&quot;) , tidyselect::starts_with(&quot;radius&quot;) ) , show_col_types = F , progress = F ) trees_cbh_temp &lt;- readr::read_csv( cbh_fnm , col_select = c(treeID, tree_cbh_m, is_training_cbh) , show_col_types = F , progress = F ) trees_hmd_temp &lt;- readr::read_csv( hmd_fnm , col_select = c(treeID, max_crown_diam_height_m, is_training_hmd) , show_col_types = F , progress = F ) trees_type_temp &lt;- readr::read_csv( type_fnm , col_select = c(treeID, tidyselect::starts_with(&quot;forest_type&quot;), hardwood_softwood) , show_col_types = F , progress = F ) # function re-cast treeID if needed recast_id_fn &lt;- function(df, cl) { if( !inherits( df$treeID , cl ) ){ if(cl==&quot;character&quot;){ df$treeID &lt;- as.character(df$treeID) }else if(cl==&quot;numeric&quot;){ df$treeID &lt;- as.numeric(df$treeID) } } return(df) } # apply trees_dbh_temp &lt;- trees_dbh_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) trees_cbh_temp &lt;- trees_cbh_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) trees_hmd_temp &lt;- trees_hmd_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) trees_type_temp &lt;- trees_type_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) # now we&#39;ll get a list of the columns in our component data to drop from our main data cols_to_drop_temp &lt;- c( names(trees_dbh_temp), names(trees_cbh_temp) , names(trees_hmd_temp), names(trees_type_temp) ) %&gt;% unique() %&gt;% setdiff(&quot;treeID&quot;) # remove cols from our primary data treetops_sf &lt;- treetops_sf %&gt;% dplyr::select( -dplyr::any_of(cols_to_drop_temp)) # join altogether using the magical purrr::reduce treetops_sf &lt;- list( treetops_sf, trees_dbh_temp, trees_cbh_temp , trees_hmd_temp, trees_type_temp ) %&gt;% purrr::reduce(\\(x,y) dplyr::left_join(x, y, by = &quot;treeID&quot;)) let’s glimpse our almost final data treetops_sf %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 36 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, … ## $ crown_area_m2 &lt;dbl&gt; 0.3750, 0.7500, 0.2500, 0.3750, 1.2500, 0.93… ## $ tree_height_m &lt;dbl&gt; 2.15, 2.34, 2.02, 2.11, 3.66, 4.37, 2.03, 5.… ## $ is_training_data &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ comp_trees_per_ha &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_relative_tree_height &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_dist_to_nearest_m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_stand_id &lt;dbl&gt; 71791, 71791, 71791, 71791, 71540, 71289, 70… ## $ crown_dia_m &lt;dbl&gt; 0.6909883, 0.9772050, 0.5641896, 0.6909883, … ## $ crown_length_m &lt;dbl&gt; 0.1666052, 0.1948469, 0.1565314, 0.1635056, … ## $ crown_volume_m3 &lt;dbl&gt; 0.04165130, 0.09742345, 0.02608857, 0.040876… ## $ cruz_tree_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_stand_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_crown_biomass_kg &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ landfire_stand_id &lt;dbl&gt; 56399, 56399, 56399, 56158, 56158, 55917, 55… ## $ landfire_tree_kg_per_m3 &lt;dbl&gt; 1.3947342, 1.3947342, 1.3947342, 0.1140463, … ## $ landfire_stand_kg_per_m3 &lt;dbl&gt; 0.06, 0.06, 0.06, 0.27, 0.27, 0.09, 0.08, 0.… ## $ landfire_crown_biomass_kg &lt;dbl&gt; 0.058092493, 0.135879822, 0.036386614, 0.004… ## $ fia_est_dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ fia_est_dbh_cm_lower &lt;dbl&gt; 1.548500, 1.572502, 1.425198, 1.466369, 2.23… ## $ fia_est_dbh_cm_upper &lt;dbl&gt; 7.350390, 7.661050, 6.790382, 7.022582, 10.9… ## $ dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ dbh_m &lt;dbl&gt; 0.03977012, 0.04149075, 0.03693238, 0.038402… ## $ ptcld_extracted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ ptcld_predicted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ basal_area_m2 &lt;dbl&gt; 0.001242235, 0.001352049, 0.001071284, 0.001… ## $ basal_area_ft2 &lt;dbl&gt; 0.01337142, 0.01455345, 0.01153130, 0.012467… ## $ radius_m &lt;dbl&gt; 0.01988506, 0.02074537, 0.01846619, 0.019201… ## $ tree_cbh_m &lt;dbl&gt; 1.983395, 2.145153, 1.863469, 1.946494, 2.79… ## $ is_training_cbh &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ max_crown_diam_height_m &lt;dbl&gt; 1.504257, 1.568958, 0.880000, 1.520000, 1.74… ## $ is_training_hmd &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE… ## $ forest_type_group_code &lt;dbl&gt; 180, 180, 180, 180, 180, 180, 180, 180, 180,… ## $ forest_type_group &lt;chr&gt; &quot;Pinyon / juniper group&quot;, &quot;Pinyon / juniper … ## $ hardwood_softwood &lt;chr&gt; &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwoo… ## $ geom &lt;POINT [m]&gt; POINT (467000.1 3808063), POINT (46700… 2.9 Crown Biomass: trees_biomass() Lastly, we’ll use trees_biomass() to estimate tree crown in kilograms. We’ll estimate biomass based on: 1) LANDFIRE’s Forest Canopy Bulk Density (CBD) data; and 2) based on the Cruz et al. (2003) equations and the FIA forest type group we got above # where should we save the file? biomass_fnm &lt;- &quot;../data/point_cloud_processing_delivery/biomass_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(biomass_fnm)){ # time it st_temp &lt;- Sys.time() # run it trees_biomass_ans &lt;- cloud2trees::trees_biomass( tree_list = treetops_sf , method = c(&quot;cruz&quot;,&quot;landfire&quot;) , study_boundary = sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_union() ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_biomass_mins &lt;- mins_temp # save biomass trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = biomass_fnm, row.names = F, append = F) # save raster df trees_biomass_ans$stand_cell_data_landfire %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_landfire.csv&quot;, row.names = F, append = F) trees_biomass_ans$stand_cell_data_cruz %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_cruz.csv&quot;, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ trees_biomass_ans &lt;- list() # biomass data trees_biomass_ans$tree_list &lt;- readr::read_csv(biomass_fnm, progress = F, show_col_types = F) # raster trees_biomass_ans$stand_cell_data_landfire &lt;- readr::read_csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_landfire.csv&quot;, progress = F, show_col_types = F) trees_biomass_ans$stand_cell_data_cruz &lt;- readr::read_csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_cruz.csv&quot;, progress = F, show_col_types = F) } Tree crown biomass estimation took a total of 5.4 minutes at processing rate of 0.08 seconds per hectare The trees_biomass() process constrains tree crown bulk density (CBD) values to 2.0 kilograms per cubic meter by default based on Mell et al. (2009) who found the dry bulk density of the tree crown was 2.6 kilograms per cubed meter using Douglas-fir trees grown on Christmas tree farms. Let’s check the distribution of CBD values estimated in our study area. trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(tidyselect::ends_with(&quot;tree_kg_per_m3&quot;)) %&gt;% tidyr::pivot_longer(cols = tidyselect::everything()) %&gt;% dplyr::mutate(name = stringr::str_remove_all(name,&quot;_tree_kg_per_m3&quot;)) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, color = name, fill = name)) + ggplot2::geom_density() + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_fill_viridis_d(option = &quot;turbo&quot;, begin = 0.3, end = 0.7, alpha = 0.7, name = &quot;biomass method&quot;) + ggplot2::scale_color_viridis_d(option = &quot;turbo&quot;, begin = 0.3, end = 0.7,name = &quot;biomass method&quot;) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(8)) + ggplot2::labs(x = &quot;tree kilograms per cubic meter&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(legend.position = &quot;none&quot;) let’s look at the summary of the tree CBD values and the resulting crown biomass trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(tidyselect::ends_with(&quot;tree_kg_per_m3&quot;), tidyselect::ends_with(&quot;crown_biomass_kg&quot;)) %&gt;% summary() ## cruz_tree_kg_per_m3 landfire_tree_kg_per_m3 cruz_crown_biomass_kg ## Min. :0.0 Min. :0.004402 Min. : 0.0 ## 1st Qu.:0.2 1st Qu.:0.071797 1st Qu.: 0.1 ## Median :0.2 Median :0.104738 Median : 0.8 ## Mean :0.3 Mean :0.158657 Mean : 22.4 ## 3rd Qu.:0.3 3rd Qu.:0.155825 3rd Qu.: 16.1 ## Max. :2.0 Max. :1.999384 Max. :3504.8 ## NA&#39;s :460979 NA&#39;s :460990 ## landfire_crown_biomass_kg ## Min. : 0.000 ## 1st Qu.: 0.045 ## Median : 0.275 ## Mean : 9.314 ## 3rd Qu.: 4.060 ## Max. :3947.367 ## NA&#39;s :11 note those NA values for the Cruz et al. (2003) method, these NA values are introduced because this particular methodology is limited in the forest types for which biomass equations exist check out the break-down by FIA forest type group trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(!is.na(forest_type_group)) %&gt;% dplyr::group_by(forest_type_group) %&gt;% dplyr::summarise( n = dplyr::n() , dplyr::across( cruz_tree_kg_per_m3 , .fns = list(mean = mean, sd = sd, min = min, max = max) ) ) %&gt;% dplyr::rename_with(~ stringr::str_remove_all(.x,&quot;cruz_tree_kg_per_m3_&quot;)) %&gt;% dplyr::arrange(desc(n)) %&gt;% kableExtra::kbl(caption = &quot;Summary of tree CBD (kg per m3) by FIA Forest Type Group&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 2.2: Summary of tree CBD (kg per m3) by FIA Forest Type Group forest_type_group n mean sd min max Ponderosa pine group 2143290 0.29 0.17 0.04 2.00 Woodland hardwoods group 239301 NA NA NA NA Pinyon / juniper group 221663 NA NA NA NA Douglas-fir group 20484 0.19 0.05 0.07 0.37 Fir / spruce / mountain hemlock group 4731 0.16 0.07 0.06 1.03 Aspen / birch group 15 NA NA NA NA We can see how the LANDFIRE Forest Canopy Bulk Density (CBD) data and Cruz et al. (2003) methodologies compare at estimating tree crown biomass for our study area we’ll make this comparison only for trees that have biomass estimated using the Cruz method as well # set the upper limit scale ul_temp &lt;- max( quantile(trees_biomass_ans$tree_list$cruz_crown_biomass_kg, probs = 0.95, na.rm = T) , quantile(trees_biomass_ans$tree_list$landfire_crown_biomass_kg, probs = 0.95, na.rm = T) ) # plot tree landfire vs. cruz crown biomass estimate trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(!is.na(cruz_crown_biomass_kg)) %&gt;% dplyr::slice_sample(n=11111) %&gt;% ggplot2::ggplot( mapping = ggplot2::aes( x = landfire_crown_biomass_kg, y = cruz_crown_biomass_kg ) ) + ggplot2::geom_abline(lwd = 1.5) + ggplot2::geom_point(ggplot2::aes(color = tree_height_m)) + ggplot2::geom_smooth(method = &quot;lm&quot;, se=F, color = &quot;tomato&quot;, linetype = &quot;dashed&quot;) + ggplot2::scale_color_viridis_c(option = &quot;mako&quot;, direction = -1, alpha = 0.8) + ggplot2::scale_x_continuous(limits = c(0, ul_temp)) + ggplot2::scale_y_continuous(limits = c(0, ul_temp)) + ggplot2::labs( x = &quot;LANDFIRE crown biomass (kg)&quot; , y = &quot;Cruz crown biomass (kg)&quot; , color = &quot;tree ht. (m)&quot; ) + ggplot2::theme_light() we can summarize the average difference between the two methods for records with both estimates trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(!is.na(cruz_crown_biomass_kg)) %&gt;% dplyr::mutate( diff_cruz_lf_kg = cruz_crown_biomass_kg-landfire_crown_biomass_kg , pct_diff_cruz_lf_kg = diff_cruz_lf_kg/landfire_crown_biomass_kg ) %&gt;% dplyr::summarise( dplyr::across( c(cruz_crown_biomass_kg,landfire_crown_biomass_kg,diff_cruz_lf_kg,pct_diff_cruz_lf_kg) , .fns = list(mean = mean) , .names = &quot;{.fn}_{.col}&quot; ) ) %&gt;% dplyr::mutate( mean_pct_diff_cruz_lf_kg = scales::percent(mean_pct_diff_cruz_lf_kg, accuracy = 0.1) , dplyr::across( .cols = -mean_pct_diff_cruz_lf_kg , .fns = ~scales::comma(.x,accuracy = 0.01) ) ) %&gt;% tidyr::pivot_longer(tidyselect::everything()) %&gt;% kableExtra::kbl(caption = &quot;Mean difference between LANDFIRE and Cruz estimated tree crown biomass in kilograms&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 2.3: Mean difference between LANDFIRE and Cruz estimated tree crown biomass in kilograms name value mean_cruz_crown_biomass_kg 22.40 mean_landfire_crown_biomass_kg 10.68 mean_diff_cruz_lf_kg 11.72 mean_pct_diff_cruz_lf_kg 185.0% Finally, let’s plot the spatial arrangement of estimated biomass using the raster data returned from trees_biomass() First, for LANDFIRE # aoi aoi_temp &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) ## Reading layer `raw_las_ctg_info&#39; from data source ## `C:\\Data\\usfs\\lidar_phys_fire_mods\\data\\point_cloud_processing_delivery\\raw_las_ctg_info.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 42 features and 34 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 467000 ymin: 3808000 xmax: 473000 ymax: 3815000 ## Projected CRS: NAD83(2011) / UTM zone 12N # get the projection for the stand cell data epsg_code_temp &lt;- trees_biomass_ans$stand_cell_data_landfire$rast_epsg_code[1] %&gt;% as.numeric() # set the color limit ul_temp &lt;- max( max(trees_biomass_ans$stand_cell_data_landfire$landfire_stand_kg_per_m3, na.rm = T) , max(trees_biomass_ans$stand_cell_data_cruz$cruz_stand_kg_per_m3, na.rm = T) ) # plot the stand cell data with trees overlaid trees_biomass_ans$stand_cell_data_landfire %&gt;% dplyr::filter(trees&gt;0) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile(ggplot2::aes(x=x,y=y,fill = landfire_stand_kg_per_m3), color = NA) + ggplot2::geom_sf(data = aoi_temp %&gt;% sf::st_transform(crs = epsg_code_temp), fill = NA) + ggplot2::labs(subtitle = &quot;LANDFIRE stand kg/m3&quot;, fill=&quot;LANDFIRE\\nstand kg/m3&quot;) + ggplot2::scale_fill_viridis_c( limits = c(NA,ul_temp) , option = &quot;rocket&quot;, direction = -1, na.value = &quot;white&quot; ) + ggplot2::coord_sf() + ggplot2::theme_void() and for Cruz # get the projection for the stand cell data epsg_code_temp &lt;- trees_biomass_ans$stand_cell_data_cruz$rast_epsg_code[1] %&gt;% as.numeric() # plot the stand cell data with trees overlaid trees_biomass_ans$stand_cell_data_cruz %&gt;% dplyr::filter(trees&gt;0) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile(ggplot2::aes(x=x,y=y,fill = cruz_stand_kg_per_m3), color = NA) + ggplot2::geom_sf(data = aoi_temp %&gt;% sf::st_transform(crs = epsg_code_temp), fill = NA) + ggplot2::labs(subtitle = &quot;Cruz stand kg/m3&quot;, fill=&quot;Cruz\\nstand kg/m3&quot;) + ggplot2::scale_fill_viridis_c( limits = c(NA,ul_temp) , option = &quot;rocket&quot;, direction = -1, na.value = &quot;white&quot; ) + ggplot2::theme_void() 2.10 Data Export Now that we have our point cloud-extracted tree list with all of the tree component measurements attached, let’s save the data for use in our analysis first we’ll attach the biomass metrics to our spatial tree list # new columns added by trees_biomass() names_temp &lt;- setdiff( trees_biomass_ans$tree_list %&gt;% names() , treetops_sf %&gt;% names() ) # recast id if needed trees_biomass_ans$tree_list &lt;- trees_biomass_ans$tree_list %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) # attach to our spatial tree points treetops_sf &lt;- treetops_sf %&gt;% dplyr::left_join( trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(dplyr::all_of(c(&quot;treeID&quot;, names_temp))) , by = &quot;treeID&quot; ) # what does our final data look like? treetops_sf %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 38 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, … ## $ crown_area_m2 &lt;dbl&gt; 0.3750, 0.7500, 0.2500, 0.3750, 1.2500, 0.93… ## $ tree_height_m &lt;dbl&gt; 2.15, 2.34, 2.02, 2.11, 3.66, 4.37, 2.03, 5.… ## $ is_training_data &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ comp_trees_per_ha &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_relative_tree_height &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_dist_to_nearest_m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_stand_id &lt;dbl&gt; 71791, 71791, 71791, 71791, 71540, 71289, 70… ## $ crown_dia_m &lt;dbl&gt; 0.6909883, 0.9772050, 0.5641896, 0.6909883, … ## $ crown_length_m &lt;dbl&gt; 0.1666052, 0.1948469, 0.1565314, 0.1635056, … ## $ crown_volume_m3 &lt;dbl&gt; 0.04165130, 0.09742345, 0.02608857, 0.040876… ## $ cruz_tree_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_stand_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_crown_biomass_kg &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ landfire_stand_id &lt;dbl&gt; 56399, 56399, 56399, 56158, 56158, 55917, 55… ## $ landfire_tree_kg_per_m3 &lt;dbl&gt; 1.3947342, 1.3947342, 1.3947342, 0.1140463, … ## $ landfire_stand_kg_per_m3 &lt;dbl&gt; 0.06, 0.06, 0.06, 0.27, 0.27, 0.09, 0.08, 0.… ## $ landfire_crown_biomass_kg &lt;dbl&gt; 0.058092493, 0.135879822, 0.036386614, 0.004… ## $ fia_est_dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ fia_est_dbh_cm_lower &lt;dbl&gt; 1.548500, 1.572502, 1.425198, 1.466369, 2.23… ## $ fia_est_dbh_cm_upper &lt;dbl&gt; 7.350390, 7.661050, 6.790382, 7.022582, 10.9… ## $ dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ dbh_m &lt;dbl&gt; 0.03977012, 0.04149075, 0.03693238, 0.038402… ## $ ptcld_extracted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ ptcld_predicted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ basal_area_m2 &lt;dbl&gt; 0.001242235, 0.001352049, 0.001071284, 0.001… ## $ basal_area_ft2 &lt;dbl&gt; 0.01337142, 0.01455345, 0.01153130, 0.012467… ## $ radius_m &lt;dbl&gt; 0.01988506, 0.02074537, 0.01846619, 0.019201… ## $ tree_cbh_m &lt;dbl&gt; 1.983395, 2.145153, 1.863469, 1.946494, 2.79… ## $ is_training_cbh &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ max_crown_diam_height_m &lt;dbl&gt; 1.504257, 1.568958, 0.880000, 1.520000, 1.74… ## $ is_training_hmd &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE… ## $ forest_type_group_code &lt;dbl&gt; 180, 180, 180, 180, 180, 180, 180, 180, 180,… ## $ forest_type_group &lt;chr&gt; &quot;Pinyon / juniper group&quot;, &quot;Pinyon / juniper … ## $ hardwood_softwood &lt;chr&gt; &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwoo… ## $ tree_x &lt;dbl&gt; 467000.1, 467000.1, 467000.1, 467000.1, 4670… ## $ tree_y &lt;dbl&gt; 3808063, 3808065, 3808067, 3808074, 3808091,… ## $ geom &lt;POINT [m]&gt; POINT (467000.1 3808063), POINT (46700… load in the crown spatial files and attach the new component data crowns_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_crowns.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() # the trees are the same, the data in the columns are different identical(nrow(crowns_sf), nrow(treetops_sf)) # attach the new columns crowns_sf &lt;- crowns_sf %&gt;% dplyr::select(treeID, tree_x, tree_y) %&gt;% dplyr::inner_join( treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(-c(tree_x, tree_y)) , by = &quot;treeID&quot; ) use the internal function from cloud2trees that writes the polygon and point vector data # this is an internal function from cloud2trees write_raster2trees_ans &lt;- function(raster2trees_ans, dir) { ### write the data to the disk if(nrow(raster2trees_ans)&gt;250e3){ # split up the detected crowns raster2trees_ans &lt;- raster2trees_ans %&gt;% dplyr::arrange(as.numeric(tree_x),as.numeric(tree_y)) %&gt;% # groups of 250k dplyr::mutate(grp = ceiling(dplyr::row_number()/250e3)) write_fnl_temp &lt;- raster2trees_ans$grp %&gt;% unique() %&gt;% purrr::map(function(x){ # dsn&#39;s cf &lt;- file.path( dir, paste0(&quot;final_detected_crowns_&quot;,x,&quot;.gpkg&quot;) ) tf &lt;- file.path( dir, paste0(&quot;final_detected_tree_tops_&quot;,x,&quot;.gpkg&quot;) ) ### write the data to the disk # crown vector polygons sf::st_write( raster2trees_ans %&gt;% dplyr::filter(grp == x) %&gt;% dplyr::select(-c(grp)) , dsn = cf , append = FALSE , quiet = TRUE ) # tree top vector points sf::st_write( # get tree points raster2trees_ans %&gt;% dplyr::filter(grp == x) %&gt;% dplyr::select(-c(grp)) %&gt;% sf::st_drop_geometry() %&gt;% sf::st_as_sf(coords = c(&quot;tree_x&quot;, &quot;tree_y&quot;), crs = sf::st_crs(raster2trees_ans)) , dsn = tf , append = FALSE , quiet = TRUE ) return( dplyr::tibble( crowns_file = cf , trees_file = tf ) ) }) %&gt;% dplyr::bind_rows() }else{ # dsn&#39;s cf &lt;- file.path( dir, &quot;final_detected_crowns.gpkg&quot; ) tf &lt;- file.path( dir, &quot;final_detected_tree_tops.gpkg&quot; ) # crown vector polygons sf::st_write( raster2trees_ans , dsn = cf , append = FALSE , quiet = TRUE ) # tree top vector points sf::st_write( # get tree points raster2trees_ans %&gt;% sf::st_drop_geometry() %&gt;% sf::st_as_sf(coords = c(&quot;tree_x&quot;, &quot;tree_y&quot;), crs = sf::st_crs(raster2trees_ans)) , dsn = tf , append = FALSE , quiet = TRUE ) # df write_fnl_temp &lt;- dplyr::tibble( crowns_file = cf , trees_file = tf ) } return(write_fnl_temp) } write the final data (this will overwrite our original data from cloud2trees() but that’s ok because the tree list and spatial information is the same) write_raster2trees_ans(crowns_sf, dir = &quot;../data/point_cloud_processing_delivery&quot;) "],["forest-stand-summary.html", "Section 3 Forest Stand Summary 3.1 Load data 3.2 Silvicultural metrics 3.3 Spatial Forest Structure", " Section 3 Forest Stand Summary In this section we’ll process the point cloud-extracted tree list data given stand boundaries that fall within the extent of the original point cloud data. When processing point cloud data with the objective for summarizing data within a forest stand, it is imperative to ensure that the point cloud extent completely covers and extends beyond the stand extent to avoid edge effects and tree artifacts. 3.1 Load data first, we’ll load our stand data and the boundary data of the point cloud that we processed # las data bounds las_ctg_sf &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;, quiet = T) # get our proj crs proj_crs &lt;- sf::st_crs(las_ctg_sf) # stands stand_sf &lt;- sf::st_read(&quot;../data/QUIK-Fire_Boundary/QUIK-Fire_Boundary.shp&quot;, quiet = T) %&gt;% sf::st_transform(proj_crs) %&gt;% dplyr::rename_with(~tolower(stringr::str_replace_all(.x,&quot;\\\\.&quot;, &quot;_&quot;))) %&gt;% dplyr::rename(unit_id = id) %&gt;% dplyr::mutate( stand_area_m2 = sf::st_area(.) %&gt;% as.numeric() , stand_area_ha = stand_area_m2/10000 ) # set our colors for the units if(nrow(stand_sf)&lt;length(harrypotter::hp_palettes$lunalovegood)){ my_pal &lt;- harrypotter::hp(n=nrow(stand_sf), option = &quot;lunalovegood&quot;) }else{ my_pal &lt;- viridis::turbo(n=nrow(stand_sf)) } what is the stand data? stand_sf %&gt;% dplyr::glimpse() ## Rows: 2 ## Columns: 7 ## $ unit_id &lt;dbl&gt; 1, 2 ## $ unit_name &lt;chr&gt; &quot;300 RD to Rim&quot;, &quot;Middle Kehl Canyon&quot; ## $ hectares &lt;dbl&gt; 119.658, 200.666 ## $ acres &lt;dbl&gt; 295.555, 495.645 ## $ geometry &lt;POLYGON [m]&gt; POLYGON ((470742 3810364, 4..., POLYGON ((469974.3 38112… ## $ stand_area_m2 &lt;dbl&gt; 1195649, 2005096 ## $ stand_area_ha &lt;dbl&gt; 119.5649, 200.5096 let’s look at these bounds on a map mapview::mapview( las_ctg_sf , layer.name = &quot;point cloud tile&quot; , color = &quot;black&quot; , lwd = 1 , alpha.regions = 0 , label = FALSE , legend = FALSE , popup = FALSE ) + mapview::mapview( stand_sf %&gt;% dplyr::select(unit_name) , zcol = &quot;unit_name&quot; , col.regions = my_pal , layer.name = &quot;stand bounds&quot; , alpha.regions = 0.8 ) we processed point cloud data well outside these stand bounds (it is not necessary to process data this far outside of our stands, a one tile buffer in this situation would have sufficed) load in the tree top points data from cloud2trees::cloud2trees() # get the data from already run treetops_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_tree_tops.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() we’ll keep only trees where the tree top point falls within one of our stands and then use this tree list to filter our crown data so that we get the full crown polygon even if it extends outside of the stand boundary treetops_sf &lt;- treetops_sf %&gt;% sf::st_intersection(stand_sf %&gt;% dplyr::select(unit_id,unit_name,tidyselect::starts_with(&quot;stand_area_&quot;))) load in the tree crown polygon data from cloud2trees::cloud2trees() and filter based on the tree list # get the data from already run crowns_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_crowns.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() %&gt;% dplyr::inner_join( treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(treeID,unit_id,unit_name,tidyselect::starts_with(&quot;stand_area_&quot;)) , by = &quot;treeID&quot; ) # the records are the same identical(nrow(treetops_sf), nrow(crowns_sf)) save the data by unit for sharing with a boss, coworker, friend, etc. # make a dir for saving this stand-level data to outdir &lt;- &quot;../data/mogollon_rim_fire_unit_trees&quot; if(!dir.exists(outdir)){dir.create(outdir, showWarnings = F)} # write crowns and tree tops write_temp &lt;- crowns_sf$unit_id %&gt;% unique() %&gt;% purrr::map(function(x, new_crs = proj_crs, my_outdir = outdir){ # dsn&#39;s cf &lt;- file.path( my_outdir, paste0(&quot;final_detected_crowns_unit_&quot;,x,&quot;.gpkg&quot;) ) tf &lt;- file.path( my_outdir, paste0(&quot;final_detected_tree_tops_unit_&quot;,x,&quot;.gpkg&quot;) ) ### write the data to the disk # crown vector polygons sf::st_write( crowns_sf %&gt;% dplyr::filter(unit_id == x) %&gt;% sf::st_transform(new_crs) , dsn = cf , append = FALSE , quiet = TRUE ) # tree top vector points sf::st_write( # get tree points crowns_sf %&gt;% dplyr::filter(unit_id == x) %&gt;% sf::st_drop_geometry() %&gt;% sf::st_as_sf(coords = c(&quot;tree_x&quot;, &quot;tree_y&quot;), crs = sf::st_crs(crowns_sf)) %&gt;% sf::st_transform(new_crs) %&gt;% dplyr::mutate( tree_x = sf::st_coordinates(.)[,1] , tree_y = sf::st_coordinates(.)[,2] ) , dsn = tf , append = FALSE , quiet = TRUE ) }) load in the DTM, aggregate to 2 m resolution, and write it dtm_rast &lt;- terra::rast(&quot;../data/point_cloud_processing_delivery/dtm_1m.tif&quot;) # quick plot terra::plot(dtm_rast, axes = F, main = &quot;DTM (m)&quot;) terra::plot(terra::vect(stand_sf), add = T, col = NA, border = &quot;black&quot;, lwd = 2) aggregate to 2 m res_temp &lt;- terra::res(dtm_rast)[1] des_res_temp &lt;- 2 if(res_temp&lt;des_res_temp){ dtm_rast &lt;- terra::aggregate( dtm_rast , fact = round(des_res_temp/res_temp) , fun = &quot;mean&quot; , na.rm = T , cores = lasR::half_cores() , filename = file.path(outdir, paste0(&quot;dtm_&quot;,des_res_temp, &quot;m.tif&quot;)) , overwrite = T ) } ## |---------|---------|---------|---------|========================================= # what is the res? terra::res(dtm_rast) ## [1] 2 2 we can also load in the CHM raster and look at that quickly chm_rast &lt;- terra::rast(&quot;../data/point_cloud_processing_delivery/chm_0.25m.tif&quot;) # quick plot chm_rast %&gt;% terra::aggregate( fact = 1/terra::res(chm_rast)[1] , fun = &quot;mean&quot; , na.rm = T , cores = lasR::half_cores() ) %&gt;% terra::plot( col = viridis::plasma(100) , axes = F, alpha = 0.8 , main = &quot;CHM (m)&quot; ) terra::plot(terra::vect(stand_sf), add = T, col = NA, border = &quot;black&quot;, lwd = 2) 3.2 Silvicultural metrics Let’s look at some common stand-level forestry metrics ### stand-level summaries silv_metrics &lt;- treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% # dplyr::filter(dbh_cm &gt;= ostory_dbh_cm) %&gt;% dplyr::ungroup() %&gt;% dplyr::group_by(unit_id, unit_name, stand_area_ha) %&gt;% dplyr::summarise( n_trees = dplyr::n_distinct(treeID) , mean_dbh_cm = mean(dbh_cm, na.rm = T) , mean_tree_height_m = mean(tree_height_m, na.rm = T) , loreys_height_m = sum(basal_area_m2*tree_height_m, na.rm = T) / sum(basal_area_m2, na.rm = T) , basal_area_m2 = sum(basal_area_m2, na.rm = T) , sum_dbh_cm_sq = sum(dbh_cm^2, na.rm = T) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( trees_per_ha = (n_trees/stand_area_ha) , basal_area_m2_per_ha = (basal_area_m2/stand_area_ha) , qmd_cm = sqrt(sum_dbh_cm_sq/n_trees) ) %&gt;% dplyr::select(-c(sum_dbh_cm_sq)) ### export tabular write.csv( silv_metrics , file.path(outdir, &quot;stand_silv_metrics.csv&quot;) , row.names = F , append = F ) stand-level silvicultural summary silv_metrics %&gt;% dplyr::select( unit_name , stand_area_ha , n_trees , mean_dbh_cm , qmd_cm , mean_tree_height_m , loreys_height_m , trees_per_ha , basal_area_m2_per_ha ) %&gt;% dplyr::mutate( dplyr::across( .cols = c(stand_area_ha, n_trees) , .fns = ~ scales::comma(.x, accuracy = 1) ) ) %&gt;% kableExtra::kbl( digits = 1 , escape = F , caption = &quot;Silvicultral metrics by stand unit&quot; , col.names = c( &quot;Unit Name&quot; , &quot;area (ha)&quot; , &quot;trees&quot; , &quot;mean&lt;br&gt;DBH (cm)&quot; , &quot;QMD (cm)&quot; , &quot;mean&lt;br&gt;Ht. (m)&quot; , &quot;Loreys&lt;br&gt;Ht. (m)&quot; , &quot;TPH&quot; , &quot;BA&lt;br&gt;m&lt;sup&gt;2&lt;/sup&gt; ha&lt;sup&gt;-1&lt;/sup&gt;&quot; ) ) %&gt;% kableExtra::kable_styling() Table 3.1: Silvicultral metrics by stand unit Unit Name area (ha) trees meanDBH (cm) QMD (cm) meanHt. (m) LoreysHt. (m) TPH BAm2 ha-1 300 RD to Rim 120 47,940 24.2 31.9 12.0 23.3 401.0 32.0 Middle Kehl Canyon 201 107,684 20.7 28.1 10.6 22.4 537.1 33.3 3.2.1 Height Distribution # there are always tree heights treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, color = unit_name, fill = unit_name)) + ggplot2::geom_density(alpha = 0.5) + ggplot2::facet_grid(cols = dplyr::vars(unit_name)) + ggplot2::scale_color_manual(values = my_pal) + ggplot2::scale_fill_manual(values = my_pal) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;&quot;, subtitle = &quot;Distribution of tree height by stand&quot;) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.y = ggplot2::element_blank() , axis.ticks.y = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) 3.2.2 DBH Distribution treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = dbh_cm, color = unit_name, fill = unit_name)) + ggplot2::geom_density(alpha = 0.5) + ggplot2::facet_grid(cols = dplyr::vars(unit_name)) + ggplot2::scale_color_manual(values = my_pal) + ggplot2::scale_fill_manual(values = my_pal) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree DBH (cm)&quot;, y = &quot;&quot;, subtitle = &quot;Distribution of tree DBH by stand&quot;) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.y = ggplot2::element_blank() , axis.ticks.y = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) 3.3 Spatial Forest Structure Because we have a spatial tree list, we can look at the spatial arrangement of forest structural metrics first, let’s make a function to crop the raster to a stand and plot the raster and stand together # function to crop and plot a raster data for a stand plot_raster_stand_fn &lt;- function( rast , stand , buffer=5 , des_res=1 , agg_fun = &quot;mean&quot; , stand_color = &quot;black&quot; , my_title = &quot;&quot; , scale_name = &quot;&quot; , bbox = F ) { # crop the raster crop_rast &lt;- rast %&gt;% terra::crop( stand %&gt;% sf::st_bbox() %&gt;% sf::st_as_sfc() %&gt;% sf::st_buffer(buffer) %&gt;% terra::vect() %&gt;% terra::project(terra::crs(rast)) ) if(!bbox){ crop_rast &lt;- crop_rast %&gt;% terra::mask( stand %&gt;% sf::st_buffer(buffer) %&gt;% terra::vect() %&gt;% terra::project(terra::crs(rast)) ) } # aggregate the raster res_temp &lt;- terra::res(rast)[1] if(res_temp&lt;des_res){ crop_rast &lt;- terra::aggregate( crop_rast , fact = round(des_res/res_temp) , fun = agg_fun , na.rm = T , cores = lasR::half_cores() ) } # plot it crop_rast %&gt;% terra::as.data.frame(xy=T) %&gt;% dplyr::rename(f=3) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile( mapping = ggplot2::aes(x=x, y=y, fill = f) ) + ggplot2::geom_sf( data = stand %&gt;% sf::st_transform(terra::crs(rast)) , color = stand_color, fill = NA, lwd = 1.5 ) + ggplot2::labs(title = my_title, fill = scale_name) + ggplot2::theme_void() } # plot_raster_stand_fn( # dtm_rast # , stand = stand_sf[1,] # , buffer = 5 # , des_res = 1 # , stand_color = my_pal[1] # , my_title = &quot;DTM&quot; # , scale_name = &quot;DTM (m)&quot; # ) 3.3.1 CHM the canopy height model is a raster dataset that represents the height of objects above the ground, we set the minimum height at 2 m # get the plot for each stand p_temp &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = max(terra::values(chm_rast), na.rm = T), ll = 0){ plot_raster_stand_fn( chm_rast , stand = stand_sf[x,] , buffer = 5 , des_res = 2 , stand_color = &quot;black&quot; , my_title = &quot;Canopy Height Model&quot; , scale_name = &quot;CHM (m)&quot; , agg_fun = &quot;max&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_viridis_c(option = &quot;plasma&quot;,limits = c(ll,ul)) }) ## |---------|---------|---------|---------|========================================= check out our plots p_temp ## [[1]] ## ## [[2]] 3.3.2 Function to Rasterize the CHM was easy to plot because it was already a raster dataset; what if we want to plot a raster of a metric based on our vector tree data? we need to define a function to rasterize our spatial tree list by aggregating the metric within a raster cell vect_to_rast_fn &lt;- function( vect , des_res = 1 , buffer = 5 , fun = &quot;mean&quot; # function(x){mean(x, na.rm=T)} , field = &quot;your_vector_attribute&quot; , zero_na = F ) { # sample sf object with a bounding box + buffer my_sf &lt;- vect %&gt;% sf::st_bbox() %&gt;% sf::st_as_sfc(crs = sf::st_crs(vect)) %&gt;% sf::st_buffer(buffer) # extract bbox coordinates bbox &lt;- sf::st_bbox(my_sf) # create a terra raster using the bbox my_raster &lt;- terra::rast( xmin = bbox[1], ymin = bbox[2], xmax = bbox[3], ymax = bbox[4] , resolution = des_res , crs = sf::st_crs(vect) ) # rasterize the vector data onto the grid, using a desired aggregation function (e.g., &quot;mean&quot;) rasterized_data &lt;- terra::rasterize( x = vect , y = my_raster , fun = fun , field = field ) if(zero_na){rasterized_data &lt;- terra::subst(rasterized_data,NA,0)} terra::crs(rasterized_data) &lt;- vect %&gt;% terra::vect() %&gt;% terra::crs() return(rasterized_data) } # vect_to_rast_fn( # treetops_sf %&gt;% dplyr::filter(unit_id==treetops_sf$unit_id[2]) # , des_res = 10 # , field = &quot;dbh_cm&quot; # , fun = &quot;mean&quot; #function(x){mean(x, na.rm=T)} # , zero_na = F # ) %&gt;% # # terra::plot() # terra::summary() we can combine our create a raster and plot it function # we can combine our create a raster and plot it function vect_to_rast_plot_fn &lt;- function( trees_vect , stand_vect , fun = &quot;mean&quot; # function(x){mean(x, na.rm=T)} , field = &quot;your_vector_attribute&quot; , zero_na = F , buffer = 5 , des_res = 10 , stand_color = &quot;black&quot; , my_title = &quot;&quot; , scale_name = &quot;&quot; ) { # get the raster rast &lt;- vect_to_rast_fn( vect = trees_vect , des_res = des_res , buffer = buffer , fun = fun , field = field , zero_na = zero_na ) # plot it plot_raster_stand_fn( rast = rast , stand = stand_vect , buffer = buffer , des_res = des_res , stand_color = stand_color , my_title = my_title , scale_name = scale_name ) } 3.3.3 Mean DBH take this for a spin to get a raster of mean DBH plotted with our stand boundary # get the plot for each stand p_temp &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = max(treetops_sf$dbh_cm,na.rm = T), ll = 0){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = &quot;mean&quot; , field = &quot;dbh_cm&quot; , zero_na = F , buffer = 5 , des_res = 20 , stand_color = &quot;black&quot; , my_title = &quot;Arrangement of Mean DBH&quot; , scale_name = &quot;DBH (cm)&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_viridis_c(option = &quot;mako&quot;, direction = -1, limits = c(ll,ul)) }) check out our plots p_temp ## [[1]] ## ## [[2]] 3.3.4 QMD to get the quadratic mean diameter (QMD) we are going to have to create a custom function that takes a list of DBH values and returns a single value QMD is a measure of the diameter of the tree of mean basal area: \\[ \\textrm{quadratic mean diameter (QMD)} = \\sqrt{\\frac{\\sum{d_{i}^{2}}}{n}} \\] , where \\(d_{i}\\) is the diameter at breast height of an individual tree, and \\(n\\) is the total number of trees. qmd_fn &lt;- function(x) { sqrt(sum(x^2, na.rm=T)/length(x[!is.na(x)])) } # get the plot for each stand p_temp &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = round(qmd_fn(max(treetops_sf$dbh_cm,na.rm = T))*.95), ll = 0){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = qmd_fn , field = &quot;dbh_cm&quot; , zero_na = F , buffer = 5 , des_res = 20 , stand_color = &quot;black&quot; , my_title = &quot;Arrangement of Quadratic Mean Diameter&quot; , scale_name = &quot;QMD (cm)&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_viridis_c(option = &quot;cividis&quot;, begin = 0.55, limits = c(ll,ul)) }) check out our plots p_temp ## [[1]] ## ## [[2]] 3.3.5 Basal Area to get Basal Area (BA) in square meters per hectare we will also need to create a function that uses the area of the raster cell # get the plot for each stand # max(silv_metrics$basal_area_m2_per_ha)*2 p_temp &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, my_res = 20, ul = NA, ll = NA){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = function(x){sum(x, na.rm=T)/((my_res^2)/10000)} , field = &quot;basal_area_m2&quot; , zero_na = F , buffer = 5 , des_res = my_res , stand_color = &quot;black&quot; , my_title = &quot;Arrangement of Basal Area&quot; , scale_name = latex2exp::TeX(&quot;BA ($m ^ 2 \\\\cdot ha^{-1}$)&quot;) ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + harrypotter::scale_fill_hp(option = &quot;slytherin&quot;, limits = c(ll,ul)) }) check out our plots p_temp ## [[1]] ## ## [[2]] 3.3.6 TPH to get trees per hectare (TPH) we will also need to create a function that uses the area of the raster cell # get the plot for each stand # max(silv_metrics$basal_area_m2_per_ha)*2 p_temp &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, my_res = 20, ul = 2333, ll = NA){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = function(x){length(x)/((my_res^2)/10000)} , field = &quot;tree_height_m&quot; # just need something to count , zero_na = F , buffer = 5 , des_res = my_res , stand_color = &quot;black&quot; , my_title = &quot;Arrangement of Trees per Hectare&quot; , scale_name = &quot;TPH&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + harrypotter::scale_fill_hp(option = &quot;mischief&quot;, limits = c(ll,ul)) }) check out our plots p_temp ## [[1]] ## ## [[2]] "],["validate-tree-detection-and-crown-delineation.html", "Section 4 Validate Tree Detection and Crown Delineation 4.1 NeonTreeEvaluation overview 4.2 lidar data in NeonTreeEvaluation 4.3 Example validation process 4.4 Full validation process", " Section 4 Validate Tree Detection and Crown Delineation In this section we’ll use the benchmark data made available in the NeonTreeEvaluation data set (Weinstein et al. 2021) to evaluate our process for lidar-based tree detection. We’ll implement our tree detection process via the [cloud2trees]https://github.com/georgewoolsey/cloud2trees) package First, load the standard libraries # bread-and-butter library(tidyverse) # the tidyverse library(viridis) # viridis colors library(harrypotter) # hp colors library(RColorBrewer) # brewer colors library(scales) # work with number and plot scales library(latex2exp) # visualization library(mapview) # interactive html maps library(kableExtra) # tables library(patchwork) # combine plots library(ggnewscale) # ggnewscale library(rgl) # rgl plotting # spatial analysis library(sf) # simple features library(lidR) # lidar data library(cloud2trees) # tha cloud2trees library(NeonTreeEvaluation) # benchmark data 4.1 NeonTreeEvaluation overview Weinstein et al. (2021) developed: a benchmark dataset of individual canopy crowns derived from multi-sensor imagery in the National Ecological Observatory Network (Table 1) that provides: 1) co-registered remote sensing data from multiple sensors (LiDAR, RGB imagery, and hyperspectral imagery) to allow comparisons of methods based on any single sensor (e.g., for LiDAR based methods), or any combination of sensors (e.g., combining RGB and hyperspectral), and 2) three types of evaluation data to allow assessing both ‘tree detection’, defined as the identifying the location of individual trees using evaluation data with a point at the crown center , and ‘crown delineation’ defined as identifying the boundary edge of crowns across a broad range of forest types. The benchmark is designed to allow flexibility in both workflow and sensor selection. (p. 2) Table 1. Summary of datasets included in the benchmark dataset. All sensor data has been cropped to the extent of NEON field sampling plots. Note the three data labeled as “Evaluation data” in the table. If you are asking “why three evaluation datasets?”, the authors provide some detail: The inclusion of multiple evaluation types is critical because each type of evaluation data has strengths and limitations in evaluating model performance. Field collected stems are the most common evaluation data used in crown detection work due to high confidence that each stem represents a location of a single tree. However, the position of a tree stem can fail to accurately represent the position of the crown as viewed from above due to a combination of spatial errors in alignment with the image data and the tendency for trees to grow at acute angles (tree lean is not measured in the NEON data), such that the center of the crown and position of the stem can be offset by several meters….Image-annotated crowns are relatively easy to scale, allowing the collection of data for a wide range of forest types and for annotation of every visible crown in the image. Using image-annotated crowns supports the evaluation of methods across a broad range of forest types and allows both recall and precision to be calculated. However, since these annotations are not generated by an observer in the field there can be errors due to interpreting the images. This problem is solved using field-annotated crowns in which an observer annotates the remote-sensing imagery on a tablet while in the field [33]. The main limitation to this approach is that it is labor intensive, meaning that only a relatively small amount of validation data can be collected, making it difficult to obtain a large number of crowns across broad scales or assess model precision. Given the tradeoffs in each evaluation type, providing multiple criteria is a useful way of balancing the need for broad scale model verification with rigorous evaluation of field-based measurements. (p. 14-15) To evaluate the performance of our aerial point cloud-based algorithm for 1) tree detection and 2) crown delineation using NeonTreeEvaluation we need to ensure our tree polygon data is formatted properly: This package takes a standard submission format of predicted crowns in either bounding box or polygons as input and returns the evaluation scores of the detections for each of the three evaluation datasets. This reproducible workflow will facilitate creating a transparent process for future comparisons among crown detection algorithms. (p. 14) The authors describe the “standard submission format” on the package GitHub: Each row contains information for one predicted bounding box. The plot_name should be named the same as the files in the dataset without extension (e.g. SJER_021_2018 not SJER_021_2018.tif) and not the full path to the file on disk. Not all evaluation data are available for all plots. Functions like evaluate_field_crowns and evaluate_image_crowns will look for matching plot name and ignore other plots. Depending on the speed of the algorithm, the simplest thing to do is predict all images in the RGB folder (see list_rgb()) and the package will handle matching images with the correct data to the correct evaluation procedure…Instead of bounding boxes, some methods may return polygons. To submit as polygons, create a single unprojected shapefile with polygons in image coordinates. Polygons must be complete with no holes. Here is an example of the above csv file in polygon format. Here the xmin, xmax, etc. columns are ignored since the information is stored in the geometry data. Simple feature collection with 6 features and 7 fields geometry type: POLYGON dimension: XY bbox: xmin: 30.39723 ymin: 122.1164 xmax: 397.5746 ymax: 400 CRS: NA xmin ymin xmax ymax score label plot_name 1 41.01716 230.8854 151.08607 342.6985 0.8098674 Tree DSNY_014_2019 2 357.32129 122.1164 397.57458 159.3758 0.6968824 Tree DSNY_014_2019 3 30.39723 136.9157 73.79434 184.9473 0.5713338 Tree DSNY_014_2019 4 260.65921 285.6689 299.68811 326.7933 0.5511004 Tree DSNY_014_2019 5 179.34564 371.6130 232.49385 400.0000 0.4697072 Tree DSNY_014_2019 6 316.27377 378.9802 363.67542 400.0000 0.3259409 Tree DSNY_014_2019 st_sfc.lst. 1 POLYGON ((41.01716 230.8854... 2 POLYGON ((357.3213 122.1164... 3 POLYGON ((30.39723 136.9157... 4 POLYGON ((260.6592 285.6689... 5 POLYGON ((179.3456 371.613,... 6 POLYGON ((316.2738 378.9802... So we are going to: run cloud2trees::cloud2trees() on all lidar data, combine into a single tree list with a row unique by a detected tree and the plot_name column (e.g. “SJER_021_2018”), as an unprojected sf data with polygons in image coordinates. We may need to run cloud2trees::simplify_multipolygon_crowns() prior to submission. 4.2 lidar data in NeonTreeEvaluation we first have to download evaluation data from the Zenodo archive (1GB), use the download() function to place the data in the correct package location. Download the much larger training data, set training=TRUE. NeonTreeEvaluation::download(training = T, force = F) let’s find what data is available # i did some digging around and the lidar data is here lidar_dir_temp &lt;- system.file(package = &quot;NeonTreeEvaluation&quot;, &quot;extdata&quot;, &quot;NeonTreeEvaluation&quot;, &quot;evaluation&quot;, &quot;LiDAR&quot;) # files lidar_files_temp &lt;- lidar_dir_temp %&gt;% list.files(recursive = T, pattern = &quot;.*\\\\.(laz|las)$&quot;, full.names = T) %&gt;% unique() # look at this lidar_files_temp %&gt;% basename() %&gt;% sample(size = 9) ## [1] &quot;HARV_058_2019.laz&quot; &quot;RMNP_025_2018.laz&quot; &quot;KONZ_025_2019.laz&quot; ## [4] &quot;OSBS_041_2019.laz&quot; &quot;MLBS_12.las&quot; &quot;OSBS_114.las&quot; ## [7] &quot;unnamed_plot_27.las&quot; &quot;unnamed_plot_153.las&quot; &quot;OSBS_30.las&quot; # let&#39;s pull out all sites with `.laz` data and create a data frame for tracking purposes lidar_df &lt;- lidar_files_temp %&gt;% dplyr::as_tibble() %&gt;% dplyr::rename(f_path = 1) %&gt;% # create some other variables dplyr::mutate( plot_name = f_path %&gt;% basename() %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;) ) # what? lidar_df %&gt;% dplyr::glimpse() ## Rows: 2,186 ## Columns: 2 ## $ f_path &lt;chr&gt; &quot;C:/Program Files/R/R-4.3.0/library/NeonTreeEvaluation/extda… ## $ plot_name &lt;chr&gt; &quot;2018_SJER_3_252000_4104000_image_628&quot;, &quot;2018_SJER_3_252000_… that’s a lot of files…let’s only process the sites with evaluation data # there are functions to get a list of all evaluation data # let&#39;s use these to filter our lidar files plotnames_temp &lt;- c( NeonTreeEvaluation::list_annotations() , NeonTreeEvaluation::list_field_stems() # this one includes file paths, so we have to clean , NeonTreeEvaluation::list_field_crowns() %&gt;% stringr::str_match(pattern=&quot;(\\\\w+).tif&quot;) %&gt;% .[,2] # there are plot_names from the submission data too , NeonTreeEvaluation::submission_polygons$plot_name %&gt;% unique() , NeonTreeEvaluation::submission$plot_name %&gt;% unique() ) %&gt;% unique() # huh? plotnames_temp %&gt;% sample(11) ## [1] &quot;JERC_064_2018&quot; &quot;DSNY_024_2019&quot; ## [3] &quot;SOAP_002_2019&quot; &quot;MLBS_073&quot; ## [5] &quot;SOAP_054_2019&quot; &quot;DSNY_020_2019&quot; ## [7] &quot;SJER_053_2018&quot; &quot;HARV_052_2018&quot; ## [9] &quot;unnamed_plot_102_competition&quot; &quot;MLBS_9_competition&quot; ## [11] &quot;TALL_008&quot; filter our lidar data list lidar_df &lt;- lidar_df %&gt;% #filter based on plots in evaluation data dplyr::filter(plot_name %in% plotnames_temp) %&gt;% # pull out site dplyr::mutate( siteID = stringr::str_extract(plot_name, &quot;[A-Z]+&quot;) ) # what? lidar_df %&gt;% dplyr::glimpse() ## Rows: 1,732 ## Columns: 3 ## $ f_path &lt;chr&gt; &quot;C:/Program Files/R/R-4.3.0/library/NeonTreeEvaluation/extda… ## $ plot_name &lt;chr&gt; &quot;2018_SJER_3_252000_4104000_image_628&quot;, &quot;2018_SJER_3_252000_… ## $ siteID &lt;chr&gt; &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJE… we will want to limit our evaluation to only sites with conifer trees since cloud2trees implements methods developed specifically to quantify conifer forest structure that may not be appropriate for other uses. we’ll use the field data in the package to look for NEON sites with conifer trees. We’ll use the NEON plant list to identify conifer species: https://data.neonscience.org/taxonomic-lists?taxonTypeCode=PLANT (click “DOWNLOAD TAXONOMIC LIST”). We’ll filter for species belonging to Class Pinopsida. conifer_spp &lt;- readr::read_csv( &quot;../data/OS_TAXON_PLANT-20220330T142149.csv&quot; , show_col_types = F , progress = F ) %&gt;% dplyr::filter( tolower(`class`) %in% c(&quot;pinopsida&quot;) ) %&gt;% dplyr::mutate( taxonID = toupper(taxonID) , vernacularName = tolower(vernacularName) , genus = stringr::str_to_title(genus) ) %&gt;% dplyr::distinct(taxonID, vernacularName, genus) what are some of these conifers? # huh? conifer_spp %&gt;% dplyr::slice_sample(n = 10) %&gt;% kableExtra::kbl(caption = &quot;Conifer species taxonID examples&quot;) %&gt;% kableExtra::kable_styling() Table 4.1: Conifer species taxonID examples taxonID vernacularName genus JUOC western juniper Juniperus TAAS pond cypress Taxodium PIEN2 apache pine Pinus PICEA spruce Picea PIPOP ponderosa pine Pinus PIPOB2 ponderosa pine Pinus PLATY6 platycladus Platycladus CUNNI cunninghamia Cunninghamia PIRE5 papershell pinyon Pinus PIARS2 arizona pine Pinus filter for NEON sites that have conifer trees based on field data from all terrestrial NEON sites with qualifying woody vegetation: https://data.neonscience.org/data-products/DP1.10098.001 conifer_sites &lt;- NeonTreeEvaluation::field %&gt;% dplyr::left_join( conifer_spp %&gt;% dplyr::mutate(is_conifer = 1) , by = &quot;taxonID&quot; ) %&gt;% dplyr::mutate(is_conifer = dplyr::coalesce(is_conifer, 0)) %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( tot = dplyr::n() , conifer = sum(is_conifer) , latitude = mean(plotLatitude) , longitude = mean(plotLongitude) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(pct_conifer = conifer/tot) what is the breakdown of woody vegetation sampled in NEON sites by the percent conifer? conifer_sites %&gt;% dplyr::select(-c(longitude,latitude)) %&gt;% dplyr::arrange(desc(pct_conifer), desc(tot)) %&gt;% dplyr::slice_head(n = 19) %&gt;% kableExtra::kbl(caption = &quot;Conifers in NEON sites&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 4.2: Conifers in NEON sites siteID tot conifer pct_conifer NIWO 1804 1804 1.00 ONAQ 88 88 1.00 MOAB 29 29 1.00 HEAL 21 21 1.00 YELL 13 13 1.00 TEAK 621 619 1.00 DEJU 173 169 0.98 ABBY 268 241 0.90 RMNP 1375 1083 0.79 SOAP 503 389 0.77 DSNY 34 26 0.76 TALL 2144 1521 0.71 OSBS 1288 858 0.67 JERC 562 336 0.60 HARV 3736 1701 0.46 TREE 1303 430 0.33 BART 3636 1022 0.28 BONA 188 48 0.26 STEI 754 151 0.20 let’s only keep NEON sites with &gt;50% of the woody vegetation sampled as conifer # minimum pct conifer min_conifer_pct &lt;- .5 # data frame of sites conifer_sites &lt;- conifer_sites %&gt;% dplyr::filter(pct_conifer&gt;min_conifer_pct) finally, we’ll filter our lidar processing data for only these conifer sites lidar_df &lt;- lidar_df %&gt;% dplyr::inner_join(conifer_sites, by = &quot;siteID&quot;) %&gt;% sf::st_as_sf(coords = c(&quot;longitude&quot;,&quot;latitude&quot;), crs = 4326, remove = F) %&gt;% # filter out corrupt las files dplyr::filter( !plot_name %in% c( &quot;NIWO_005_2018&quot; , &quot;SOAP_014_2018&quot; , &quot;MOAB_003_2018&quot; , &quot;NIWO_009_2018&quot; , &quot;TEAK_028_2018&quot; , &quot;YELL_058_2020&quot; , &quot;RMNP_011_2018&quot; , &quot;YELL_030_2018&quot; , &quot;YELL_051_2019&quot; , &quot;SOAP_014_2019&quot; , &quot;TEAK_005_2018&quot; ) ) what NEON sites have conifers and the most lidar plots lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(siteID) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::slice_head(n=11) %&gt;% kableExtra::kbl(caption = &quot;NEON sites with conifers and lidar plots&quot;) %&gt;% kableExtra::kable_styling() Table 4.3: NEON sites with conifers and lidar plots siteID n TEAK 120 DEJU 87 YELL 83 OSBS 71 SOAP 71 NIWO 63 ABBY 55 TALL 54 JERC 50 DSNY 46 HEAL 29 what is the spatial distribution of these sites? lidar_df %&gt;% dplyr::count(siteID) %&gt;% mapview::mapview( zcol = &quot;siteID&quot;, legend = F , layer.name = &quot;NEON site&quot; , col.regions = viridis::turbo(n=nrow(conifer_sites)) ) that’s pretty good geographic coverage and in places that we expect to have conifers ;D 4.3 Example validation process now that we have our lidar data that we can test our point cloud-based tree detection and crown segmentation process against, let’s walk through the validation for a single plot we’ll test with a single point cloud in our filtered list from conifer sites with validation data lidar_df_row &lt;- 1 # lidar_df$f_path[47] %&gt;% lidR::readLAS() %&gt;% lidR::st_crs() 4.3.1 Preliminaries 4.3.1.1 View the point cloud this step isn’t necessary for validation, but let’s see what this point cloud data looks like We can plot the point cloud with and color by the point height lidar_df$f_path[lidar_df_row] %&gt;% lidR::readLAS() %&gt;% lidR::plot( color = &quot;Z&quot;, breaks = &quot;quantile&quot;, bg = &quot;white&quot;, legend = T , pal = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) ) there are trees in there for sure (and conifer trees by the looks of it) let’s look at the co-registered RGB imagery (notice that the NeonTreeEvaluation commands rely on the deprecated raster package :) # read rgb rgb_temp &lt;- lidar_df$plot_name[lidar_df_row] %&gt;% NeonTreeEvaluation::get_data(type = &quot;rgb&quot;) %&gt;% raster::stack() # read image annotated crown data and make polygons polys_temp &lt;- lidar_df$plot_name[lidar_df_row] %&gt;% NeonTreeEvaluation::get_data(type = &quot;annotations&quot;) %&gt;% NeonTreeEvaluation::xml_parse() polys_temp &lt;- NeonTreeEvaluation::boxes_to_spatial_polygons(polys_temp,rgb_temp) # plot terra::plotRGB(rgb_temp %&gt;% terra::rast()) terra::plot( polys_temp %&gt;% terra::vect() , col = NA, border = &quot;red&quot; , lwd = 2 , add = TRUE ) 4.3.1.2 ITD variable window We discussed our method for individual tree detection (ITD) in this prior section. For our validation, we’ll be using the default window size in the cloud2trees::cloud2trees() and cloud2trees::raster2trees() settings. Let’s see what that looks like ws_temp &lt;- cloud2trees::itd_ws_functions()[[&quot;log_fn&quot;]] ggplot() + geom_function(fun=ws_temp, lwd=1.2, color = &quot;navy&quot;) + xlim(-5,60) + labs( x = &quot;heights&quot;, y = &quot;ws&quot; , subtitle = &quot;`cloud2trees` default ITD variable window function&quot; ) + theme_light() 4.3.2 Filter for “canopy” trees First, we’ll process the point cloud and get a tree list using our cloud2trees::cloud2trees() method with all defaults except we’ll raise the minimum height of trees to search to 3 m. The (Weinstein et al. 2021) benchmark was developed specifically for “canopy” trees and the field-collected stems evaluation data only includes &gt;10 cm DBH trees: NEON field crews sample all trees within a plot that are greater than 10cm DBH, regardless of whether the tree crown can be seen in the remote sensing image data. While understory tree detection is an important area of future work, the scope of this benchmark is focused on crowns in the canopy that are visible from above. (p. 10) In order to set up our point cloud-based algorithm to find “canopy” trees, we’ll identify the shortest live tree in the field-collected stems data to set our min_height in cloud2trees::cloud2trees() we’ll use the filters found in clean_field_data() from NeonTreeEvaluation as an internal function percentile_for_ht &lt;- 0.05 # get non-na heights from neon field measured trees neon_field_heights &lt;- NeonTreeEvaluation::field %&gt;% # filters found in `clean_field_data()` dplyr::filter( !is.na(itcEasting) , !stringr::str_detect(eventID,&quot;2014&quot;) , growthForm %in% c(&quot;single bole tree&quot;,&quot;multi-bole tree&quot;,&quot;small tree&quot;,&quot;sapling&quot;) , stemDiameter&gt;15 , (height&gt;3|is.na(height)) ) %&gt;% # getting only non-na dplyr::filter(!is.na(height)) # get 5th tile ht by site neon_site_heights &lt;- neon_field_heights %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( site_prcntl_ht = quantile(floor(height), probs = percentile_for_ht, na.rm = T) ) %&gt;% dplyr::ungroup() look at the summary of height data across all sites and plots summary(neon_field_heights$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.10 11.60 15.90 16.48 20.30 119.80 what does this look like for each NEON site? neon_field_heights %&gt;% dplyr::inner_join(neon_site_heights) %&gt;% ggplot(aes(x = height, group = siteID)) + geom_density(color = &quot;gold&quot;,fill = &quot;gold&quot;, alpha = 0.7, lwd = 1.2) + geom_vline(aes(xintercept = site_prcntl_ht), linetype = &quot;dashed&quot;) + facet_wrap(facets = vars(siteID), ncol = 7, scales = &quot;free&quot;) + scale_x_continuous(breaks = scales::breaks_extended(6)) + labs( x = &quot;height (m)&quot;, y = &quot;&quot; , subtitle = paste0( &quot;heights of \\&quot;canopy\\&quot; trees in NeonTreeEvaluation with &quot; , scales::number(percentile_for_ht*100, accuracy = 1) , &quot;th percentile by site&quot; ) ) + theme_light() + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) what is the overall 5th percentile to use for minimum height? (neon_min_ht &lt;- quantile(neon_field_heights$height, probs = percentile_for_ht) %&gt;% floor()) ## 5% ## 6 let’s attach this to our data frame of lidar data files lidar_df &lt;- lidar_df %&gt;% dplyr::left_join(neon_site_heights, by = &quot;siteID&quot;) %&gt;% dplyr::mutate(site_prcntl_ht = dplyr::coalesce(site_prcntl_ht,neon_min_ht)) 4.3.3 cloud2trees::cloud2trees() ans &lt;- cloud2trees::cloud2trees( input_las_dir = lidar_df$f_path[lidar_df_row] , output_dir = tempdir() , min_height = lidar_df$site_prcntl_ht[lidar_df_row] ) ## Read files headers: [==========] 100% (1 threads) Overall: [ ] 0% (1 threads) | : no progress Overall: [ ] 0% (1 threads) | read_las: [ ] 0% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 1% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 2% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 3% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 4% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 5% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 6% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 7% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 8% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [ ] 9% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 10% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 11% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 12% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 13% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 14% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 15% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 16% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 17% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 18% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [= ] 19% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 20% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 21% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 22% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 23% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 24% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 25% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 26% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 27% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 28% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [== ] 29% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 30% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 31% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 32% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 33% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 34% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 35% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 36% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 37% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 38% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [=== ] 39% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 40% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 41% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 42% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 43% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 44% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 45% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 46% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 47% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 48% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==== ] 49% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 50% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 51% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 52% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 53% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 54% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 55% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 56% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 57% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 58% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [===== ] 59% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 60% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 61% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 62% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 63% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 64% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 65% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 66% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 67% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 68% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [====== ] 69% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 70% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 71% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 72% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 73% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 74% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 75% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 76% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 77% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 78% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======= ] 79% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 80% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 81% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 82% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 83% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 84% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 85% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 86% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 87% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 88% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [======== ] 89% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 90% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 91% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 92% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 93% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 94% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 95% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 96% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 97% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 98% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [========= ] 99% (1 threads) Overall: [ ] 0% (1 threads) | read_las: [==========] 100% (1 threads) Overall: [ ] 0% (1 threads) | CSF: no progress Overall: [ ] 0% (1 threads) | Delaunay triangulation: no progress Overall: [ ] 0% (1 threads) | Delaunay triangulation: no progress Overall: [ ] 0% (1 threads) | Interpolation: [ ] 0% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 1% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 2% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 3% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 4% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 5% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 6% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 7% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 8% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 9% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [==========] 100% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 0% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 1% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 2% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 3% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 4% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 5% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 6% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 7% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 8% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [ ] 9% (10 threads) Overall: [ ] 0% (1 threads) | Interpolation: [==========] 100% (10 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 0% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 1% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 2% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 3% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 4% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 5% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 6% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 7% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 8% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [ ] 9% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 10% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 11% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 12% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 13% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 14% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 15% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 16% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 17% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 18% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [= ] 19% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 20% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 21% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 22% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 23% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 24% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 25% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 26% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 27% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 28% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [== ] 29% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 30% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 31% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 32% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 33% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 34% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 35% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 36% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 37% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 38% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [=== ] 39% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 40% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 41% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 42% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 43% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 44% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 45% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 46% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 47% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 48% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==== ] 49% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 50% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 51% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 52% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 53% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 54% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 55% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 56% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 57% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 58% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [===== ] 59% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 60% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 61% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 62% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 63% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 64% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 65% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 66% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 67% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 68% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [====== ] 69% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 70% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 71% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 72% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 73% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 74% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 75% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 76% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 77% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 78% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======= ] 79% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 80% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 81% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 82% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 83% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 84% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 85% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 86% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 87% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 88% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [======== ] 89% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 90% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 91% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 92% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 93% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 94% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 95% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 96% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 97% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 98% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [========= ] 99% (1 threads) Overall: [ ] 0% (1 threads) | Write LAS: [==========] 100% (1 threads) Overall: [==========] 100% (1 threads) | Overall: [==========] 100% (1 threads) quick check our our heights ans$crowns_sf$tree_height_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6.498 13.042 30.052 28.916 46.069 52.462 4.3.4 Format extracted tree polygons we need to format our extracted trees for NeonTreeEvaluation evaluation and submission first, we’ll simplify multipolygon crowns ans$crowns_sf &lt;- cloud2trees::simplify_multipolygon_crowns(ans$crowns_sf) check out our extracted trees ggplot2::ggplot() + ggplot2::geom_tile( data = ans$chm_rast %&gt;% terra::as.data.frame(xy=T) %&gt;% dplyr::rename(f=3) , mapping = ggplot2::aes(x = x, y = y, fill = f) , na.rm = T ) + harrypotter::scale_fill_hp( option = &quot;gryffindor&quot; , breaks = scales::breaks_extended(n=10) ) + ggplot2::geom_sf( data = ans$crowns_sf , fill = NA, color = &quot;gray33&quot;, lwd = 1 ) + ggplot2::scale_x_continuous(expand = c(0, 0)) + ggplot2::scale_y_continuous(expand = c(0, 0)) + ggplot2::labs(x = &quot;&quot;, y = &quot;&quot;, fill = &quot;CHM (m)&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text = ggplot2::element_blank()) we’ll reserve judgement and let the data talk format the data for NeonTreeEvaluation submission and evaluation return_sf &lt;- ans$crowns_sf %&gt;% sf::st_set_geometry(&quot;geometry&quot;) %&gt;% dplyr::rowwise(&quot;treeID&quot;) %&gt;% dplyr::mutate( xmin = sf::st_bbox(geometry)[1] , ymin = sf::st_bbox(geometry)[2] , xmax = sf::st_bbox(geometry)[3] , ymax = sf::st_bbox(geometry)[4] ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( label = &quot;Tree&quot; , plot_name = lidar_df$plot_name[lidar_df_row] ) %&gt;% dplyr::select(xmin,xmax,ymin,ymax,label,plot_name) %&gt;% sf::st_set_crs(NA) # what? return_sf %&gt;% dplyr::glimpse() ## Rows: 23 ## Columns: 7 ## $ xmin &lt;dbl&gt; 314481.0, 314489.2, 314481.2, 314459.0, 314456.0, 314456.8, … ## $ xmax &lt;dbl&gt; 314483.2, 314496.0, 314491.5, 314463.2, 314458.0, 314460.8, … ## $ ymin &lt;dbl&gt; 4099617, 4099607, 4099606, 4099608, 4099608, 4099606, 409959… ## $ ymax &lt;dbl&gt; 4099620, 4099620, 4099619, 4099612, 4099611, 4099611, 409960… ## $ label &lt;chr&gt; &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tre… ## $ plot_name &lt;chr&gt; &quot;2018_TEAK_3_314000_4099000_image_334&quot;, &quot;2018_TEAK_3_314000_… ## $ geometry &lt;POLYGON&gt; POLYGON ((314481 4099620, 3..., POLYGON ((314489.2 40996… does this match the submission polygon data from the NeonTreeEvaluation package? NeonTreeEvaluation::submission_polygons %&gt;% dplyr::glimpse() ## Rows: 126,574 ## Columns: 8 ## $ xmin &lt;dbl&gt; 41.01716, 357.32129, 30.39723, 260.65921, 179.34564, 316.2… ## $ ymin &lt;dbl&gt; 230.8854218, 122.1164017, 136.9156647, 285.6688843, 371.61… ## $ xmax &lt;dbl&gt; 151.08607, 397.57458, 73.79434, 299.68811, 232.49385, 363.… ## $ ymax &lt;dbl&gt; 342.69846, 159.37578, 184.94730, 326.79330, 400.00000, 400… ## $ score &lt;dbl&gt; 0.8098674, 0.6968824, 0.5713338, 0.5511004, 0.4697072, 0.3… ## $ label &lt;chr&gt; &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;T… ## $ plot_name &lt;chr&gt; &quot;DSNY_014_2019&quot;, &quot;DSNY_014_2019&quot;, &quot;DSNY_014_2019&quot;, &quot;DSNY_0… ## $ st_sfc.lst. &lt;POLYGON&gt; POLYGON ((41.01716 230.8854..., POLYGON ((357.3213 122… yes, except for the “score” column which I’m pretty sure is an artifact from after evaluation? 4.3.5 Test evaluation We compared our tree detection and crown delineation results to the three types of evaluation data (i.e. “ground truth” data) presented by Weinstein et al. (2021): field-collected stems, image-annotated crowns, and field-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective, and field-annotated crowns combine the benefits of both but are highly resource-intensive. 4.3.5.1 Scores for an image-annotated crowns The main data source are image-annotated crowns, in which a single observer annotated visible trees in 200 40m x 40m images from across the United States. Get the benchmark score image-annotated “ground truth” data. in testing, including the sf polygon data did not work…switching to the bbox method with sf::st_drop_geometry() rslt_img_annttd_crwns &lt;- NeonTreeEvaluation::evaluate_image_crowns( predictions = return_sf %&gt;% sf::st_drop_geometry() , show = T , summarize = T ) ## [1] &quot;2018_TEAK_3_314000_4099000_image_334&quot; in the plot (if there is a plot), “red” boxes are crowns our point cloud-based method extracted and “black” are the image annotated crowns it looks like the overlay is generally the same but we are still extracting trees that may not be considered “canopy” trees ; what is in the return from NeonTreeEvaluation::evaluate_image_crowns() ? rslt_img_annttd_crwns %&gt;% names() ## [1] &quot;overall&quot; &quot;by_site&quot; &quot;plot_level&quot; &quot;count_error&quot; overall: must be across all NEON sites, plots, and trees included for evaluation rslt_img_annttd_crwns$overall ## # A tibble: 1 × 2 ## precision recall ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.261 0.24 by_site: must be across plots, and trees included for evaluation in a NEON sites rslt_img_annttd_crwns$by_site ## # A tibble: 1 × 3 ## # Groups: Site [1] ## Site recall precision ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TEAK 0.24 0.261 plot_level: must be across trees included for evaluation in a NEON site, plot combination rslt_img_annttd_crwns$plot_level ## # A tibble: 1 × 3 ## # Groups: plot_name [1] ## plot_name recall precision ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018_TEAK_3_314000_4099000_image_334 0.24 0.261 count_error is the number of predicted trees minus the number of “ground truth” trees but in graphical form so we’ll skip it 4.3.5.2 Scores for an field-annotated crowns The second data source is a small number of field-annotated crowns from two geographic sites. These crowns were drawn on a tablet while physically standing in the field, thereby reducing the uncertainty in crown segmentation. not all plots have field-annotated crowns NeonTreeEvaluation::evaluate_field_crowns() returns an error if “No plot names matching the field crown data, see list_field_crowns for paths to RGB field crown imagery.”… so, we’ll have to capture errors in our checks? # safe it safe_evaluate_field_crowns &lt;- purrr::safely(NeonTreeEvaluation::evaluate_field_crowns) # test it rslt_fld_crwns &lt;- safe_evaluate_field_crowns( predictions = return_sf %&gt;% sf::st_drop_geometry() , show = T , summarize = T ) did it do it? rslt_fld_crwns$error ## &lt;simpleError in .f(...): No plot names matching the field crown data, see list_field_crowns for paths to RGB field crown imagery.&gt; nope! 4.3.5.3 Scores for an field-collected stems The third data source is the NEON Woody Vegetation Structure Dataset. Each tree stem is represented by a single point. This data has been filtered to represent overstory trees visible in the remote sensing imagery. not all plots have field-collected stems NeonTreeEvaluation::evaluate_field_stems() returns an error if “No submitted plot_names with matching field stem data, see list_field_stems()”… so, we’ll have to capture errors in our checks? # safe it safe_evaluate_field_stems &lt;- purrr::safely(NeonTreeEvaluation::evaluate_field_stems) # test it rslt_fld_stems &lt;- safe_evaluate_field_stems( predictions = return_sf %&gt;% sf::st_drop_geometry() , show = T , summarize = T ) did it do it? rslt_fld_stems$error ## &lt;simpleError in .f(...): No submitted plot_names with matching field stem data, see list_field_stems()&gt; nope! 4.4 Full validation process Above, we tested the validation process for a single sample plot. Here we’ll create a function to detect trees and delineate tree crowns using our cloud2trees::cloud2trees() method to apply over all sample plots. Then we’ll compare our tree detection and crown delineation results to the three types of evaluation data (i.e. “ground truth” data) presented by Weinstein et al. (2021): field-collected stems, image-annotated crowns, and field-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective, and field-annotated crowns combine the benefits of both but are highly resource-intensive. 4.4.1 Function to extract trees let’s create a function to extract trees and format for evaluation using our lidar_df data which includes a data frame of file paths with the appropriate plot name cloud2trees_for_eval &lt;- function(lidar_df_row, lidar_df, ws, min_height = NA) { # message message(paste0(&quot;doing the work for ...... &quot;, lidar_df$plot_name[lidar_df_row])) # run c2t qc2t &lt;- purrr::safely(cloud2trees::cloud2trees) ans &lt;- qc2t( input_las_dir = lidar_df$f_path[lidar_df_row] , output_dir = tempdir() , min_height = dplyr::coalesce(min_height, lidar_df$site_prcntl_ht[lidar_df_row], 2) , ws = ws ) if(!is.null(ans$error)){return(NULL)} ans &lt;- ans$result # simp ans$crowns_sf &lt;- cloud2trees::simplify_multipolygon_crowns(ans$crowns_sf) # return return_sf &lt;- ans$crowns_sf %&gt;% sf::st_set_geometry(&quot;geometry&quot;) %&gt;% dplyr::rowwise(&quot;treeID&quot;) %&gt;% dplyr::mutate( xmin = sf::st_bbox(geometry)[1] , ymin = sf::st_bbox(geometry)[2] , xmax = sf::st_bbox(geometry)[3] , ymax = sf::st_bbox(geometry)[4] ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( label = &quot;Tree&quot; , plot_name = lidar_df$plot_name[lidar_df_row] ) %&gt;% dplyr::select(xmin,xmax,ymin,ymax,label,plot_name) %&gt;% sf::st_set_crs(NA) # give the workers a rest Sys.sleep(3) return(return_sf) } 4.4.2 Build evaluation data let’s run each point cloud through our lidar-based tree detection implemented via cloud2trees::cloud2trees() # where should we save the file? submission_fn &lt;- &quot;../data/NeonTreeEvaluation_submission.gpkg&quot; # if we don&#39;t already have the data, run it if(!file.exists(submission_fn)){ # if(T){ my_submission &lt;- # 1:nrow(lidar_df) %&gt;% # uncomment this when it gets real sample(1:nrow(lidar_df), size = 222) %&gt;% purrr::map(\\(x) cloud2trees_for_eval( lidar_df_row = x , lidar_df = lidar_df , ws = cloud2trees::itd_ws_functions()[[&quot;log_fn&quot;]] ) ) %&gt;% dplyr::bind_rows() # save it sf::st_write(my_submission, submission_fn, append = F) }else{ my_submission &lt;- sf::st_read(submission_fn) } what did we get? my_submission %&gt;% dplyr::glimpse() ## Rows: 9,547 ## Columns: 7 ## $ xmin &lt;dbl&gt; 553118.8, 553114.0, 553114.5, 553119.0, 553102.2, 553102.5, … ## $ xmax &lt;dbl&gt; 553120.0, 553115.5, 553115.8, 553119.8, 553103.0, 553103.2, … ## $ ymin &lt;dbl&gt; 5065836, 5065821, 5065821, 5065819, 5065812, 5065812, 410758… ## $ ymax &lt;dbl&gt; 5065837, 5065823, 5065823, 5065819, 5065813, 5065812, 410758… ## $ label &lt;chr&gt; &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tre… ## $ plot_name &lt;chr&gt; &quot;ABBY_021_2019&quot;, &quot;ABBY_021_2019&quot;, &quot;ABBY_021_2019&quot;, &quot;ABBY_021… ## $ geom &lt;POLYGON&gt; POLYGON ((553119.2 5065837,..., POLYGON ((553114.8 50658… trees detected by plot my_submission %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(plot_name) %&gt;% dplyr::slice_sample(n=14) %&gt;% dplyr::arrange(plot_name) %&gt;% kableExtra::kbl(caption = &quot;cloud2trees ITD results by plot&quot;, digits = 0) %&gt;% kableExtra::kable_styling() Table 4.4: cloud2trees ITD results by plot plot_name n 2018_TEAK_3_318000_4107000_image_416 9 2018_TEAK_3_323000_4098000_image_691 40 DEJU_005_2018 12 DEJU_018_2018 35 DEJU_045_2019 13 DSNY_027_2018 10 NIWO_007_2018 112 NIWO_015_2020 112 NIWO_040_2020 73 SOAP_057_2019 24 SOAP_060_2018 5 TEAK_061_2018 32 YELL_052_2018 39 YELL_061_2019 14 4.4.3 Full evaluation We compared our tree detection and crown delineation results to the three types of evaluation data (i.e. “ground truth” data) presented by Weinstein et al. (2021): field-collected stems, image-annotated crowns, and field-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective, and field-annotated crowns combine the benefits of both but are highly resource-intensive. 4.4.3.1 Scores for an image-annotated crowns The main data source are image-annotated crowns, in which a single observer annotated visible trees in 200 40m x 40m images from across the United States. Get the benchmark score image-annotated “ground truth” data. in testing, including the sf polygon data did not work…switching to the bbox method with sf::st_drop_geometry() rslt_img_annttd_crwns &lt;- NeonTreeEvaluation::evaluate_image_crowns( predictions = my_submission %&gt;% sf::st_drop_geometry() , show = F , summarize = T ) overall: must be across all NEON sites, plots, and trees included for evaluation rslt_img_annttd_crwns$overall %&gt;% kableExtra::kbl( caption = &quot;Overall: cloud2trees ITD performance versus image-annotated crowns&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.5: Overall: cloud2trees ITD performance versus image-annotated crowns precision recall 0.301 0.42 by_site: must be across plots, and trees included for evaluation in a NEON sites rslt_img_annttd_crwns$by_site %&gt;% kableExtra::kbl( caption = &quot;NEON site-level: cloud2trees ITD performance versus image-annotated crowns&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.6: NEON site-level: cloud2trees ITD performance versus image-annotated crowns Site recall precision ABBY 0.702 0.541 DSNY 1.000 0.250 JERC 0.857 0.316 NIWO 0.274 0.353 OSBS 0.469 0.303 SOAP 0.220 0.224 TALL 0.241 0.359 TEAK 0.365 0.316 plot_level: must be across trees included for evaluation in a NEON site, plot combination rslt_img_annttd_crwns$plot_level %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_sample(n=14) %&gt;% dplyr::arrange(plot_name) %&gt;% kableExtra::kbl( caption = &quot;NEON plot-level: cloud2trees ITD performance versus image-annotated crowns&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.7: NEON plot-level: cloud2trees ITD performance versus image-annotated crowns plot_name recall precision 2018_TEAK_3_314000_4108000_image_86 0.000 0.000 2018_TEAK_3_315000_4101000_image_600 0.600 0.474 2018_TEAK_3_315000_4107000_image_237 0.591 0.500 2018_TEAK_3_318000_4102000_image_483 0.667 0.308 2018_TEAK_3_320000_4095000_image_616 0.302 0.381 2018_TEAK_3_322000_4096000_image_368 0.286 0.222 2018_TEAK_3_323000_4098000_image_691 0.333 0.025 OSBS_022_2019 0.000 0.000 OSBS_023_2019 0.500 0.105 SOAP_031_2019 0.220 0.224 TEAK_049_2018 0.440 0.407 TEAK_053_2018 0.300 0.375 TEAK_054_2018 0.290 0.237 TEAK_062_2018 0.282 0.367 how many plots did we evaluate that met all criteria?: had point cloud data in the benchmark was from a NEON site with primarily conifer trees *had image-annotated crowns for evaluation # how many plots did we evaluate that met all criteria? rslt_img_annttd_crwns$plot_level %&gt;% dplyr::ungroup() %&gt;% dplyr::distinct(plot_name) %&gt;% nrow() ## [1] 31 how many plots were evaluated using cloud2trees::cloud2trees() by NEON site? img_annttd_crwn_plots_temp &lt;- lidar_df %&gt;% dplyr::inner_join( rslt_img_annttd_crwns$plot_level %&gt;% dplyr::ungroup() %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) # plots evaluated by neon site img_annttd_crwn_plots_temp %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(siteID) %&gt;% dplyr::arrange(desc(n)) %&gt;% kableExtra::kbl( caption = &quot;Number of 40m x 40m plots with image-annotated crowns evaluated using cloud2trees by NEON site&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.8: Number of 40m x 40m plots with image-annotated crowns evaluated using cloud2trees by NEON site siteID n TEAK 17 OSBS 6 NIWO 3 ABBY 1 DSNY 1 JERC 1 SOAP 1 TALL 1 where are these NEON sites evaluated using cloud2trees::cloud2trees()? img_annttd_crwn_plots_temp %&gt;% dplyr::count(siteID) %&gt;% mapview::mapview( cex = &quot;n&quot;, legend = F , layer.name = &quot;NEON site&quot; ) let’s review some figures comparing the image-annotated crowns versus the bounding box of our delineated crown polygons in the RGB plots: “red” boxes are crowns from our point cloud-based method extracted and “black” are the image annotated crowns rslt_img_annttd_crwns$plot_level %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_sample(n=4) %&gt;% dplyr::pull(plot_name) %&gt;% purrr::map(\\(x) NeonTreeEvaluation::image_crowns( predictions = my_submission %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(plot_name==x) , show = T , project_boxes = F )) in the RGB plots: “red” boxes are crowns from our point cloud-based method extracted and “black” are the image annotated crowns 4.4.3.2 Scores for an field-annotated crowns The second data source is a small number of field-annotated crowns from two geographic sites. These crowns were drawn on a tablet while physically standing in the field, thereby reducing the uncertainty in crown segmentation. none of the field-annotated crown sites have aerial lidar data # i did some digging around and the lidar data is here lidar_dir_temp &lt;- system.file(package = &quot;NeonTreeEvaluation&quot;, &quot;extdata&quot;, &quot;NeonTreeEvaluation&quot;, &quot;evaluation&quot;, &quot;LiDAR&quot;) # files lidar_files_temp &lt;- lidar_dir_temp %&gt;% list.files(recursive = T, pattern = &quot;.*\\\\.(laz|las)$&quot;, full.names = T) %&gt;% basename() %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;) %&gt;% unique() # what sites have field annotated crowns? fc_temp &lt;- NeonTreeEvaluation::list_field_crowns() %&gt;% stringr::str_match(pattern=&quot;(\\\\w+).tif&quot;) %&gt;% .[,2] # are any lidar files available for these sites? lidar_files_temp[lidar_files_temp %in% fc_temp] %&gt;% length() ## [1] 0 nope! 4.4.3.3 Scores for an field-collected stems The third data source is the NEON Woody Vegetation Structure Dataset. Each tree stem is represented by a single point. This data has been filtered to represent overstory trees visible in the remote sensing imagery. not all plots have field-collected stems in testing, including the sf polygon data did not work…switching to the bbox method with sf::st_drop_geometry() rslt_fld_stems &lt;- evaluate_field_stems( predictions = my_submission %&gt;% sf::st_drop_geometry() , show = F , summarize = T ) overall: must be across all NEON sites, plots, and trees included for evaluation rslt_fld_stems$overall %&gt;% kableExtra::kbl( caption = &quot;Overall: cloud2trees ITD performance versus field-collected stems&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.9: Overall: cloud2trees ITD performance versus field-collected stems recall 0.776 by_site: must be across plots, and trees included for evaluation in a NEON sites rslt_fld_stems$by_site %&gt;% kableExtra::kbl( caption = &quot;NEON site-level: cloud2trees ITD performance versus field-collected stems&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.10: NEON site-level: cloud2trees ITD performance versus field-collected stems Site recall ABBY 0.333 DEJU 0.885 JERC 0.743 MOAB 0.909 NIWO 0.501 OSBS 0.914 TALL 0.667 plot_level: must be across trees included for evaluation in a NEON site, plot combination rslt_fld_stems$plot_level %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_sample(n=14) %&gt;% dplyr::arrange(plot_name) %&gt;% kableExtra::kbl( caption = &quot;NEON plot-level: cloud2trees ITD performance versus field-collected stems&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.11: NEON plot-level: cloud2trees ITD performance versus field-collected stems siteID plot_name recall n DEJU DEJU_016 1.000 2 DEJU DEJU_061 1.000 5 JERC JERC_052 1.000 3 JERC JERC_060 0.778 9 NIWO NIWO_005 0.571 14 NIWO NIWO_007 0.407 54 NIWO NIWO_040 0.450 40 OSBS OSBS_009 0.800 5 OSBS OSBS_026 0.833 18 OSBS OSBS_028 1.000 5 OSBS OSBS_029 0.750 20 OSBS OSBS_035 1.167 12 OSBS OSBS_038 0.722 18 TALL TALL_055 0.667 21 how many plots did we evaluate that met all criteria?: had point cloud data in the benchmark was from a NEON site with primarily conifer trees *had field-collected stems for evaluation # how many plots did we evaluate that met all criteria? rslt_fld_stems$plot_level %&gt;% dplyr::ungroup() %&gt;% dplyr::distinct(plot_name) %&gt;% nrow() ## [1] 43 how many plots were evaluated using cloud2trees::cloud2trees() by NEON site? fld_stems_plots_temp &lt;- lidar_df %&gt;% dplyr::mutate( sht_plt = plot_name %&gt;% stringr::str_extract( rslt_fld_stems$plot_level %&gt;% dplyr::ungroup() %&gt;% dplyr::distinct(plot_name) %&gt;% dplyr::pull(plot_name) %&gt;% paste(collapse = &quot;|&quot;) ) ) %&gt;% dplyr::filter(!is.na(sht_plt)) %&gt;% dplyr::group_by(siteID,sht_plt) %&gt;% dplyr::summarise(n=dplyr::n()) %&gt;% dplyr::ungroup() %&gt;% dplyr::rename(plot_name=sht_plt) # plots evaluated by neon site fld_stems_plots_temp %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(siteID) %&gt;% dplyr::arrange(desc(n)) %&gt;% kableExtra::kbl( caption = &quot;Number of 40m x 40m plots with field-collected stems evaluated using cloud2trees by NEON site&quot; , digits = 3 ) %&gt;% kableExtra::kable_styling() Table 4.12: Number of 40m x 40m plots with field-collected stems evaluated using cloud2trees by NEON site siteID n OSBS 16 DEJU 9 NIWO 9 JERC 5 TALL 2 ABBY 1 MOAB 1 where are these NEON sites evaluated using cloud2trees::cloud2trees()? fld_stems_plots_temp %&gt;% dplyr::count(siteID) %&gt;% mapview::mapview( cex = &quot;n&quot;, legend = F , layer.name = &quot;NEON site&quot; ) "],["cbh.html", "Section 5 CBH Process 5.1 Example lidar data 5.2 Extract CBH", " Section 5 CBH Process let’s go through the process to extract CBH from the point cloud 5.1 Example lidar data we’ll use some of our trees and height-normalized point cloud data generated in our point cloud processing load the tree crown polygons # get the data from already run crowns_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_crowns.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() load the central 0.1 ha area of our study area aoi &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_union() %&gt;% sf::st_centroid() %&gt;% sf::st_buffer(sqrt(1000/4), endCapStyle = &quot;SQUARE&quot;) %&gt;% sf::st_transform(sf::st_crs(crowns_sf)) ## Reading layer `raw_las_ctg_info&#39; from data source ## `C:\\Data\\usfs\\lidar_phys_fire_mods\\data\\point_cloud_processing_delivery\\raw_las_ctg_info.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 42 features and 34 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 467000 ymin: 3808000 xmax: 473000 ymax: 3815000 ## Projected CRS: NAD83(2011) / UTM zone 12N filter for trees in the aoi crowns_sf &lt;- crowns_sf %&gt;% # join so we get the full crown dplyr::inner_join( sf::st_intersection(crowns_sf, aoi) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::distinct(treeID) , by = &quot;treeID&quot; ) %&gt;% # make treeID numeric for later dplyr::mutate( treeID_bu = treeID , treeID = treeID %&gt;% as.factor %&gt;% as.numeric() ) %&gt;% cloud2trees::simplify_multipolygon_crowns() now, we’ll load in the point cloud data for our aoi las_ctg &lt;- lidR::readLAScatalog(&quot;../data/point_cloud_processing_delivery/norm_las/&quot;) lidR::opt_progress(las_ctg) &lt;- F las &lt;- lidR::clip_roi(las_ctg, crowns_sf %&gt;% sf::st_bbox() %&gt;% sf::st_as_sfc() %&gt;% sf::st_buffer(2)) plot a sample las %&gt;% lidR::plot( color = &quot;Z&quot;, bg = &quot;white&quot;, legend = T , pal = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) ) now we’ll attach the treeID column to the normalized las file las &lt;- cloud2trees::polygon_attribute_to_las(las = las, poly_df = crowns_sf, attribute = &quot;treeID&quot;, force_crs = T) let’s look at our point cloud colored by treeID las %&gt;% lidR::filter_poi(!is.na(treeID)) %&gt;% lidR::plot(color = &quot;treeID&quot;, bg = &quot;white&quot;, legend = F) let’s filter for a single tree # sf one_tree_sf &lt;- crowns_sf %&gt;% dplyr::filter(tree_height_m == max(tree_height_m)) %&gt;% dplyr::slice(1) # las one_tree_las &lt;- las %&gt;% lidR::filter_poi(treeID==one_tree_sf$treeID) check our one tree point cloud one_tree_las %&gt;% lidR::plot( color = &quot;Z&quot;, bg = &quot;white&quot;, legend = T , pal = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) ) 5.2 Extract CBH using the same settings as our processing # CALL IT ladderfuelsr_cbh_ans &lt;- cloud2trees::ladderfuelsr_cbh( las = one_tree_las , min_vhp_n = 3 , voxel_grain_size_m = 1 , dist_btwn_bins_m = 1 , min_fuel_layer_ht_m = 1 , lad_pct_gap = 25 , lad_pct_base = 25 , num_jump_steps = 1 , min_lad_pct = 10 , frst_layer_min_ht_m = 1 ) ## [1] &quot;treeID: 21&quot; ladderfuelsr_cbh_ans %&gt;% names() ## [1] &quot;gaps_fbhs&quot; &quot;lad_profile&quot; &quot;gaps_perc&quot; &quot;metrics_distance&quot; ## [5] &quot;metrics_depth&quot; &quot;real_fbh&quot; &quot;real_depth&quot; &quot;eff_gap&quot; ## [9] &quot;layers_lad_df&quot; &quot;cbh_metrics&quot; ladderfuelsr_cbh_ans$lad_profile %&gt;% dplyr::glimpse() ## Rows: 30 ## Columns: 3 ## $ treeID &lt;fct&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,… ## $ height &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, … ## $ lad &lt;dbl&gt; 0.000000000, 0.002752691, 0.000000000, 0.000000000, 0.000000000… Create our own plot of the gaps and fuel layers base height in the vertical tree profile ggplot() + geom_path(data = ladderfuelsr_cbh_ans$lad_profile, mapping = aes(x = lad, y = height)) + geom_point(data = ladderfuelsr_cbh_ans$lad_profile, mapping = aes(x = lad, y = height)) + # gaps data geom_hline( data = ladderfuelsr_cbh_ans$gaps_fbhs %&gt;% dplyr::select( tidyselect::starts_with(&quot;gap&quot;) &amp; !tidyselect::starts_with(&quot;gap_&quot;) ) %&gt;% tidyr::pivot_longer(dplyr::everything()) , aes(yintercept = value, color = &quot;gaps&quot;) , linetype = &quot;dotted&quot; , lwd = 1.2 ) + # fbh data geom_hline( data = ladderfuelsr_cbh_ans$gaps_fbhs %&gt;% dplyr::select( tidyselect::starts_with(&quot;cbh&quot;) &amp; !tidyselect::starts_with(&quot;cbh_&quot;) ) %&gt;% tidyr::pivot_longer(dplyr::everything()) , aes(yintercept = value, color = &quot;FBHs&quot;) , linetype = &quot;dotdash&quot; , lwd = 1.2 ) + scale_color_manual(values = c(&quot;green4&quot;, &quot;red&quot;), name = &quot;&quot;) + scale_y_continuous(breaks = scales::extended_breaks(10)) + theme_light() + theme(legend.position = &quot;top&quot;) note, the gap and cbh columns in ladderfuelsr_cbh_ans$gaps_fbhs contain the data needed for the plot ( but not the gap_ and cbh_ columns ;/ ). but what are these columns? cbh - Height of the fuel layer base height (m) gap - Height of gap between fuel layers (m) 5.2.1 Return CBH Metrics ladderfuelsr_cbh_ans$cbh_metrics %&gt;% dplyr::glimpse() ## Rows: 1 ## Columns: 29 ## $ treeID &lt;fct&gt; 21 ## $ treeID1 &lt;dbl&gt; 21 ## $ dptf1 &lt;dbl&gt; 19 ## $ effdist1 &lt;dbl&gt; 10 ## $ Hcbh1 &lt;dbl&gt; 11.5 ## $ Hdist1 &lt;dbl&gt; 10.5 ## $ Hdptf1 &lt;dbl&gt; 30.5 ## $ max1 &lt;dbl&gt; 30.5 ## $ Hcbh1_Hdptf1 &lt;dbl&gt; 98.31702 ## $ max_height &lt;dbl&gt; 30.5 ## $ nlayers &lt;dbl&gt; 1 ## $ maxlad_Hcbh &lt;dbl&gt; 11.5 ## $ maxlad_Hdist &lt;dbl&gt; 10.5 ## $ maxlad_Hdptf &lt;dbl&gt; 30.5 ## $ maxlad_dptf &lt;dbl&gt; 19 ## $ maxlad_effdist &lt;dbl&gt; 10 ## $ maxlad_lad &lt;dbl&gt; 98.31702 ## $ max_Hcbh &lt;dbl&gt; 11.5 ## $ max_Hdist &lt;dbl&gt; 10.5 ## $ max_Hdptf &lt;dbl&gt; 30.5 ## $ max_dptf &lt;dbl&gt; 19 ## $ max_effdist &lt;dbl&gt; 10 ## $ max_lad &lt;dbl&gt; 98.31702 ## $ last_Hcbh &lt;dbl&gt; 11.5 ## $ last_Hdist &lt;dbl&gt; 10.5 ## $ last_Hdptf &lt;dbl&gt; 30.5 ## $ last_dptf &lt;dbl&gt; 19 ## $ last_effdist &lt;dbl&gt; 10 ## $ last_lad &lt;dbl&gt; 98.31702 what are these? treeID: tree ID with strings and numeric values treeID1: tree ID with only numeric values dptf: Depth of fuel layers (m) after considering distances greater than the actual height bin step effdist: Effective distance between consecutive fuel layers (m) after considering distances greater than any number of steps Hcbh: Base height of each fuel separated by a distance greater than the certain number of steps Hdptf: Height of the depth of fuel layers (m) after considering distances greater than the actual step Hdist: Height of the distance (&gt; any number of steps) between consecutive fuel layers (m) Hcbh_Hdptf - Percentage of LAD values comprised in each effective fuel layer maxlad_Hcbh - Height of the CBH of the segmented tree based on the maximum LAD percentage maxlad1_Hcbh - Height of the CBH from the second fuel layer when the maximum LAD occurred in the first fuel layer but its depth &lt;= “hdepth1_height” max_Hcbh - Height of the CBH of the segmented tree based on the maximum distance found in its profile last_Hcbh - Height of the CBH of the segmented tree based on the last distance found in its profile maxlad_ - Values of distance and fuel depth and their corresponding heights at the maximum LAD percentage maxlad1_ - Values of distance and fuel depth and their corresponding heights for the second fuel layer when the maximum LAD occurred in the first fuel layer but its depth &lt;= “hdepth1_height” max_ - Values of distance and fuel depth and their corresponding heights at the maximum distance last_ - Values of distance and fuel depth and their corresponding heights at the last distance nlayers - Number of effective fuel layers max_height - Maximum height of the tree profile there are also some plotting functions # Generate plots for fuels LAD metrics plots_cbh_maxlad &lt;- LadderFuelsR::get_plots_cbh_LAD( LAD_profiles = ladderfuelsr_cbh_ans$lad_profile , cbh_metrics = ladderfuelsr_cbh_ans$cbh_metrics , min_height = 0.5 ) plots_cbh_maxdist &lt;- LadderFuelsR::get_plots_cbh_maxdist( LAD_profiles = ladderfuelsr_cbh_ans$lad_profile , cbh_metrics = ladderfuelsr_cbh_ans$cbh_metrics , min_height = 0.5 ) plots_cbh_lastdist &lt;- LadderFuelsR::get_plots_cbh_lastdist( LAD_profiles = ladderfuelsr_cbh_ans$lad_profile , cbh_metrics = ladderfuelsr_cbh_ans$cbh_metrics , min_height = 0.5 ) # patchwork them (plots_cbh_maxlad[[1]] + labs(title = &quot;get_plots_cbh_LAD&quot;)) + (plots_cbh_maxdist[[1]] + labs(title = &quot;get_plots_cbh_maxdist&quot;)) + (plots_cbh_lastdist[[1]] + labs(title = &quot;get_plots_cbh_lastdist&quot;)) + patchwork::plot_layout(ncol = 2) these plots represent the three criteria to define the CBH in a segmented tree: get_plots_cbh_LAD = the fuel layer containing the maximum LAD percentage (column named maxlad_Hcbh) get_plots_cbh_maxdist = the fuel layer located at the highest distance (column named max_Hcbh) get_plots_cbh_lastdist = the fuel layer separated by the last effective distance (column named last_Hcbh) 5.2.2 CBH on the point cloud can we make a view of the CBH on the point cloud? # make a matrix to represent the cbh x_temp &lt;- seq( min(one_tree_las@data$X) , max(one_tree_las@data$X) , length.out = 2 ) y_temp &lt;- seq( min(one_tree_las@data$Y) , max(one_tree_las@data$Y) , length.out = 2 ) xy_temp &lt;- expand.grid(x = x_temp, y = y_temp) z_temp &lt;- matrix( rep( ladderfuelsr_cbh_ans$cbh_metrics$last_Hcbh , nrow(xy_temp) ) , nrow = length(x_temp), ncol = length(y_temp) ) # plot it plot3D::scatter3D( x = one_tree_las@data$X , y = one_tree_las@data$Y , z = one_tree_las@data$Z , colvar = one_tree_las@data$Z , cex = 0.3, pch = 19 , colkey = T , phi = -6 , col = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) , main =&quot;CBH shown in black&quot; , surf = list( x = x_temp , y = y_temp , z = z_temp , facets = NA , border = &quot;black&quot; , lwd = 2 ) ) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
