[["index.html", "Aerial LiDAR for Fire Model Inputs Section 1 Introduction 1.1 Objective 1.2 Data", " Aerial LiDAR for Fire Model Inputs George Woolsey 25 February, 2025 Section 1 Introduction Code in support of “Using aerial LiDAR data for object-based physical fire modeling in conifer forests of the southwestern US” 1.1 Objective The objective of this study is to demonstrate the use of aerial LiDAR data to create inputs for physics-based fire models in frequent-fire forests of the southwestern United States. We review the methods used to extract tree location, species, and physical form from aerial LiDAR data. We evaluate this canopy crown detection methodology using a benchmark data set created to standardize evaluation metrics (Weinstein et al. 2021). We explain how to format this data for seamless integration with two commonly used object-based physical fire modeling tools. We demonstrate the end-to-end process using a case study from the southwestern United States. 1.2 Data Lidar data from the Mogollon Rim area of the Coconino National Forest about 20 km north of Payson, Arizona, USA was acquired from the USGS LidarExplorer. The aerial lidar data was collected between August 2013 and October 2014 under lidar project “AZ_USFS_3DEP_Processing_2019_D20” (project ID: 195122). "],["point-cloud-processing.html", "Section 2 Point Cloud Processing 2.1 Lidar Data Location 2.2 Individial Tree Detection Tuning: itd_tuning() 2.3 Point Cloud Tree Extraction: cloud2trees() 2.4 DBH Modeling: trees_dbh() 2.5 CBH Modeling: trees_cbh()", " Section 2 Point Cloud Processing In this section we’ll process the raw point cloud data using the cloud2trees R package developed to provide accessible routines for processing point cloud data collected by airborne lidar or generated using UAS imagery and photogrammetry (e.g. structure from motion). The cloud2trees package can be installed by following the directions listed in the README file on GitHub. If one is still experiencing difficulties installing the package, see the example.R file which details how to install the package using a virgin R instance. ## remotes helps us get packages hosted on github install.packages(&quot;remotes&quot;) ## get cloud2trees remotes::install_github(repo = &quot;georgewoolsey/cloud2trees&quot;, upgrade = F) Load the standard libraries we use to do work # bread-and-butter library(tidyverse) # the tidyverse library(viridis) # viridis colors library(harrypotter) # hp colors library(RColorBrewer) # brewer colors library(scales) # work with number and plot scales library(latex2exp) # visualization library(mapview) # interactive html maps library(kableExtra) # tables library(patchwork) # combine plots # spatial analysis library(terra) # raster library(sf) # simple features library(lidR) # lidar data library(cloud2trees) # the cloud2trees 2.1 Lidar Data Location Let’s check out the lidar data we got from the Mogollon Rim area of the Coconino National Forest about 20 km north of Payson, Arizona, USA using the USGS LidarExplorer. # directory with the downloaded .las|.laz files f &lt;- &quot;e:/lidar_phys_fire_mods/data/mogollon_rim_az_lidar/&quot; # is there data? list.files(f, pattern = &quot;.*\\\\.(laz|las)$&quot;) %&gt;% length() ## [1] 42 # what files are in here? list.files(f, pattern = &quot;.*\\\\.(laz|las)$&quot;)[1:3] ## [1] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3808.laz&quot; ## [2] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3809.laz&quot; ## [3] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3810.laz&quot; We’ll plot our point cloud data tiles real quick to orient ourselves lidR::readLAScatalog(f) %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% mapview::mapview(popup = F, layer.name = &quot;point cloud tile&quot;) 2.2 Individial Tree Detection Tuning: itd_tuning() The cloud2trees package performs individual tree detection using lidR::locate_trees() with the lidR::lmf() algorithm. The local maximum filter algorithm allows for a constant window size or a variable window size defined by a function. See the lidR package book section by point cloud processing expert Jean-Romain Roussel for excellent detail on ITD and defining window size. The itd_tuning() function is used to visually assess tree crown delineation results from different window size functions used for the detection of individual trees. itd_tuning() allows users to test different window size functions on a sample of data to determine which function is most suitable for the area being analyzed. The preferred function can then be used in the ws parameter in raster2trees() and cloud2trees(). Let’s run itd_tuning() on our data starting with default window size functions # run itd_tuning() itd_tuning_ans &lt;- cloud2trees::itd_tuning(f) # what did we get? itd_tuning_ans %&gt;% names() ## [1] &quot;plot_samples&quot; &quot;ws_fn_list&quot; check the ws_fn_list return which includes the different window size functions tested # what ws_fn_list itd_tuning_ans$ws_fn_list %&gt;% str() ## List of 3 ## $ lin_fn:function (x) ## $ exp_fn:function (x) ## $ log_fn:function (x) let’s look at the function definition for the linear function (lin_fn) # the linear function itd_tuning_ans$ws_fn_list$lin_fn ## function (x) ## { ## y &lt;- dplyr::case_when(is.na(x) ~ 0.001, x &lt; 0 ~ 0.001, x &lt; ## 2 ~ 1, x &gt; 30 ~ 5, TRUE ~ 0.75 + (x * 0.14)) ## return(y) ## } ## &lt;bytecode: 0x000001fde3a9a5a8&gt; ## &lt;environment: 0x000001fde3aacd38&gt; let’s plot all of the functions we tested with our call to itd_tuning() using the defaults # shape of the ws functions ggplot() + geom_function(aes(color = &quot;lin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$lin_fn, lwd=1.2) + geom_function(aes(color = &quot;nonlin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$nonlin_fn, lwd=1.2) + geom_function(aes(color = &quot;exp_fn&quot;),fun=itd_tuning_ans$ws_fn_list$exp_fn, lwd=1.2) + xlim(-5,60) + harrypotter::scale_color_hp_d(option = &quot;hermionegranger&quot;) + labs(x = &quot;heights&quot;, y = &quot;ws&quot;, color = &quot;ws function&quot;) + theme_light() now, let’s see how those window size functions impacted individual tree detection by checking the plot_samples return # tuning plot itd_tuning_ans$plot_samples Looking at the first sample, the exponential function (exp_fn) resulted in too few trees detected in the overstory class. The clearest evidence of this is in the center of the left-hand side of the plot in the first sample. There is a clear “valley” in the CHM which the linear (lin_fn) and non-linear (nonlin_fn) correctly split into two trees but the exponential function misses this split. Furthermore, the exponential function results in too many tree splits for short trees as can be seen in the second sample plot in the lower-left corner small tree group. The linear and the non-linear function are very similar in detecting overstory trees but the linear function perhaps does a better job splitting up clumps of smaller trees. In the third sample plot the linear function does a better job splitting up the short tree group in the upper-right corner small tree group compared to the non-linear function (there is no way that a tree that short [3-6 m tall] would have such a large crown area as in the non-linear split). If we had one gripe about the linear function, it’s maybe that it results in too many trees in small-tree patches. Let’s define our own custom linear function that slightly increases the window size for shorter trees compared to the default linear function. # custom linear function custom_lin &lt;- function (x){ y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &lt; 2 ~ 1.2 , x &gt; 30 ~ 5 , TRUE ~ 0.9 + (x * 0.139) ) return(y) } # shape of the ws functions ggplot() + geom_function(aes(color = &quot;lin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$lin_fn, lwd=1.2) + geom_function(aes(color = &quot;nonlin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$nonlin_fn, lwd=1.2) + geom_function(aes(color = &quot;custom_lin&quot;),fun=custom_lin, lwd=1.2) + xlim(-5,60) + harrypotter::scale_color_hp_d(option = &quot;hermionegranger&quot;) + labs(x = &quot;heights&quot;, y = &quot;ws&quot;, color = &quot;ws function&quot;) + theme_light() We’ll run another sample test using itd_tuning()with our new function (call it “my_custom_lin” for extra clarity) compared to the default linear and non-linear functions and this time we’ll ask for four sample plots of 0.1 ha. itd_tuning_ans2 &lt;- cloud2trees::itd_tuning( f , ws_fn_list = list( my_custom_lin = custom_lin , lin_fn = itd_tuning_ans$ws_fn_list$lin_fn , nonlin_fn = itd_tuning_ans$ws_fn_list$nonlin_fn ) , n_samples = 4 ) now, let’s see how those window size functions impacted individual tree detection by checking the plot_samples return # tuning plot itd_tuning_ans2$plot_samples Our custom linear function (my_custom_lin) strikes a good balance between detection of lower canopy trees (e.g. &lt;10 m in height) without improperly subdividing dominant canopy trees based on the areas sampled. Let’s move forward with our custom linear function in the raster2trees() and cloud2trees() functions. 2.3 Point Cloud Tree Extraction: cloud2trees() The cloud2trees() function combines methods in the cloud2trees package for an all-in-one approach. We’ll call this function without estimating any of the additional tree components (the estimate_* parameters) which we will do separately to show the full process. With all other options turned off, cloud2trees() will: 1) generate a CHM from the point cloud using cloud2raster(); and 2) perform individual tree detection using raster2trees(). cloud2trees_ans &lt;- cloud2trees::cloud2trees( output_dir = &quot;../data&quot; , input_las_dir = f # we defined this above , accuracy_level = 2 , dtm_res_m = 1 , chm_res_m = 0.25 , min_height = 2 , ws = custom_lin # here it is , keep_intrmdt = T # these are turned off by default but we&#39;ll be explicit , estimate_tree_dbh = F , estimate_tree_competition = F , estimate_tree_type = F , estimate_tree_hmd = F , estimate_tree_cbh = F ) we should have a spatial tree list with tree height attached cloud2trees_ans$crowns_sf %&gt;% dplyr::select(treeID, tree_x, tree_y, tree_height_m, crown_area_m2) %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 6 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, &quot;1872834&quot;, &quot;… ## $ tree_x &lt;dbl&gt; 467000.1, 467000.1, 467000.1, 467000.1, 467000.1, 467000… ## $ tree_y &lt;dbl&gt; 3808063, 3808065, 3808067, 3808074, 3808091, 3808124, 38… ## $ tree_height_m &lt;dbl&gt; 2.15, 2.34, 2.02, 2.11, 3.66, 4.37, 2.03, 5.60, 17.72, 2… ## $ crown_area_m2 &lt;dbl&gt; 0.3750, 0.7500, 0.2500, 0.3750, 1.2500, 0.9375, 0.3750, … ## $ geom &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((467000 3808..., MULTIPOLYGO… That’s a lot of trees! The cloud2trees() function dropped off a lot of additional data in a folder titled “point_cloud_processing_delivery” which is nested where we told the command to write the data (output_dir = \"../data\" parameter setting). Let’s load in the “processed_tracking_data.csv” file to see how long that cloud2trees() process took to run. Run times are, of course, dependent on computer processing and I am working on a laptop typical of a spatial analyst (especially outside of the US Federal Government) running Windows with an Intel i7-10750H 6-core computer processor unit and 32 gigabytes of random-access memory. # load processed_tracking_data.csv processing_data &lt;- readr::read_csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , progress = F , show_col_types = F ) # what? processing_data %&gt;% dplyr::select(1:4) %&gt;% dplyr::glimpse() ## Rows: 1 ## Columns: 4 ## $ number_of_points &lt;dbl&gt; 571949084 ## $ las_area_m2 &lt;dbl&gt; 41999160 ## $ timer_cloud2raster_mins &lt;dbl&gt; 135.4638 ## $ timer_raster2trees_mins &lt;dbl&gt; 93.28212 let’s do some math # total tree extraction time trees_mins_temp &lt;- processing_data$timer_cloud2raster_mins[1] + processing_data$timer_raster2trees_mins[1] # ha ha_temp &lt;- round(processing_data$las_area_m2[1]/10000) # secs per ha rate_temp &lt;- (trees_mins_temp*60) / ha_temp # point density dens_temp &lt;- processing_data$number_of_points[1] / processing_data$las_area_m2[1] Tree extraction over 4,200 hectares took a total of 228.7 minutes at processing rate of 3.27 seconds per hectare on lidar data with a point density of 13.6 points per square meter. 2.4 DBH Modeling: trees_dbh() The trees_dbh() function uses the TreeMap FIA plot data in the area of the tree list to estimate the height-DBH allometry relationship. The height predicting DBH model built from the FIA data is then used to predict DBH based on tree height in the tree list. trees_dbh_ans &lt;- cloud2trees::trees_dbh() 2.5 CBH Modeling: trees_cbh() The trees_cbh() function does work # where should we save the file? cbh_fn &lt;- &quot;../data/point_cloud_processing_delivery/cbh_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(cbh_fn)){ # sample size nsamp_temp &lt;- 100000 # time it st_temp &lt;- Sys.time() # run it trees_cbh_ans &lt;- cloud2trees::trees_cbh( trees_poly = cloud2trees_ans$crowns_sf , norm_las = &quot;../data/point_cloud_processing_delivery/norm_las/&quot; , tree_sample_n = nsamp_temp # , tree_sample_prop = 0.08 , which_cbh = &quot;lowest&quot; , estimate_missing_cbh = TRUE , min_vhp_n = 3 , voxel_grain_size_m = 1 , dist_btwn_bins_m = 1 , min_fuel_layer_ht_m = 1 , lad_pct_gap = 25 , lad_pct_base = 25 , num_jump_steps = 1 , min_lad_pct = 10 , frst_layer_min_ht_m = 1 , force_same_crs = T ) # timer ## author note: test 1 with 20k sample took 58.56778 mins mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_cbh_mins &lt;- mins_temp processing_data$sttng_cbh_tree_sample_n &lt;- nsamp_temp processing_data$sttng_cbh_tree_sample_prop &lt;- NA # save cbh trees_cbh_ans %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = cbh_fn, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ # cbh data trees_cbh_ans &lt;- readr::read_csv(cbh_fn, progress = F, show_col_types = F) # re-cast treeID if needed if( !inherits( trees_cbh_ans$treeID , class(cloud2trees_ans$crowns_sf$treeID) ) ){ if(inherits(cloud2trees_ans$crowns_sf$treeID, &quot;character&quot;)){ trees_cbh_ans$treeID &lt;- as.character(trees_cbh_ans$treeID) }else if(inherits(cloud2trees_ans$crowns_sf$treeID, &quot;numeric&quot;)){ trees_cbh_ans$treeID &lt;- as.numeric(trees_cbh_ans$treeID) } } } CBH extraction for 50,000 trees took a total of 159.4 minutes at processing rate of 3.19 minutes per 1,000 trees…this is not fast and mostly a limitation of the LadderFuelsR which only allows for processing one tree at a time. Perhaps the author of cloud2trees will enable parallel processing at some point in the future??? Let’s see what we got back from trees_cbh() trees_cbh_ans %&gt;% dplyr::select(treeID, tree_cbh_m, is_training_cbh) %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 3 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, &quot;1872834&quot;,… ## $ tree_cbh_m &lt;dbl&gt; 2.001330, 2.178191, 1.880319, 1.964096, 3.193244, 3.20… ## $ is_training_cbh &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… We requested CBH extraction for 50,000 trees. For how many trees was CBH successfully extracted from the point cloud? trees_cbh_ans %&gt;% dplyr::count(is_training_cbh) ## # A tibble: 2 × 2 ## is_training_cbh n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 2615015 ## 2 TRUE 14480 That’s not a great success ;( … Someone should do something (or should they?) "],["validate-tree-detection-and-crown-delineation.html", "Section 3 Validate Tree Detection and Crown Delineation 3.1 NeonTreeEvaluation overview 3.2 lidar data in NeonTreeEvaluation 3.3 Example validation process", " Section 3 Validate Tree Detection and Crown Delineation In this section we’ll use the benchmark data made available in the NeonTreeEvaluation data set (Weinstein et al. 2021) to evaluate our process for lidar-based tree detection. We’ll implement our tree detection process via the [cloud2trees]https://github.com/georgewoolsey/cloud2trees) package First, load the standard libraries # bread-and-butter library(tidyverse) # the tidyverse library(viridis) # viridis colors library(harrypotter) # hp colors library(RColorBrewer) # brewer colors library(scales) # work with number and plot scales library(latex2exp) # visualization library(mapview) # interactive html maps library(kableExtra) # tables library(patchwork) # combine plots library(ggnewscale) # ggnewscale # spatial analysis library(sf) # simple features library(lidR) # lidar data library(cloud2trees) # tha cloud2trees library(NeonTreeEvaluation) # benchmark data 3.1 NeonTreeEvaluation overview Weinstein et al. (2021) developed: a benchmark dataset of individual canopy crowns derived from multi-sensor imagery in the National Ecological Observatory Network (Table 1) that provides: 1) co-registered remote sensing data from multiple sensors (LiDAR, RGB imagery, and hyperspectral imagery) to allow comparisons of methods based on any single sensor (e.g., for LiDAR based methods), or any combination of sensors (e.g., combining RGB and hyperspectral), and 2) three types of evaluation data to allow assessing both ‘tree detection’, defined as the identifying the location of individual trees using evaluation data with a point at the crown center , and ‘crown delineation’ defined as identifying the boundary edge of crowns across a broad range of forest types. The benchmark is designed to allow flexibility in both workflow and sensor selection. (p. 2) Table 1. Summary of datasets included in the benchmark dataset. All sensor data has been cropped to the extent of NEON field sampling plots. Note the three data labeled as “Evaluation data” in the table. If you are asking “why three evaluation datasets?”, the authors provide some detail: The inclusion of multiple evaluation types is critical because each type of evaluation data has strengths and limitations in evaluating model performance. Field collected stems are the most common evaluation data used in crown detection work due to high confidence that each stem represents a location of a single tree. However, the position of a tree stem can fail to accurately represent the position of the crown as viewed from above due to a combination of spatial errors in alignment with the image data and the tendency for trees to grow at acute angles (tree lean is not measured in the NEON data), such that the center of the crown and position of the stem can be offset by several meters….Image-annotated crowns are relatively easy to scale, allowing the collection of data for a wide range of forest types and for annotation of every visible crown in the image. Using image-annotated crowns supports the evaluation of methods across a broad range of forest types and allows both recall and precision to be calculated. However, since these annotations are not generated by an observer in the field there can be errors due to interpreting the images. This problem is solved using field-annotated crowns in which an observer annotates the remote-sensing imagery on a tablet while in the field [33]. The main limitation to this approach is that it is labor intensive, meaning that only a relatively small amount of validation data can be collected, making it difficult to obtain a large number of crowns across broad scales or assess model precision. Given the tradeoffs in each evaluation type, providing multiple criteria is a useful way of balancing the need for broad scale model verification with rigorous evaluation of field-based measurements. (p. 14-15) To evaluate the performance of our aerial point cloud-based algorithm for 1) tree detection and 2) crown delineation using NeonTreeEvaluation we need to ensure our tree polygon data is formatted properly: This package takes a standard submission format of predicted crowns in either bounding box or polygons as input and returns the evaluation scores of the detections for each of the three evaluation datasets. This reproducible workflow will facilitate creating a transparent process for future comparisons among crown detection algorithms. (p. 14) The authors describe the “standard submission format” on the package GitHub: Each row contains information for one predicted bounding box. The plot_name should be named the same as the files in the dataset without extension (e.g. SJER_021_2018 not SJER_021_2018.tif) and not the full path to the file on disk. Not all evaluation data are available for all plots. Functions like evaluate_field_crowns and evaluate_image_crowns will look for matching plot name and ignore other plots. Depending on the speed of the algorithm, the simplest thing to do is predict all images in the RGB folder (see list_rgb()) and the package will handle matching images with the correct data to the correct evaluation procedure…Instead of bounding boxes, some methods may return polygons. To submit as polygons, create a single unprojected shapefile with polygons in image coordinates. Polygons must be complete with no holes. Here is an example of the above csv file in polygon format. Here the xmin, xmax, etc. columns are ignored since the information is stored in the geometry data. Simple feature collection with 6 features and 7 fields geometry type: POLYGON dimension: XY bbox: xmin: 30.39723 ymin: 122.1164 xmax: 397.5746 ymax: 400 CRS: NA xmin ymin xmax ymax score label plot_name 1 41.01716 230.8854 151.08607 342.6985 0.8098674 Tree DSNY_014_2019 2 357.32129 122.1164 397.57458 159.3758 0.6968824 Tree DSNY_014_2019 3 30.39723 136.9157 73.79434 184.9473 0.5713338 Tree DSNY_014_2019 4 260.65921 285.6689 299.68811 326.7933 0.5511004 Tree DSNY_014_2019 5 179.34564 371.6130 232.49385 400.0000 0.4697072 Tree DSNY_014_2019 6 316.27377 378.9802 363.67542 400.0000 0.3259409 Tree DSNY_014_2019 st_sfc.lst. 1 POLYGON ((41.01716 230.8854... 2 POLYGON ((357.3213 122.1164... 3 POLYGON ((30.39723 136.9157... 4 POLYGON ((260.6592 285.6689... 5 POLYGON ((179.3456 371.613,... 6 POLYGON ((316.2738 378.9802... So we are going to: run cloud2trees::cloud2trees() on all lidar data, combine into a single tree list with a row unique by a detected tree and the plot_name column (e.g. “SJER_021_2018”), as an unprojected sf data with polygons in image coordinates. We may need to run cloud2trees::simplify_multipolygon_crowns() prior to submission. 3.2 lidar data in NeonTreeEvaluation we first have to download evaluation data from the Zenodo archive (1GB), use the download() function to place the data in the correct package location. Download the much larger training data, set training=TRUE. NeonTreeEvaluation::download(training = T, force = F) let’s find what data is available # i did some digging around and the lidar data is here lidar_dir_temp &lt;- system.file(package = &quot;NeonTreeEvaluation&quot;, &quot;extdata&quot;, &quot;NeonTreeEvaluation&quot;, &quot;evaluation&quot;, &quot;LiDAR&quot;) # files lidar_files_temp &lt;- lidar_dir_temp %&gt;% list.files(recursive = T, pattern = &quot;.*\\\\.(laz|las)$&quot;, full.names = T) %&gt;% unique() # look at this lidar_files_temp %&gt;% basename() %&gt;% sample(size = 9) ## [1] &quot;DEJU_020_2018.laz&quot; &quot;unnamed_plot_11.las&quot; &quot;BART_079_2019.laz&quot; ## [4] &quot;OSBS_121.las&quot; &quot;SOAP_057_2019.laz&quot; &quot;unnamed_plot_84.las&quot; ## [7] &quot;BART_010_2019.laz&quot; &quot;LENO_011_2019.laz&quot; &quot;LENO_030_2019.laz&quot; # let&#39;s pull out all sites with `.laz` data and create a data frame for tracking purposes lidar_df &lt;- lidar_files_temp %&gt;% dplyr::as_tibble() %&gt;% dplyr::rename(f_path = 1) %&gt;% # create some other variables dplyr::mutate( plot_name = f_path %&gt;% basename() %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;) ) # what? lidar_df %&gt;% dplyr::glimpse() ## Rows: 2,186 ## Columns: 2 ## $ f_path &lt;chr&gt; &quot;C:/Program Files/R/R-4.3.0/library/NeonTreeEvaluation/extda… ## $ plot_name &lt;chr&gt; &quot;2018_SJER_3_252000_4104000_image_628&quot;, &quot;2018_SJER_3_252000_… that’s a lot of files…let’s only process the sites with evaluation data # there are functions to get a list of all evaluation data # let&#39;s use these to filter our lidar files plotnames_temp &lt;- c( NeonTreeEvaluation::list_annotations() , NeonTreeEvaluation::list_field_stems() # this one includes file paths, so we have to clean , NeonTreeEvaluation::list_field_crowns() %&gt;% stringr::str_match(pattern=&quot;(\\\\w+).tif&quot;) %&gt;% .[,2] # there are plot_names from the submission data too , NeonTreeEvaluation::submission_polygons$plot_name %&gt;% unique() , NeonTreeEvaluation::submission$plot_name %&gt;% unique() ) %&gt;% unique() # huh? plotnames_temp %&gt;% sample(11) ## [1] &quot;ABBY_075_2019&quot; &quot;OSBS_86_competition&quot; &quot;OSBS_56_competition&quot; ## [4] &quot;UKFS_016_2018&quot; &quot;NIWO_011_2019&quot; &quot;UNDE_061_2017&quot; ## [7] &quot;SJER_008_2019&quot; &quot;GRSM_006_2018&quot; &quot;SCBI_061&quot; ## [10] &quot;JERC_047_2019&quot; &quot;TALL_044&quot; filter our lidar data list lidar_df &lt;- lidar_df %&gt;% #filter based on plots in evaluation data dplyr::filter(plot_name %in% plotnames_temp) %&gt;% # pull out site dplyr::mutate( siteID = stringr::str_extract(plot_name, &quot;[A-Z]+&quot;) ) # what? lidar_df %&gt;% dplyr::glimpse() ## Rows: 1,732 ## Columns: 3 ## $ f_path &lt;chr&gt; &quot;C:/Program Files/R/R-4.3.0/library/NeonTreeEvaluation/extda… ## $ plot_name &lt;chr&gt; &quot;2018_SJER_3_252000_4104000_image_628&quot;, &quot;2018_SJER_3_252000_… ## $ siteID &lt;chr&gt; &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJER&quot;, &quot;SJE… we will want to limit our evaluation to only sites with conifer trees since cloud2trees implements methods developed specifically to quantify conifer forest structure that may not be appropriate for other uses. we’ll use the field data in the package to look for NEON sites with conifer trees. We’ll use the NEON plant list to identify conifer species: https://data.neonscience.org/taxonomic-lists?taxonTypeCode=PLANT (click “DOWNLOAD TAXONOMIC LIST”). We’ll filter for species belonging to Class Pinopsida. conifer_spp &lt;- readr::read_csv( &quot;../data/OS_TAXON_PLANT-20220330T142149.csv&quot; , show_col_types = F , progress = F ) %&gt;% dplyr::filter( tolower(`class`) %in% c(&quot;pinopsida&quot;) ) %&gt;% dplyr::mutate( taxonID = toupper(taxonID) , vernacularName = tolower(vernacularName) , genus = stringr::str_to_title(genus) ) %&gt;% dplyr::distinct(taxonID, vernacularName, genus) what are some of these conifers? # huh? conifer_spp %&gt;% dplyr::slice_sample(n = 10) %&gt;% kableExtra::kbl(caption = &quot;Conifer species taxonID examples&quot;) %&gt;% kableExtra::kable_styling() Table 3.1: Conifer species taxonID examples taxonID vernacularName genus LAOC western larch Larix JUNIP juniper Juniperus PIRE5 papershell pinyon Pinus PIELE2 slash pine Pinus HESPE13SPP hybrid cypress Hesperotropsis PIEC2 shortleaf pine Pinus CALLI7 cypress-pine Callitris HEAB2 santa cruz cypress Hesperocyparis JUCOC4 common juniper Juniperus PIELD florida slash pine Pinus filter for NEON sites that have conifer trees based on field data from all terrestrial NEON sites with qualifying woody vegetation: https://data.neonscience.org/data-products/DP1.10098.001 conifer_sites &lt;- NeonTreeEvaluation::field %&gt;% dplyr::left_join( conifer_spp %&gt;% dplyr::mutate(is_conifer = 1) , by = &quot;taxonID&quot; ) %&gt;% dplyr::mutate(is_conifer = dplyr::coalesce(is_conifer, 0)) %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( tot = dplyr::n() , conifer = sum(is_conifer) , latitude = mean(plotLatitude) , longitude = mean(plotLongitude) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(pct_conifer = conifer/tot) what is the breakdown of woody vegetation sampled in NEON sites by the percent conifer? conifer_sites %&gt;% dplyr::select(-c(longitude,latitude)) %&gt;% dplyr::arrange(desc(pct_conifer), desc(tot)) %&gt;% dplyr::slice_head(n = 19) %&gt;% kableExtra::kbl(caption = &quot;Conifers in NEON sites&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 3.2: Conifers in NEON sites siteID tot conifer pct_conifer NIWO 1804 1804 1.00 ONAQ 88 88 1.00 MOAB 29 29 1.00 HEAL 21 21 1.00 YELL 13 13 1.00 TEAK 621 619 1.00 DEJU 173 169 0.98 ABBY 268 241 0.90 RMNP 1375 1083 0.79 SOAP 503 389 0.77 DSNY 34 26 0.76 TALL 2144 1521 0.71 OSBS 1288 858 0.67 JERC 562 336 0.60 HARV 3736 1701 0.46 TREE 1303 430 0.33 BART 3636 1022 0.28 BONA 188 48 0.26 STEI 754 151 0.20 let’s only keep NEON sites with &gt;50% of the woody vegetation sampled as conifer # minimum pct conifer min_conifer_pct &lt;- .5 # data frame of sites conifer_sites &lt;- conifer_sites %&gt;% dplyr::filter(pct_conifer&gt;=min_conifer_pct) finally, we’ll filter our lidar processing data for only these conifer sites lidar_df &lt;- lidar_df %&gt;% dplyr::inner_join(conifer_sites, by = &quot;siteID&quot;) %&gt;% sf::st_as_sf(coords = c(&quot;longitude&quot;,&quot;latitude&quot;), crs = 4326, remove = F) what NEON sites have conifers and the most lidar plots lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(siteID) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::slice_head(n=11) %&gt;% kableExtra::kbl(caption = &quot;NEON sites with conifers and lidar plots&quot;) %&gt;% kableExtra::kable_styling() Table 3.3: NEON sites with conifers and lidar plots siteID n TEAK 122 DEJU 87 YELL 86 SOAP 73 OSBS 71 NIWO 65 ABBY 55 TALL 54 JERC 50 DSNY 46 RMNP 30 where are these though? lidar_df %&gt;% dplyr::count(siteID) %&gt;% mapview::mapview( zcol = &quot;siteID&quot;, legend = F , layer.name = &quot;NEON site&quot; , col.regions = viridis::turbo(n=nrow(conifer_sites)) ) that’s pretty good geographic coverage and in places that we expect to have conifers ;D 3.3 Example validation process now that we have our lidar data that we can test our process against, let’s walk through the validation for a single plot "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
