[["index.html", "Aerial LiDAR for Fire Model Inputs Section 1 Introduction 1.1 Objective 1.2 Data", " Aerial LiDAR for Fire Model Inputs George Woolsey 26 March, 2025 Section 1 Introduction Code in support of “Using aerial LiDAR data for object-based physical fire modeling in conifer forests of the southwestern US” 1.1 Objective The objective of this study is to demonstrate the use of aerial LiDAR data to create inputs for physics-based fire models in frequent-fire forests of the southwestern United States. We review the methods used to extract tree location, species, and physical form from aerial LiDAR data. We evaluate this canopy crown detection methodology using a benchmark data set created to standardize evaluation metrics (Weinstein et al. 2021). We explain how to format this data for seamless integration with two commonly used object-based physical fire modeling tools. We demonstrate the end-to-end process using a case study from the southwestern United States. 1.2 Data Lidar data from the Mogollon Rim area of the Coconino National Forest about 20 km north of Payson, Arizona, USA was acquired from the USGS LidarExplorer. The aerial lidar data was collected between August 2013 and October 2014 under lidar project “AZ_USFS_3DEP_Processing_2019_D20” (project ID: 195122). "],["processing.html", "Section 2 Point Cloud Processing 2.1 Lidar Data Location 2.2 Individial Tree Detection Tuning: itd_tuning() 2.3 Point Cloud Tree Extraction: cloud2trees() 2.4 DBH Modeling: trees_dbh() 2.5 HMD Modeling: trees_hmd() 2.6 CBH Modeling: trees_cbh() 2.7 Forest Type: trees_type() 2.8 Combine data 2.9 Crown Biomass: trees_biomass() 2.10 Processing Time Summary 2.11 Data Export", " Section 2 Point Cloud Processing In this section we’ll process the raw point cloud data using the cloud2trees R package developed to provide accessible routines for processing point cloud data collected by airborne lidar or generated using UAS imagery and photogrammetry (e.g. structure from motion). The cloud2trees package can be installed by following the directions listed in the README file on GitHub. If one is still experiencing difficulties installing the package, see the example.R file which details how to install the package using a virgin R instance. ## remotes helps us get packages hosted on github install.packages(&quot;remotes&quot;) ## get cloud2trees remotes::install_github(repo = &quot;georgewoolsey/cloud2trees&quot;, upgrade = F) Load the standard libraries we use to do work # bread-and-butter library(tidyverse) # the tidyverse library(viridis) # viridis colors library(harrypotter) # hp colors library(RColorBrewer) # brewer colors library(scales) # work with number and plot scales library(latex2exp) # visualization library(mapview) # interactive html maps library(kableExtra) # tables library(patchwork) # combine plots # spatial analysis library(terra) # raster library(sf) # simple features library(lidR) # lidar data library(cloud2trees) # the cloud2trees 2.1 Lidar Data Location Let’s check out the lidar data we got from the Mogollon Rim area of the Coconino National Forest about 20 km north of Payson, Arizona, USA using the USGS LidarExplorer. # directory with the downloaded .las|.laz files f &lt;- &quot;e:/lidar_phys_fire_mods/data/mogollon_rim_az_lidar/&quot; # is there data? list.files(f, pattern = &quot;.*\\\\.(laz|las)$&quot;) %&gt;% length() ## [1] 42 # what files are in here? list.files(f, pattern = &quot;.*\\\\.(laz|las)$&quot;)[1:3] ## [1] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3808.laz&quot; ## [2] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3809.laz&quot; ## [3] &quot;USGS_LPC_AZ_USFS_3DEP_Processing_2019_D20_w0467n3810.laz&quot; what information does lidR read from the catalog? ctg_temp &lt;- lidR::readLAScatalog(f) ctg_temp ## class : LAScatalog (v1.4 format 6) ## extent : 467000, 473000, 3808000, 3815000 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83(2011) / UTM zone 12N + NAVD88 height - Geoid18 (m) ## area : 42 km² ## points : 571.95 million points ## density : 13.6 points/m² ## density : 11.3 pulses/m² ## num. files : 42 that’s a lot of points…can an ordinary laptop handle it? we’ll find out. We’ll plot our point cloud data tiles real quick to orient ourselves ctg_temp %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% mapview::mapview(popup = F, layer.name = &quot;point cloud tile&quot;) 2.2 Individial Tree Detection Tuning: itd_tuning() The cloud2trees package performs individual tree detection using lidR::locate_trees() with the lidR::lmf() algorithm. The local maximum filter algorithm allows for a constant window size or a variable window size defined by a function. See the lidR package book section by point cloud processing expert Jean-Romain Roussel for excellent detail on ITD and defining window size. The itd_tuning() function is used to visually assess tree crown delineation results from different window size functions used for the detection of individual trees. itd_tuning() allows users to test different window size functions on a sample of data to determine which function is most suitable for the area being analyzed. The preferred function can then be used in the ws parameter in raster2trees() and cloud2trees(). Let’s run itd_tuning() on our data starting with default window size functions # run itd_tuning() itd_tuning_ans &lt;- cloud2trees::itd_tuning(f) # what did we get? itd_tuning_ans %&gt;% names() ## [1] &quot;plot_samples&quot; &quot;ws_fn_list&quot; check the ws_fn_list return which includes the different window size functions tested # what ws_fn_list itd_tuning_ans$ws_fn_list %&gt;% str() ## List of 3 ## $ lin_fn:function (x) ## $ exp_fn:function (x) ## $ log_fn:function (x) let’s look at the function definition for the linear function (lin_fn) # the linear function itd_tuning_ans$ws_fn_list$lin_fn ## function (x) ## { ## y &lt;- dplyr::case_when(is.na(x) ~ 0.001, x &lt; 0 ~ 0.001, x &lt; ## 2 ~ 1, x &gt; 30 ~ 5, TRUE ~ 0.75 + (x * 0.14)) ## return(y) ## } ## &lt;bytecode: 0x0000011f63134080&gt; ## &lt;environment: 0x0000011f6315ed88&gt; let’s plot all of the functions we tested with our call to itd_tuning() using the defaults # shape of the ws functions ggplot() + geom_function(aes(color = &quot;lin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$lin_fn, lwd=1.2) + geom_function(aes(color = &quot;log_fn&quot;),fun=itd_tuning_ans$ws_fn_list$log_fn, lwd=1.2) + geom_function(aes(color = &quot;exp_fn&quot;),fun=itd_tuning_ans$ws_fn_list$exp_fn, lwd=1.2) + xlim(-5,60) + harrypotter::scale_color_hp_d(option = &quot;hermionegranger&quot;) + labs(x = &quot;heights&quot;, y = &quot;ws&quot;, color = &quot;ws function&quot;) + theme_light() now, let’s see how those window size functions impacted individual tree detection by checking the plot_samples return # tuning plot itd_tuning_ans$plot_samples Looking at the first sample, the logarithmic function (log_fn) resulted in too few trees detected in the overstory class. The clearest evidence of this is in the center of the left-hand side of the plot in the first sample. There is a clear “valley” in the CHM which the linear (lin_fn) and exponential (exp_fn) correctly split into two trees but the logarithmic function misses this split. Furthermore, the logarithmic function results in too many tree splits for short trees as can be seen in the second sample plot in the upper-right corner small tree group. The linear and the exponential function are very similar in detecting overstory trees but the linear function perhaps does a better job splitting up clumps of smaller trees. In the second sample plot the linear function does a better job splitting up the short tree group in the upper-right corner compared to the exponential function (there is no way that a tree that short [3-6 m tall] would have such a large crown area as in the exponential function split). If we had one gripe about the linear function, it’s maybe that it results in too many trees in small-tree patches. Let’s define our own custom linear function that slightly increases the window size for shorter trees compared to the default linear function. # custom linear function custom_lin &lt;- function (x){ y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &lt; 2 ~ 1.2 , x &gt; 30 ~ 5 , TRUE ~ 0.9 + (x * 0.139) ) return(y) } # shape of the ws functions ggplot() + geom_function(aes(color = &quot;lin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$lin_fn, lwd=1.2) + geom_function(aes(color = &quot;nonlin_fn&quot;),fun=itd_tuning_ans$ws_fn_list$nonlin_fn, lwd=1.2) + geom_function(aes(color = &quot;custom_lin&quot;),fun=custom_lin, lwd=1.2) + xlim(-5,60) + harrypotter::scale_color_hp_d(option = &quot;hermionegranger&quot;) + labs(x = &quot;heights&quot;, y = &quot;ws&quot;, color = &quot;ws function&quot;) + theme_light() We’ll run another sample test using itd_tuning()with our new function (call it “my_custom_lin” for extra clarity) compared to the default linear and exponential functions. itd_tuning_ans2 &lt;- cloud2trees::itd_tuning( f , ws_fn_list = list( my_custom_lin = custom_lin , lin_fn = itd_tuning_ans$ws_fn_list$lin_fn , exp_fn = itd_tuning_ans$ws_fn_list$exp_fn ) ) now, let’s see how those window size functions impacted individual tree detection by checking the plot_samples return # tuning plot itd_tuning_ans2$plot_samples Our custom linear function (my_custom_lin) strikes a good balance between detection of lower canopy trees (e.g. &lt;10 m in height) without improperly subdividing dominant canopy trees based on the areas sampled. Let’s move forward with our custom linear function in the raster2trees() and cloud2trees() functions. 2.3 Point Cloud Tree Extraction: cloud2trees() The cloud2trees() function combines methods in the cloud2trees package for an all-in-one approach. We’ll call this function without estimating any of the additional tree components (the estimate_* parameters) which we will do separately to show the full process. With all other options turned off, cloud2trees() will: 1) generate a CHM from the point cloud using cloud2raster(); and 2) perform individual tree detection using raster2trees(). cloud2trees_ans &lt;- cloud2trees::cloud2trees( output_dir = &quot;../data&quot; , input_las_dir = f # we defined this above , accuracy_level = 2 , dtm_res_m = 1 , chm_res_m = 0.25 , min_height = 2 , ws = custom_lin # here it is , keep_intrmdt = T # these are turned off by default but we&#39;ll be explicit , estimate_tree_dbh = F , estimate_tree_competition = F , estimate_tree_type = F , estimate_tree_hmd = F , estimate_tree_cbh = F ) we should have a spatial tree list with tree height attached cloud2trees_ans$crowns_sf %&gt;% dplyr::select(treeID, tree_x, tree_y, tree_height_m, crown_area_m2) %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 6 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, &quot;1872834&quot;, &quot;… ## $ tree_x &lt;dbl&gt; 467000.1, 467000.1, 467000.1, 467000.1, 467000.1, 467000… ## $ tree_y &lt;dbl&gt; 3808063, 3808065, 3808067, 3808074, 3808091, 3808124, 38… ## $ tree_height_m &lt;dbl&gt; 2.15, 2.34, 2.02, 2.11, 3.66, 4.37, 2.03, 5.60, 17.72, 2… ## $ crown_area_m2 &lt;dbl&gt; 0.3750, 0.7500, 0.2500, 0.3750, 1.2500, 0.9375, 0.3750, … ## $ geom &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((467000 3808..., MULTIPOLYGO… That’s a lot of trees! Let’s check some out in the central part of our study area overlaid on some satellite imagery. Note, that we do not expect the trees we detected to match with what is visible in the satellite imagery because the collection dates vary, the projections vary, the scales vary, etc, etc. cloud2trees_ans$crowns_sf %&gt;% sf::st_intersection( ctg_temp %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% sf::st_union() %&gt;% sf::st_centroid() %&gt;% sf::st_buffer(80, endCapStyle = &quot;SQUARE&quot;) %&gt;% sf::st_transform(sf::st_crs(cloud2trees_ans$crowns_sf)) ) %&gt;% mapview::mapview( layer.name = &quot;example trees&quot; , color = &quot;white&quot; , lwd = 1 , alpha.regions = 0 , label = FALSE , legend = FALSE , popup = FALSE , map.types = c(&quot;Esri.WorldImagery&quot;) ) Let’s look at the distribution of tree height in our study area # there are always tree heights cloud2trees_ans$treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m)) + ggplot2::geom_density(fill = &quot;navy&quot;, color = &quot;navy&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) …that’s a lot of small trees let’s look at the summary statistics cloud2trees_ans$treetops_sf$tree_height_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.870 4.550 8.236 11.520 69.990 The cloud2trees() function dropped off a lot of additional data in a folder titled “point_cloud_processing_delivery” which is nested where we told the command to write the data (output_dir = \"../data\" parameter setting). Let’s load in the “processed_tracking_data.csv” file to see how long that cloud2trees() process took to run. Run times are, of course, dependent on computer processing and I am working on a laptop typical of a spatial analyst (especially outside of the US Federal Government) running Windows with an Intel i7-10750H 6-core computer processor unit and 32 gigabytes of random-access memory. # load processed_tracking_data.csv processing_data &lt;- readr::read_csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , progress = F , show_col_types = F ) # what? processing_data %&gt;% dplyr::select(1:4) %&gt;% dplyr::glimpse() ## Rows: 1 ## Columns: 4 ## $ number_of_points &lt;dbl&gt; 571949084 ## $ las_area_m2 &lt;dbl&gt; 41999160 ## $ timer_cloud2raster_mins &lt;dbl&gt; 135.4638 ## $ timer_raster2trees_mins &lt;dbl&gt; 93.28212 let’s do some math # total tree extraction time trees_mins_temp &lt;- processing_data$timer_cloud2raster_mins[1] + processing_data$timer_raster2trees_mins[1] # ha ha_temp &lt;- round(processing_data$las_area_m2[1]/10000) # secs per ha rate_temp &lt;- (trees_mins_temp*60) / ha_temp # point density dens_temp &lt;- processing_data$number_of_points[1] / processing_data$las_area_m2[1] Tree extraction over 4,200 hectares took a total of 228.7 minutes at processing rate of 3.27 seconds per hectare on lidar data with a point density of 13.6 points per square meter. 2.4 DBH Modeling: trees_dbh() The trees_dbh() function uses the TreeMap FIA plot data in the area of the tree list to estimate the height-DBH allometry relationship. The height predicting DBH model built from the FIA data is then used to predict DBH based on tree height in the tree list. # where should we save the file? dbh_fnm &lt;- &quot;../data/point_cloud_processing_delivery/dbh_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(dbh_fnm)){ # time it st_temp &lt;- Sys.time() # run it trees_dbh_ans &lt;- cloud2trees::trees_dbh( tree_list = cloud2trees_ans$treetops_sf , outfolder = &quot;../data/point_cloud_processing_delivery&quot; ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_dbh_mins &lt;- mins_temp # save dbh trees_dbh_ans %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = dbh_fnm, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ # dbh data trees_dbh_ans &lt;- readr::read_csv(dbh_fnm, progress = F, show_col_types = F) } Estimating DBH for our tree list of 2.63 M trees over an area of 4,200 hectares took 23.4 minutes at a rate of 0.33 seconds per hectare let’s check the relationship between height and DBH as estimated by the regional allometric relationship trees_dbh_ans %&gt;% dplyr::slice_sample(n=7777) %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, y = dbh_cm)) + ggplot2::geom_point(color = &quot;navy&quot;, alpha = 0.6) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;tree DBH (cm)&quot;) + ggplot2::scale_x_continuous(limits = c(0,NA)) + ggplot2::scale_y_continuous(limits = c(0,NA)) + ggplot2::theme_light() Let’s look at the distribution of tree diameter in our study area trees_dbh_ans %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = dbh_cm)) + ggplot2::geom_density(fill = &quot;brown&quot;, color = &quot;brown&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree DBH (cm)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) …that’s a lot of small trees let’s look at the summary statistics trees_dbh_ans$dbh_cm %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.693 4.872 7.185 15.753 19.908 170.971 cloud2trees::trees_dbh() saved the actual model estimated using the Bayesian modelling package brms which we can load and review dbh_mod_temp &lt;- readRDS(&quot;../data/point_cloud_processing_delivery/regional_dbh_height_model.rds&quot;) # what is this? dbh_mod_temp %&gt;% class() ## [1] &quot;brmsfit&quot; we can draw fit curves with probability bands using the tidybayes package library(tidybayes) # define our height range to predict over dplyr::tibble(tree_height_m = seq(from = 0, to = 50, by = 1)) %&gt;% tidybayes::add_epred_draws(dbh_mod_temp, ndraws = 2000) %&gt;% ggplot2::ggplot(ggplot2::aes(x = tree_height_m)) + tidybayes::stat_lineribbon( ggplot2::aes(y = .epred, color = &quot;estimate&quot;) , .width = c(0.5,0.95) , lwd = 0.6 ) + ggplot2::scale_fill_brewer(palette = &quot;Oranges&quot;) + ggplot2::scale_color_manual(values = c(&quot;gray33&quot;)) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;est. tree DBH (cm)&quot;, color = &quot;&quot;) + ggplot2::scale_x_continuous(limits = c(0,NA), breaks = scales::extended_breaks(n=11)) + ggplot2::scale_y_continuous(limits = c(0,NA), breaks = scales::extended_breaks(n=11)) + ggplot2::theme_light() The probability bands are there in shades of orange but they are so tight to the median estimate that it’s impossible to see them. The model confidence bands are so narrow because the model was trained with lots of FIA measured trees over the broad study area we can check how many FIA measured trees were used to train our model because cloud2trees::trees_dbh() also writes the training data to the disk (that’s neat, but we don’t always need to see how the sausage is made) readr::read_csv( &quot;../data/point_cloud_processing_delivery/regional_dbh_height_model_training_data.csv&quot; , progress = F , show_col_types = F ) %&gt;% dplyr::summarise(training_trees = sum(tree_weight)) %&gt;% dplyr::pull(training_trees) %&gt;% scales::comma(accuracy = 1) ## [1] &quot;2,038,807&quot; that’s how many FIA measured trees were used to train our model 2.5 HMD Modeling: trees_hmd() The trees_hmd() function uses the tree crown polygons we delineated from the point cloud with the columns treeID and tree_height_m to attempt to extract height to maximum crown diameter (HMD) directly from the height normalized point cloud by finding the height of the non-ground point farthest from the tree center (i.e. tree top). HMD refers to the vertical height at which a tree’s crown has its widest horizontal spread. It describes a characteristic of the overall crown shape and structure. We have 2.63 M trees but we’ll attempt to extract HMD for the full tree list (100% census) and model the trees for which this process cannot extract a value based on the data we successfully extracted from the point cloud. This is a memory-intensive process, so we’ll clear all objects from our R session and read them back in later after processing. # clean session remove(list = base::setdiff(ls(), c(&quot;processing_data&quot;,&quot;dbh_fnm&quot;))) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 6146916 328.3 93446332 4990.6 71922472 3841.1 ## Vcells 13207459 100.8 617224017 4709.1 651011262 4966.9 # where should we save the file? hmd_fnm &lt;- &quot;../data/point_cloud_processing_delivery/hmd_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(hmd_fnm)){ # sample proportion sample_prop_temp &lt;- 1 # time it st_temp &lt;- Sys.time() # run it trees_hmd_ans &lt;- cloud2trees::trees_hmd( trees_poly = &quot;../data/point_cloud_processing_delivery&quot; , norm_las = &quot;../data/point_cloud_processing_delivery/norm_las/&quot; , tree_sample_prop = sample_prop_temp , force_same_crs = T , estimate_missing_hmd = T ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_hmd_mins &lt;- mins_temp processing_data$sttng_hmd_tree_sample_n &lt;- NA processing_data$sttng_hmd_tree_sample_prop &lt;- sample_prop_temp # save hmd trees_hmd_ans %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = hmd_fnm, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ # hmd data trees_hmd_ans &lt;- readr::read_csv(hmd_fnm, progress = F, show_col_types = F) } HMD extraction took a total of 333.5 minutes at processing rate of 7.61 seconds per 1,000 trees We attempted to extract HMD from 100% of our tree list, let’s see our success rate trees_hmd_ans %&gt;% dplyr::count(is_training_hmd) %&gt;% dplyr::mutate(pct = n/sum(n)) ## # A tibble: 2 × 3 ## is_training_hmd n pct ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 1414752 0.538 ## 2 TRUE 1214743 0.462 all of the records marked as “training data” had HMD successfully extracted from the point cloud and were used to estimate a height-HMD allometry relationship that is spatially informed using the relative tree location let’s look at the training versus the modeled HMD versus height trees_hmd_ans %&gt;% dplyr::slice_sample(n = 11111) %&gt;% dplyr::arrange(desc(is_training_hmd)) %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, y = max_crown_diam_height_m, color=is_training_hmd)) + ggplot2::geom_point() + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;tree HMD (m)&quot;) + ggplot2::scale_y_continuous(breaks = scales::extended_breaks(n=12)) + ggplot2::scale_x_continuous(breaks = scales::extended_breaks(n=14)) + ggplot2::scale_color_viridis_d(option = &quot;turbo&quot;, begin = 0.2, alpha = 0.7, name = &quot;is HMD\\nfrom cloud?&quot;) + ggplot2::theme_light() Let’s look at the distribution of HMD in our study area trees_hmd_ans %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = max_crown_diam_height_m)) + ggplot2::geom_density(fill = &quot;coral&quot;, color = &quot;coral&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree HMD (m)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) and look at the summary statistics of HMD trees_hmd_ans$max_crown_diam_height_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.660 2.080 2.886 3.007 39.490 2.6 CBH Modeling: trees_cbh() The trees_cbh() function uses the tree crown polygons we delineated from the point cloud with the columns treeID and tree_height_m to attempt to extract crown base height (CBH) directly from the height normalized point cloud using the process outlined in Viedma et al. (2024). This is a memory-intensive process, so we’ll clear all objects from our R session and read them back in later after processing. # clean session remove(list = base::setdiff(ls(), c(&quot;processing_data&quot;,&quot;dbh_fnm&quot;,&quot;hmd_fnm&quot;))) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 6167469 329.4 74757066 3992.5 71922472 3841.1 ## Vcells 69280000 528.6 493779214 3767.3 651011262 4966.9 We’ll attempt to extract CBH for a sample and model the rest based on the data we successfully extracted from the point cloud. # where should we save the file? cbh_fnm &lt;- &quot;../data/point_cloud_processing_delivery/cbh_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(cbh_fnm)){ # sample proportion sample_prop_temp &lt;- 0.50 # time it st_temp &lt;- Sys.time() # run it trees_cbh_ans &lt;- cloud2trees::trees_cbh( trees_poly = &quot;../data/point_cloud_processing_delivery&quot; , norm_las = &quot;../data/point_cloud_processing_delivery/norm_las/&quot; , tree_sample_prop = sample_prop_temp , which_cbh = &quot;lowest&quot; , estimate_missing_cbh = TRUE , min_vhp_n = 3 , voxel_grain_size_m = 1 , dist_btwn_bins_m = 1 , min_fuel_layer_ht_m = 1 , lad_pct_gap = 25 , lad_pct_base = 25 , num_jump_steps = 1 , min_lad_pct = 10 , frst_layer_min_ht_m = 1 , force_same_crs = T ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_cbh_mins &lt;- mins_temp processing_data$sttng_cbh_tree_sample_n &lt;- NA processing_data$sttng_cbh_tree_sample_prop &lt;- sample_prop_temp # save cbh trees_cbh_ans %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = cbh_fnm, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ # cbh data trees_cbh_ans &lt;- readr::read_csv(cbh_fnm, progress = F, show_col_types = F) } CBH extraction took a total of 3,387.7 minutes (56.5 hours) at processing rate of 77.30 seconds per 1,000 trees We attempted to extract CBH from 50% of our tree list (1.31 M trees), let’s see our success rate # scales::percent(processing_data$sttng_cbh_tree_sample_prop) trees_cbh_ans %&gt;% dplyr::count(is_training_cbh) %&gt;% dplyr::mutate(pct = n/sum(n)) ## # A tibble: 2 × 3 ## is_training_cbh n pct ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 2291970 0.872 ## 2 TRUE 337525 0.128 That equates to a 25.7% CBH extraction success rate….not a great success ;( This is especially worrisome given the significant time required to attempt CBH extraction from the point cloud. The low success rate was influenced by numerous factors including: 1) the low point density of this aerial point cloud; 2) the abundance of short, understory trees which likely lack a sufficient point density to extract a defined LAD profile; 3) in the settings used in the trees_cbh() call we required a tree to have at least three vertical height profiles using a 1 m voxel grain size and classified fuel layers as having at least 10 percent leaf area density. all of the records marked as training data had CBH successfully extracted from the point cloud and were used to estimate a height-CBH allometry relationship that is spatially informed using the relative tree location let’s look at the training versus the modeled CBH versus height trees_cbh_ans %&gt;% dplyr::slice_sample(n = 11111) %&gt;% dplyr::arrange(is_training_cbh) %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, y = tree_cbh_m, color=is_training_cbh)) + ggplot2::geom_point() + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;tree CBH (m)&quot;) + ggplot2::scale_y_continuous(breaks = scales::extended_breaks(n=12)) + ggplot2::scale_x_continuous(breaks = scales::extended_breaks(n=14)) + ggplot2::scale_color_viridis_d(alpha = 0.7, name = &quot;is CBH\\nfrom cloud?&quot;) + ggplot2::theme_light() Let’s look at the distribution of CBH in our study area trees_cbh_ans %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_cbh_m)) + ggplot2::geom_density(fill = &quot;maroon4&quot;, color = &quot;maroon4&quot;, alpha = 0.2) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree CBH (m)&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text.y = ggplot2::element_blank(), axis.ticks.y = ggplot2::element_blank()) and look at the summary statistics of CBH trees_cbh_ans$tree_cbh_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.500 2.396 3.045 3.668 4.500 28.500 clean our session # clean session remove(list = base::setdiff(ls(), c(&quot;processing_data&quot;,&quot;dbh_fnm&quot;,&quot;hmd_fnm&quot;,&quot;cbh_fnm&quot;))) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 6167547 329.4 59805653 3194.0 71922472 3841.1 ## Vcells 69280672 528.6 395023372 3013.8 651011262 4966.9 2.7 Forest Type: trees_type() We’ll now use trees_type() to attach species information using USDA Forest Inventory and Analysis (FIA) codes. FIA Forest Type Group Code is attached to each tree in the tree list based on the spatial overlap with the Forest Type Groups of the Continental United States data (Wilson 2023). We now need to read back in our full spatial data frame of points to use as the input tree list, let’s just load the tree points # get the data from already run treetops_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_tree_tops.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read(dsn = x, quiet = T) ) %&gt;% dplyr::bind_rows() let’s get the FIA forest type group for our list # where should we save the file? type_fnm &lt;- &quot;../data/point_cloud_processing_delivery/type_data.csv&quot; type_rast_fnm &lt;- &quot;../data/point_cloud_processing_delivery/type_rast.tif&quot; # if we don&#39;t already have the data, run it if(!file.exists(type_fnm)){ # time it st_temp &lt;- Sys.time() # run it trees_type_ans &lt;- cloud2trees::trees_type( tree_list = treetops_sf , study_boundary = sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_union() ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_type_mins &lt;- mins_temp # save type trees_type_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = type_fnm, row.names = F, append = F) # save raster trees_type_ans$foresttype_rast %&gt;% terra::writeRaster(type_rast_fnm, overwrite=T) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ trees_type_ans &lt;- list() # type data trees_type_ans$tree_list &lt;- readr::read_csv(type_fnm, progress = F, show_col_types = F) # raster trees_type_ans$foresttype_rast &lt;- terra::rast(type_rast_fnm) } Let’s look at the FIA Forest Type Group data we extracted for the tree list. trees_type_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(forest_type_group_code, forest_type_group) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::mutate(pct = n/sum(n)) %&gt;% kableExtra::kbl(caption = &quot;Count of trees by FIA Forest Type Group&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 2.1: Count of trees by FIA Forest Type Group forest_type_group_code forest_type_group n pct 220 Ponderosa pine group 2143301 0.82 970 Woodland hardwoods group 239301 0.09 180 Pinyon / juniper group 221663 0.08 200 Douglas-fir group 20484 0.01 260 Fir / spruce / mountain hemlock group 4731 0.00 900 Aspen / birch group 15 0.00 Let’s attach FIA Forest Types Group name to the raster (foresttype_rast) of the area we searched and plot it # load in the forest type data ext_data_temp &lt;- cloud2trees::find_ext_data() foresttype_lookup &lt;- file.path(ext_data_temp$foresttype_dir, &quot;foresttype_lookup.csv&quot;) %&gt;% readr::read_csv(progress = F, show_col_types = F) %&gt;% dplyr::distinct(forest_type_group_code, forest_type_group, hardwood_softwood) # what? foresttype_lookup %&gt;% dplyr::glimpse() ## Rows: 35 ## Columns: 3 ## $ forest_type_group_code &lt;dbl&gt; 100, 120, 140, 150, 160, 170, 180, 200, 220, 24… ## $ forest_type_group &lt;chr&gt; &quot;White / red / jack pine group&quot;, &quot;Spruce / fir … ## $ hardwood_softwood &lt;chr&gt; &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;,… plot the FIA Forest Types Group raster of our study area # study area aoi_temp &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_transform(terra::crs(trees_type_ans$foresttype_rast)) # plot raster r_plt &lt;- trees_type_ans$foresttype_rast %&gt;% terra::crop(aoi_temp %&gt;% sf::st_buffer(100) %&gt;% terra::vect()) %&gt;% terra::as.data.frame(xy=T) %&gt;% dplyr::rename(forest_type_group_code = 3) %&gt;% dplyr::left_join(foresttype_lookup, by = &quot;forest_type_group_code&quot;) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile(mapping = ggplot2::aes(x=x, y=y, fill = forest_type_group)) + ggplot2::labs(fill = &quot;FIA forest\\ntype group&quot;) + harrypotter::scale_fill_hp_d(&quot;lunalovegood&quot;, direction=-1) + ggplot2::theme_void() + ggplot2::theme(legend.position = &quot;top&quot;) + ggplot2::guides(fill = ggplot2::guide_legend(nrow = 2, byrow = T)) # add our study boundary r_plt + ggplot2::geom_sf(data = aoi_temp, color = &quot;white&quot;, fill = NA) 2.8 Combine data before our last tree component calculation, we’ll bring all of our data together because the last component relies on this data # read all of our data back in trees_dbh_temp &lt;- readr::read_csv( dbh_fnm , col_select = c( treeID, tidyselect::contains(&quot;dbh&quot;) , tidyselect::starts_with(&quot;basal_area&quot;) , tidyselect::starts_with(&quot;radius&quot;) ) , show_col_types = F , progress = F ) trees_cbh_temp &lt;- readr::read_csv( cbh_fnm , col_select = c(treeID, tree_cbh_m, is_training_cbh) , show_col_types = F , progress = F ) trees_hmd_temp &lt;- readr::read_csv( hmd_fnm , col_select = c(treeID, max_crown_diam_height_m, is_training_hmd) , show_col_types = F , progress = F ) trees_type_temp &lt;- readr::read_csv( type_fnm , col_select = c(treeID, tidyselect::starts_with(&quot;forest_type&quot;), hardwood_softwood) , show_col_types = F , progress = F ) # function re-cast treeID if needed recast_id_fn &lt;- function(df, cl) { if( !inherits( df$treeID , cl ) ){ if(cl==&quot;character&quot;){ df$treeID &lt;- format(df$treeID, scientific = F, trim = T) %&gt;% as.character() %&gt;% stringr::str_squish() }else if(cl==&quot;numeric&quot;){ df$treeID &lt;- as.numeric(df$treeID) } } return(df) } # apply trees_dbh_temp &lt;- trees_dbh_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) trees_cbh_temp &lt;- trees_cbh_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) trees_hmd_temp &lt;- trees_hmd_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) trees_type_temp &lt;- trees_type_temp %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) # c(&quot;700000&quot;,&quot;600000&quot;,&quot;100000&quot;,&quot;200000&quot;,&quot;300000&quot;,&quot;2000000&quot;,&quot;800000&quot;,&quot;400000&quot;,&quot;1000000&quot;,&quot;900000&quot;,&quot;500000&quot; ) # trees_type_temp %&gt;% # dplyr::filter( # treeID %in% # c(&quot;700000&quot;,&quot;600000&quot;,&quot;100000&quot;,&quot;200000&quot;,&quot;300000&quot;,&quot;2000000&quot;,&quot;800000&quot;,&quot;400000&quot;,&quot;1000000&quot;,&quot;900000&quot;,&quot;500000&quot; ) # ) # now we&#39;ll get a list of the columns in our component data to drop from our main data cols_to_drop_temp &lt;- c( names(trees_dbh_temp), names(trees_cbh_temp) , names(trees_hmd_temp), names(trees_type_temp) ) %&gt;% unique() %&gt;% setdiff(&quot;treeID&quot;) # remove cols from our primary data treetops_sf &lt;- treetops_sf %&gt;% dplyr::select( -dplyr::any_of(cols_to_drop_temp)) # join altogether using the magical purrr::reduce treetops_sf &lt;- list( treetops_sf, trees_dbh_temp, trees_cbh_temp , trees_hmd_temp, trees_type_temp ) %&gt;% purrr::reduce(\\(x,y) dplyr::left_join(x, y, by = &quot;treeID&quot;)) let’s glimpse our almost final data treetops_sf %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 36 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, … ## $ crown_area_m2 &lt;dbl&gt; 0.3750, 0.7500, 0.2500, 0.3750, 1.2500, 0.93… ## $ tree_height_m &lt;dbl&gt; 2.15, 2.34, 2.02, 2.11, 3.66, 4.37, 2.03, 5.… ## $ is_training_data &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ comp_trees_per_ha &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_relative_tree_height &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_dist_to_nearest_m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_stand_id &lt;dbl&gt; 71791, 71791, 71791, 71791, 71540, 71289, 70… ## $ cruz_tree_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_stand_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_crown_biomass_kg &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ landfire_stand_id &lt;dbl&gt; 56399, 56399, 56399, 56158, 56158, 55917, 55… ## $ crown_dia_m &lt;dbl&gt; 0.6909883, 0.9772050, 0.5641896, 0.6909883, … ## $ crown_length_m &lt;dbl&gt; 0.1666052, 0.1948469, 0.1565314, 0.1635056, … ## $ crown_volume_m3 &lt;dbl&gt; 0.04165130, 0.09742345, 0.02608857, 0.040876… ## $ landfire_tree_kg_per_m3 &lt;dbl&gt; 1.3947342, 1.3947342, 1.3947342, 0.1140463, … ## $ landfire_stand_kg_per_m3 &lt;dbl&gt; 0.06, 0.06, 0.06, 0.27, 0.27, 0.09, 0.08, 0.… ## $ landfire_crown_biomass_kg &lt;dbl&gt; 0.058092493, 0.135879822, 0.036386614, 0.004… ## $ fia_est_dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ fia_est_dbh_cm_lower &lt;dbl&gt; 1.548500, 1.572502, 1.425198, 1.466369, 2.23… ## $ fia_est_dbh_cm_upper &lt;dbl&gt; 7.350390, 7.661050, 6.790382, 7.022582, 10.9… ## $ dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ dbh_m &lt;dbl&gt; 0.03977012, 0.04149075, 0.03693238, 0.038402… ## $ ptcld_extracted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ ptcld_predicted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ basal_area_m2 &lt;dbl&gt; 0.001242235, 0.001352049, 0.001071284, 0.001… ## $ basal_area_ft2 &lt;dbl&gt; 0.01337142, 0.01455345, 0.01153130, 0.012467… ## $ radius_m &lt;dbl&gt; 0.01988506, 0.02074537, 0.01846619, 0.019201… ## $ tree_cbh_m &lt;dbl&gt; 1.983395, 2.145153, 1.863469, 1.946494, 2.79… ## $ is_training_cbh &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ max_crown_diam_height_m &lt;dbl&gt; 1.504257, 1.568958, 0.880000, 1.520000, 1.74… ## $ is_training_hmd &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE… ## $ forest_type_group_code &lt;dbl&gt; 180, 180, 180, 180, 180, 180, 180, 180, 180,… ## $ forest_type_group &lt;chr&gt; &quot;Pinyon / juniper group&quot;, &quot;Pinyon / juniper … ## $ hardwood_softwood &lt;chr&gt; &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwoo… ## $ geom &lt;POINT [m]&gt; POINT (467000.1 3808063), POINT (46700… 2.9 Crown Biomass: trees_biomass() Lastly, we’ll use trees_biomass() to estimate tree crown biomass in kilograms. We’ll estimate biomass based on: 1) LANDFIRE’s Forest Canopy Bulk Density (CBD) data; and 2) based on the Cruz et al. (2003) equations and the FIA forest type group we got above # where should we save the file? biomass_fnm &lt;- &quot;../data/point_cloud_processing_delivery/biomass_data.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(biomass_fnm)){ # time it st_temp &lt;- Sys.time() # run it trees_biomass_ans &lt;- cloud2trees::trees_biomass( tree_list = treetops_sf , method = c(&quot;cruz&quot;,&quot;landfire&quot;) , study_boundary = sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_union() ) # timer mins_temp &lt;- difftime(Sys.time(),st_temp,units = &quot;mins&quot;) %&gt;% as.numeric() processing_data$timer_trees_biomass_mins &lt;- mins_temp # save biomass trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = biomass_fnm, row.names = F, append = F) # save raster df trees_biomass_ans$stand_cell_data_landfire %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_landfire.csv&quot;, row.names = F, append = F) trees_biomass_ans$stand_cell_data_cruz %&gt;% sf::st_drop_geometry() %&gt;% write.csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_cruz.csv&quot;, row.names = F, append = F) # save tracking processing_data %&gt;% write.csv( file = &quot;../data/point_cloud_processing_delivery/processed_tracking_data.csv&quot; , row.names = F, append = F ) }else{ trees_biomass_ans &lt;- list() # biomass data trees_biomass_ans$tree_list &lt;- readr::read_csv(biomass_fnm, progress = F, show_col_types = F) # raster trees_biomass_ans$stand_cell_data_landfire &lt;- readr::read_csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_landfire.csv&quot;, progress = F, show_col_types = F) trees_biomass_ans$stand_cell_data_cruz &lt;- readr::read_csv(file = &quot;../data/point_cloud_processing_delivery/stand_cell_data_cruz.csv&quot;, progress = F, show_col_types = F) } Tree crown biomass estimation took a total of 5.8 minutes at processing rate of 0.08 seconds per hectare The trees_biomass() process constrains tree crown bulk density (CBD) values to 2.0 kilograms per cubic meter by default based on Mell et al. (2009) who found the dry bulk density of the tree crown was 2.6 kilograms per cubed meter using Douglas-fir trees grown on Christmas tree farms. Let’s check the distribution of CBD values estimated in our study area. trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(tidyselect::ends_with(&quot;tree_kg_per_m3&quot;)) %&gt;% tidyr::pivot_longer(cols = tidyselect::everything()) %&gt;% dplyr::mutate(name = stringr::str_remove_all(name,&quot;_tree_kg_per_m3&quot;)) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, color = name, fill = name)) + ggplot2::geom_density() + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_fill_viridis_d(option = &quot;turbo&quot;, begin = 0.3, end = 0.7, alpha = 0.7, name = &quot;biomass method&quot;) + ggplot2::scale_color_viridis_d(option = &quot;turbo&quot;, begin = 0.3, end = 0.7,name = &quot;biomass method&quot;) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(8)) + ggplot2::labs(x = &quot;tree kilograms per cubic meter&quot;, y = &quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(legend.position = &quot;none&quot;) let’s look at the summary of the tree CBD values and the resulting crown biomass trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(tidyselect::ends_with(&quot;tree_kg_per_m3&quot;), tidyselect::ends_with(&quot;crown_biomass_kg&quot;)) %&gt;% summary() ## cruz_tree_kg_per_m3 landfire_tree_kg_per_m3 cruz_crown_biomass_kg ## Min. :0.0 Min. :0.004402 Min. : 0.0 ## 1st Qu.:0.2 1st Qu.:0.071797 1st Qu.: 0.1 ## Median :0.2 Median :0.104729 Median : 0.8 ## Mean :0.3 Mean :0.158657 Mean : 22.4 ## 3rd Qu.:0.3 3rd Qu.:0.155825 3rd Qu.: 16.1 ## Max. :2.0 Max. :1.999384 Max. :3504.8 ## NA&#39;s :460979 NA&#39;s :460979 ## landfire_crown_biomass_kg ## Min. : 0.000 ## 1st Qu.: 0.045 ## Median : 0.275 ## Mean : 9.314 ## 3rd Qu.: 4.060 ## Max. :3947.367 ## note those NA values for the Cruz et al. (2003) method, these NA values are introduced because this particular methodology is limited in the forest types for which biomass equations exist check out the break-down by FIA forest type group trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(!is.na(forest_type_group)) %&gt;% dplyr::group_by(forest_type_group) %&gt;% dplyr::summarise( n = dplyr::n() , dplyr::across( cruz_tree_kg_per_m3 , .fns = list(mean = mean, sd = sd, min = min, max = max) ) ) %&gt;% dplyr::rename_with(~ stringr::str_remove_all(.x,&quot;cruz_tree_kg_per_m3_&quot;)) %&gt;% dplyr::arrange(desc(n)) %&gt;% kableExtra::kbl(caption = &quot;Summary of tree CBD (kg per m3) by FIA Forest Type Group&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 2.2: Summary of tree CBD (kg per m3) by FIA Forest Type Group forest_type_group n mean sd min max Ponderosa pine group 2143301 0.29 0.17 0.04 2.00 Woodland hardwoods group 239301 NA NA NA NA Pinyon / juniper group 221663 NA NA NA NA Douglas-fir group 20484 0.19 0.05 0.07 0.37 Fir / spruce / mountain hemlock group 4731 0.16 0.07 0.06 1.03 Aspen / birch group 15 NA NA NA NA We can see how the LANDFIRE Forest Canopy Bulk Density (CBD) data and Cruz et al. (2003) methodologies compare at estimating tree crown biomass for our study area we’ll make this comparison only for trees that have biomass estimated using the Cruz method as well # set the upper limit scale ul_temp &lt;- max( quantile(trees_biomass_ans$tree_list$cruz_crown_biomass_kg, probs = 0.95, na.rm = T) , quantile(trees_biomass_ans$tree_list$landfire_crown_biomass_kg, probs = 0.95, na.rm = T) ) # plot tree landfire vs. cruz crown biomass estimate trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(!is.na(cruz_crown_biomass_kg)) %&gt;% dplyr::slice_sample(n=11111) %&gt;% ggplot2::ggplot( mapping = ggplot2::aes( x = landfire_crown_biomass_kg, y = cruz_crown_biomass_kg ) ) + ggplot2::geom_abline(lwd = 1.5) + ggplot2::geom_point(ggplot2::aes(color = tree_height_m)) + ggplot2::geom_smooth(method = &quot;lm&quot;, se=F, color = &quot;tomato&quot;, linetype = &quot;dashed&quot;) + ggplot2::scale_color_viridis_c(option = &quot;mako&quot;, direction = -1, alpha = 0.8) + ggplot2::scale_x_continuous(limits = c(0, ul_temp)) + ggplot2::scale_y_continuous(limits = c(0, ul_temp)) + ggplot2::labs( x = &quot;LANDFIRE crown biomass (kg)&quot; , y = &quot;Cruz crown biomass (kg)&quot; , color = &quot;tree ht. (m)&quot; ) + ggplot2::theme_light() we can summarize the average difference between the two methods for records with both estimates trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(!is.na(cruz_crown_biomass_kg)) %&gt;% dplyr::mutate( diff_cruz_lf_kg = cruz_crown_biomass_kg-landfire_crown_biomass_kg , pct_diff_cruz_lf_kg = diff_cruz_lf_kg/landfire_crown_biomass_kg ) %&gt;% dplyr::summarise( dplyr::across( c(cruz_crown_biomass_kg,landfire_crown_biomass_kg,diff_cruz_lf_kg,pct_diff_cruz_lf_kg) , .fns = list(mean = mean) , .names = &quot;{.fn}_{.col}&quot; ) ) %&gt;% dplyr::mutate( mean_pct_diff_cruz_lf_kg = scales::percent(mean_pct_diff_cruz_lf_kg, accuracy = 0.1) , dplyr::across( .cols = -mean_pct_diff_cruz_lf_kg , .fns = ~scales::comma(.x,accuracy = 0.01) ) ) %&gt;% tidyr::pivot_longer(tidyselect::everything()) %&gt;% kableExtra::kbl(caption = &quot;Mean difference between LANDFIRE and Cruz estimated tree crown biomass in kilograms&quot;, digits = 2) %&gt;% kableExtra::kable_styling() Table 2.3: Mean difference between LANDFIRE and Cruz estimated tree crown biomass in kilograms name value mean_cruz_crown_biomass_kg 22.40 mean_landfire_crown_biomass_kg 10.68 mean_diff_cruz_lf_kg 11.72 mean_pct_diff_cruz_lf_kg 185.0% Finally, let’s plot the spatial arrangement of estimated biomass using the raster data returned from trees_biomass() First, for LANDFIRE # aoi aoi_temp &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) ## Reading layer `raw_las_ctg_info&#39; from data source ## `C:\\Data\\usfs\\lidar_phys_fire_mods\\data\\point_cloud_processing_delivery\\raw_las_ctg_info.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 42 features and 34 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 467000 ymin: 3808000 xmax: 473000 ymax: 3815000 ## Projected CRS: NAD83(2011) / UTM zone 12N # get the projection for the stand cell data epsg_code_temp &lt;- trees_biomass_ans$stand_cell_data_landfire$rast_epsg_code[1] %&gt;% as.numeric() # set the color limit ul_temp &lt;- max( max(trees_biomass_ans$stand_cell_data_landfire$landfire_stand_kg_per_m3, na.rm = T) , max(trees_biomass_ans$stand_cell_data_cruz$cruz_stand_kg_per_m3, na.rm = T) ) # plot the stand cell data with trees overlaid trees_biomass_ans$stand_cell_data_landfire %&gt;% dplyr::filter(trees&gt;0) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile(ggplot2::aes(x=x,y=y,fill = landfire_stand_kg_per_m3), color = NA) + ggplot2::geom_sf(data = aoi_temp %&gt;% sf::st_transform(crs = epsg_code_temp), fill = NA) + ggplot2::labs(subtitle = &quot;LANDFIRE stand kg/m3&quot;, fill=&quot;LANDFIRE\\nstand kg/m3&quot;) + ggplot2::scale_fill_viridis_c( limits = c(NA,ul_temp) , option = &quot;rocket&quot;, direction = -1, na.value = &quot;white&quot; ) + ggplot2::coord_sf() + ggplot2::theme_void() and for Cruz # get the projection for the stand cell data epsg_code_temp &lt;- trees_biomass_ans$stand_cell_data_cruz$rast_epsg_code[1] %&gt;% as.numeric() # plot the stand cell data with trees overlaid trees_biomass_ans$stand_cell_data_cruz %&gt;% dplyr::filter(trees&gt;0) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile(ggplot2::aes(x=x,y=y,fill = cruz_stand_kg_per_m3), color = NA) + ggplot2::geom_sf(data = aoi_temp %&gt;% sf::st_transform(crs = epsg_code_temp), fill = NA) + ggplot2::labs(subtitle = &quot;Cruz stand kg/m3&quot;, fill=&quot;Cruz\\nstand kg/m3&quot;) + ggplot2::scale_fill_viridis_c( limits = c(NA,ul_temp) , option = &quot;rocket&quot;, direction = -1, na.value = &quot;white&quot; ) + ggplot2::theme_void() 2.10 Processing Time Summary Let’s summarize how long all of this point cloud processing took and determine which steps were most computationally intensive. # aggregate the total processing time processing_data &lt;- processing_data %&gt;% dplyr::mutate( timer_total_time_mins = timer_cloud2raster_mins + timer_raster2trees_mins + timer_trees_dbh_mins + timer_trees_cbh_mins + timer_trees_type_mins + timer_trees_hmd_mins + timer_trees_biomass_mins + timer_write_data_mins , timer_tree_extraction_mins = timer_cloud2raster_mins + timer_raster2trees_mins ) %&gt;% dplyr::mutate(dplyr::across( .cols = c(timer_tree_extraction_mins, timer_trees_dbh_mins, timer_trees_cbh_mins, timer_trees_type_mins, timer_trees_hmd_mins, timer_trees_biomass_mins, timer_write_data_mins) , .fns = ~ .x/timer_total_time_mins , .names = &quot;{.col}_pct&quot; )) # table it table_temp &lt;- processing_data %&gt;% dplyr::select( c(timer_tree_extraction_mins, timer_trees_dbh_mins, timer_trees_cbh_mins, timer_trees_type_mins, timer_trees_hmd_mins, timer_trees_biomass_mins, timer_write_data_mins , c(tidyselect::ends_with(&quot;_pct&quot;) &amp; tidyselect::starts_with(&quot;timer_&quot;)) ) ) %&gt;% tidyr::pivot_longer(dplyr::everything()) %&gt;% dplyr::mutate( units = stringr::word(name, -1, sep = &quot;_&quot;) , section = name %&gt;% stringr::str_remove_all(&quot;timer_&quot;) %&gt;% stringr::str_remove_all(&quot;_mins&quot;) %&gt;% stringr::str_remove_all(&quot;_pct&quot;) ) %&gt;% dplyr::select(-name) %&gt;% tidyr::pivot_wider(names_from = units, values_from = value) %&gt;% dplyr::mutate( mins_lab = scales::comma(mins,accuracy = 0.1) , pct_lab = scales::percent(pct,accuracy = 0.1) ) table it table_temp %&gt;% dplyr::select(section, tidyselect::ends_with(&quot;_lab&quot;)) %&gt;% kableExtra::kbl( caption = &quot;Point cloud processing section run time&quot; , col.names = c( &quot;Processing section&quot; , &quot;time (minutes)&quot; , &quot;% of total time&quot; ) ) %&gt;% kableExtra::kable_styling() Table 2.4: Point cloud processing section run time Processing section time (minutes) % of total time tree_extraction 228.7 5.7% trees_dbh 23.4 0.6% trees_cbh 3,387.7 84.8% trees_type 2.1 0.1% trees_hmd 333.5 8.3% trees_biomass 5.8 0.1% write_data 16.0 0.4% plot it 2.11 Data Export Now that we have our point cloud-extracted tree list with all of the tree component measurements attached, let’s save the data for use in our analysis first we’ll attach the biomass metrics to our spatial tree list # new columns added by trees_biomass() names_temp &lt;- setdiff( trees_biomass_ans$tree_list %&gt;% names() , treetops_sf %&gt;% dplyr::select(!tidyselect::starts_with(&quot;cruz_&quot;)) %&gt;% dplyr::select(!tidyselect::starts_with(&quot;landfire_&quot;)) %&gt;% dplyr::select( -dplyr::any_of(c( &quot;crown_dia_m&quot;,&quot;crown_length_m&quot;,&quot;crown_volume_m3&quot; ))) %&gt;% names() ) # recast id if needed trees_biomass_ans$tree_list &lt;- trees_biomass_ans$tree_list %&gt;% recast_id_fn(cl = class(treetops_sf$treeID)) # attach to our spatial tree points treetops_sf &lt;- treetops_sf %&gt;% dplyr::select(!tidyselect::starts_with(&quot;cruz_&quot;)) %&gt;% dplyr::select(!tidyselect::starts_with(&quot;landfire_&quot;)) %&gt;% dplyr::select( -dplyr::any_of(c( &quot;crown_dia_m&quot;,&quot;crown_length_m&quot;,&quot;crown_volume_m3&quot; ))) %&gt;% dplyr::left_join( trees_biomass_ans$tree_list %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(dplyr::all_of(c(&quot;treeID&quot;, names_temp))) , by = &quot;treeID&quot; ) # what does our final data look like? treetops_sf %&gt;% dplyr::glimpse() ## Rows: 2,629,495 ## Columns: 38 ## $ treeID &lt;chr&gt; &quot;1877144&quot;, &quot;1876803&quot;, &quot;1876566&quot;, &quot;1875427&quot;, … ## $ crown_area_m2 &lt;dbl&gt; 0.3750, 0.7500, 0.2500, 0.3750, 1.2500, 0.93… ## $ tree_height_m &lt;dbl&gt; 2.15, 2.34, 2.02, 2.11, 3.66, 4.37, 2.03, 5.… ## $ is_training_data &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ comp_trees_per_ha &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_relative_tree_height &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ comp_dist_to_nearest_m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ fia_est_dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ fia_est_dbh_cm_lower &lt;dbl&gt; 1.548500, 1.572502, 1.425198, 1.466369, 2.23… ## $ fia_est_dbh_cm_upper &lt;dbl&gt; 7.350390, 7.661050, 6.790382, 7.022582, 10.9… ## $ dbh_cm &lt;dbl&gt; 3.977012, 4.149075, 3.693238, 3.840213, 5.92… ## $ dbh_m &lt;dbl&gt; 0.03977012, 0.04149075, 0.03693238, 0.038402… ## $ ptcld_extracted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ ptcld_predicted_dbh_cm &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ basal_area_m2 &lt;dbl&gt; 0.001242235, 0.001352049, 0.001071284, 0.001… ## $ basal_area_ft2 &lt;dbl&gt; 0.01337142, 0.01455345, 0.01153130, 0.012467… ## $ radius_m &lt;dbl&gt; 0.01988506, 0.02074537, 0.01846619, 0.019201… ## $ tree_cbh_m &lt;dbl&gt; 1.983395, 2.145153, 1.863469, 1.946494, 2.79… ## $ is_training_cbh &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ max_crown_diam_height_m &lt;dbl&gt; 1.504257, 1.568958, 0.880000, 1.520000, 1.74… ## $ is_training_hmd &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE… ## $ forest_type_group_code &lt;dbl&gt; 180, 180, 180, 180, 180, 180, 180, 180, 180,… ## $ forest_type_group &lt;chr&gt; &quot;Pinyon / juniper group&quot;, &quot;Pinyon / juniper … ## $ hardwood_softwood &lt;chr&gt; &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwoo… ## $ tree_x &lt;dbl&gt; 467000.1, 467000.1, 467000.1, 467000.1, 4670… ## $ tree_y &lt;dbl&gt; 3808063, 3808065, 3808067, 3808074, 3808091,… ## $ cruz_stand_id &lt;dbl&gt; 71791, 71791, 71791, 71791, 71540, 71289, 70… ## $ cruz_tree_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_stand_kg_per_m3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ cruz_crown_biomass_kg &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ landfire_stand_id &lt;dbl&gt; 56399, 56399, 56399, 56158, 56158, 55917, 55… ## $ crown_dia_m &lt;dbl&gt; 0.6909883, 0.9772050, 0.5641896, 0.6909883, … ## $ crown_length_m &lt;dbl&gt; 0.1666052, 0.1948469, 0.1565314, 0.1635056, … ## $ crown_volume_m3 &lt;dbl&gt; 0.04165130, 0.09742345, 0.02608857, 0.040876… ## $ landfire_tree_kg_per_m3 &lt;dbl&gt; 1.3947342, 1.3947342, 1.3947342, 0.1140463, … ## $ landfire_stand_kg_per_m3 &lt;dbl&gt; 0.06, 0.06, 0.06, 0.27, 0.27, 0.09, 0.08, 0.… ## $ landfire_crown_biomass_kg &lt;dbl&gt; 0.058092493, 0.135879822, 0.036386614, 0.004… ## $ geom &lt;POINT [m]&gt; POINT (467000.1 3808063), POINT (46700… load in the crown spatial files and attach the new component data crowns_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_crowns.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() # the trees are the same, the data in the columns are different identical(nrow(crowns_sf), nrow(treetops_sf)) # attach the new columns crowns_sf &lt;- crowns_sf %&gt;% dplyr::select(treeID, tree_x, tree_y) %&gt;% dplyr::inner_join( treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(-c(tree_x, tree_y)) , by = &quot;treeID&quot; ) use the internal function from cloud2trees that writes the polygon and point vector data # this is an internal function from cloud2trees write_raster2trees_ans &lt;- function(raster2trees_ans, dir) { ### write the data to the disk if(nrow(raster2trees_ans)&gt;250e3){ # split up the detected crowns raster2trees_ans &lt;- raster2trees_ans %&gt;% dplyr::arrange(as.numeric(tree_x),as.numeric(tree_y)) %&gt;% # groups of 250k dplyr::mutate(grp = ceiling(dplyr::row_number()/250e3)) write_fnl_temp &lt;- raster2trees_ans$grp %&gt;% unique() %&gt;% purrr::map(function(x){ # dsn&#39;s cf &lt;- file.path( dir, paste0(&quot;final_detected_crowns_&quot;,x,&quot;.gpkg&quot;) ) tf &lt;- file.path( dir, paste0(&quot;final_detected_tree_tops_&quot;,x,&quot;.gpkg&quot;) ) ### write the data to the disk # crown vector polygons sf::st_write( raster2trees_ans %&gt;% dplyr::filter(grp == x) %&gt;% dplyr::select(-c(grp)) , dsn = cf , append = FALSE , quiet = TRUE ) # tree top vector points sf::st_write( # get tree points raster2trees_ans %&gt;% dplyr::filter(grp == x) %&gt;% dplyr::select(-c(grp)) %&gt;% sf::st_drop_geometry() %&gt;% sf::st_as_sf(coords = c(&quot;tree_x&quot;, &quot;tree_y&quot;), crs = sf::st_crs(raster2trees_ans)) , dsn = tf , append = FALSE , quiet = TRUE ) return( dplyr::tibble( crowns_file = cf , trees_file = tf ) ) }) %&gt;% dplyr::bind_rows() }else{ # dsn&#39;s cf &lt;- file.path( dir, &quot;final_detected_crowns.gpkg&quot; ) tf &lt;- file.path( dir, &quot;final_detected_tree_tops.gpkg&quot; ) # crown vector polygons sf::st_write( raster2trees_ans , dsn = cf , append = FALSE , quiet = TRUE ) # tree top vector points sf::st_write( # get tree points raster2trees_ans %&gt;% sf::st_drop_geometry() %&gt;% sf::st_as_sf(coords = c(&quot;tree_x&quot;, &quot;tree_y&quot;), crs = sf::st_crs(raster2trees_ans)) , dsn = tf , append = FALSE , quiet = TRUE ) # df write_fnl_temp &lt;- dplyr::tibble( crowns_file = cf , trees_file = tf ) } return(write_fnl_temp) } write the final data (this will overwrite our original data from cloud2trees() but that’s ok because the tree list and spatial information is the same) write_raster2trees_ans(crowns_sf, dir = &quot;../data/point_cloud_processing_delivery&quot;) "],["forest-stand-summary.html", "Section 3 Forest Stand Summary 3.1 Load data 3.2 Silvicultural metrics 3.3 Spatial Forest Structure 3.4 Canopy Cover 3.5 Write Data", " Section 3 Forest Stand Summary In this section we’ll process the point cloud-extracted tree list data given stand boundaries that fall within the extent of the original point cloud data. When processing point cloud data with the objective for summarizing data within a forest stand, it is imperative to ensure that the point cloud extent completely covers and extends beyond the stand extent to avoid edge effects and tree artifacts. create a directory for saving our analysis output # make a dir for saving this stand-level data to outdir &lt;- &quot;../data/mogollon_rim_fire_unit_trees&quot; if(!dir.exists(outdir)){dir.create(outdir, showWarnings = F)} 3.1 Load data first, we’ll load our stand data and the boundary data of the point cloud that we processed # las data bounds las_ctg_sf &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;, quiet = T) # get our proj crs proj_crs &lt;- sf::st_crs(las_ctg_sf) # stands stand_sf &lt;- sf::st_read(&quot;../data/QUIK-Fire_Boundary/QUIK-Fire_Boundary.shp&quot;, quiet = T) %&gt;% sf::st_transform(proj_crs) %&gt;% dplyr::rename_with(~tolower(stringr::str_replace_all(.x,&quot;\\\\.&quot;, &quot;_&quot;))) %&gt;% dplyr::rename(unit_id = id) %&gt;% dplyr::mutate( stand_area_m2 = sf::st_area(.) %&gt;% as.numeric() , stand_area_ha = stand_area_m2/10000 ) # set our colors for the units if(nrow(stand_sf)&lt;length(harrypotter::hp_palettes$hermionegranger)){ my_pal &lt;- harrypotter::hp(n=nrow(stand_sf), option = &quot;hermionegranger&quot;) }else{ my_pal &lt;- viridis::turbo(n=nrow(stand_sf)) } what is the stand data? stand_sf %&gt;% dplyr::glimpse() ## Rows: 2 ## Columns: 7 ## $ unit_id &lt;dbl&gt; 1, 2 ## $ unit_name &lt;chr&gt; &quot;300 RD to Rim&quot;, &quot;Middle Kehl Canyon&quot; ## $ hectares &lt;dbl&gt; 119.658, 200.666 ## $ acres &lt;dbl&gt; 295.555, 495.645 ## $ geometry &lt;POLYGON [m]&gt; POLYGON ((470742 3810364, 4..., POLYGON ((469974.3 38112… ## $ stand_area_m2 &lt;dbl&gt; 1195649, 2005096 ## $ stand_area_ha &lt;dbl&gt; 119.5649, 200.5096 let’s look at these bounds on a map mapview::mapview( las_ctg_sf , layer.name = &quot;point cloud tile&quot; , color = &quot;black&quot; , lwd = 1 , alpha.regions = 0 , label = FALSE , legend = FALSE , popup = FALSE ) + mapview::mapview( stand_sf %&gt;% dplyr::select(unit_name) , zcol = &quot;unit_name&quot; , col.regions = my_pal , layer.name = &quot;stand bounds&quot; , alpha.regions = 0.8 ) we processed point cloud data well outside these stand bounds (it is not necessary to process data this far outside of our stands, a one tile buffer in this situation would have sufficed) load in the tree top points data from cloud2trees::cloud2trees() # get the data from already run treetops_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_tree_tops.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() we’ll keep only trees where the tree top point falls within one of our stands and then use this tree list to filter our crown data so that we get the full crown polygon even if it extends outside of the stand boundary treetops_sf &lt;- treetops_sf %&gt;% sf::st_intersection(stand_sf %&gt;% dplyr::select(unit_id,unit_name,tidyselect::starts_with(&quot;stand_area_&quot;))) The tree top point data is easier to work with for analysis since it takes less memory to load into our session and has the exact same structure as the crown polygons. At the end, we’ll bring in our crown polygon data, filter it with this tree list, and save it for sharing. load in the DTM, aggregate to 2 m resolution, and write it dtm_rast &lt;- terra::rast(&quot;../data/point_cloud_processing_delivery/dtm_1m.tif&quot;) # quick plot terra::plot(dtm_rast, axes = F, main = &quot;DTM (m)&quot;) terra::plot(terra::vect(stand_sf), add = T, col = NA, border = &quot;black&quot;, lwd = 2) aggregate to 2 m res_temp &lt;- terra::res(dtm_rast)[1] des_res_temp &lt;- 2 if(res_temp&lt;des_res_temp){ dtm_rast &lt;- terra::aggregate( dtm_rast , fact = round(des_res_temp/res_temp) , fun = &quot;mean&quot; , na.rm = T , cores = lasR::half_cores() , filename = file.path(outdir, paste0(&quot;dtm_&quot;,des_res_temp, &quot;m.tif&quot;)) , overwrite = T ) } # what is the res? terra::res(dtm_rast) we can also load in the CHM raster and look at that quickly chm_rast &lt;- terra::rast(&quot;../data/point_cloud_processing_delivery/chm_0.25m.tif&quot;) # quick plot chm_rast %&gt;% terra::aggregate( fact = 1/terra::res(chm_rast)[1] , fun = &quot;mean&quot; , na.rm = T , cores = lasR::half_cores() ) %&gt;% terra::plot( col = viridis::plasma(100) , axes = F, alpha = 0.8 , main = &quot;CHM (m)&quot; ) terra::plot(terra::vect(stand_sf), add = T, col = NA, border = &quot;black&quot;, lwd = 2) 3.2 Silvicultural metrics Let’s look at some common stand-level forestry metrics ### stand-level summaries silv_metrics &lt;- treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% # dplyr::filter(dbh_cm &gt;= ostory_dbh_cm) %&gt;% dplyr::ungroup() %&gt;% dplyr::group_by(unit_id, unit_name, stand_area_ha) %&gt;% dplyr::summarise( n_trees = dplyr::n_distinct(treeID) , mean_dbh_cm = mean(dbh_cm, na.rm = T) , mean_tree_height_m = mean(tree_height_m, na.rm = T) , loreys_height_m = sum(basal_area_m2*tree_height_m, na.rm = T) / sum(basal_area_m2, na.rm = T) , basal_area_m2 = sum(basal_area_m2, na.rm = T) , sum_dbh_cm_sq = sum(dbh_cm^2, na.rm = T) , landfire_crown_biomass_kg = sum(landfire_crown_biomass_kg, na.rm = T) , cruz_crown_biomass_kg = sum(cruz_crown_biomass_kg, na.rm = T) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( trees_per_ha = (n_trees/stand_area_ha) , basal_area_m2_per_ha = (basal_area_m2/stand_area_ha) , qmd_cm = sqrt(sum_dbh_cm_sq/n_trees) , landfire_cfl_kg_m2 = landfire_crown_biomass_kg/(stand_area_ha*10000) , cruz_cfl_kg_m2 = cruz_crown_biomass_kg/(stand_area_ha*10000) ) %&gt;% dplyr::select(-c(sum_dbh_cm_sq,landfire_crown_biomass_kg,cruz_crown_biomass_kg)) ### export tabular write.csv( silv_metrics , file.path(outdir, &quot;stand_silv_metrics.csv&quot;) , row.names = F , append = F ) stand-level silvicultural summary silv_metrics %&gt;% dplyr::select( unit_name , stand_area_ha , n_trees , mean_dbh_cm , qmd_cm , mean_tree_height_m , loreys_height_m , trees_per_ha , basal_area_m2_per_ha , landfire_cfl_kg_m2 , cruz_cfl_kg_m2 ) %&gt;% dplyr::mutate( dplyr::across( .cols = c(stand_area_ha, n_trees) , .fns = ~ scales::comma(.x, accuracy = 1) ) ) %&gt;% kableExtra::kbl( digits = 1 , escape = F , caption = &quot;Silvicultral metrics by stand unit&quot; , col.names = c( &quot;Unit Name&quot; , &quot;area (ha)&quot; , &quot;trees&quot; , &quot;mean&lt;br&gt;DBH (cm)&quot; , &quot;QMD (cm)&quot; , &quot;mean&lt;br&gt;Ht. (m)&quot; , &quot;Loreys&lt;br&gt;Ht. (m)&quot; , &quot;TPH&quot; , &quot;BA&lt;br&gt;m&lt;sup&gt;2&lt;/sup&gt; ha&lt;sup&gt;-1&lt;/sup&gt;&quot; , &quot;LANDFIRE CFL&lt;br&gt;kg m&lt;sup&gt;-2&lt;/sup&gt;&quot; , &quot;Cruz CFL&lt;br&gt;kg m&lt;sup&gt;-2&lt;/sup&gt;&quot; ) ) %&gt;% kableExtra::kable_styling(font_size = 12) Table 3.1: Silvicultral metrics by stand unit Unit Name area (ha) trees meanDBH (cm) QMD (cm) meanHt. (m) LoreysHt. (m) TPH BAm2 ha-1 LANDFIRE CFLkg m-2 Cruz CFLkg m-2 300 RD to Rim 120 47,940 24.2 31.9 12.0 23.3 401.0 32.0 0.8 1.6 Middle Kehl Canyon 201 107,684 20.7 28.1 10.6 22.4 537.1 33.3 0.8 1.6 3.2.1 Height Distribution # there are always tree heights treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = tree_height_m, color = unit_name, fill = unit_name)) + ggplot2::geom_density(alpha = 0.7) + ggplot2::facet_grid(cols = dplyr::vars(unit_name)) + ggplot2::scale_color_manual(values = my_pal) + ggplot2::scale_fill_manual(values = my_pal) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;&quot;, subtitle = &quot;Distribution of tree height by stand&quot;) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.y = ggplot2::element_blank() , axis.ticks.y = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) 3.2.2 DBH Distribution treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = dbh_cm, color = unit_name, fill = unit_name)) + ggplot2::geom_density(alpha = 0.7) + ggplot2::facet_grid(cols = dplyr::vars(unit_name)) + ggplot2::scale_color_manual(values = my_pal) + ggplot2::scale_fill_manual(values = my_pal) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs(x = &quot;tree DBH (cm)&quot;, y = &quot;&quot;, subtitle = &quot;Distribution of tree DBH by stand&quot;) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.y = ggplot2::element_blank() , axis.ticks.y = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) 3.2.3 Forest Type The cloud2trees::trees_type() function was used in our point cloud processing pipeline to attach FIA Forest Type Group Code information using the Forest Type Groups of the Continental United States data (Wilson 2023). let’s check out the distribution of forest type groups overall across units treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(forest_type_group) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::mutate( pct = scales::percent(n/sum(n), accuracy = 0.1) ) %&gt;% dplyr::mutate(n = scales::comma(n,accuracy=1)) %&gt;% kableExtra::kbl( caption = &quot;Count of trees by FIA Forest Type Group overall&quot; , digits = 2 , col.names = c( &quot;&quot; , &quot;# trees&quot; , &quot;% trees&quot; ) ) %&gt;% kableExtra::kable_styling() Table 3.2: Count of trees by FIA Forest Type Group overall # trees % trees Ponderosa pine group 153,061 98.4% Pinyon / juniper group 779 0.5% Woodland hardwoods group 767 0.5% Douglas-fir group 589 0.4% Fir / spruce / mountain hemlock group 413 0.3% Aspen / birch group 15 0.0% let’s check out the distribution of forest type groups for each unit treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(unit_name, forest_type_group) %&gt;% dplyr::arrange(unit_name, desc(n)) %&gt;% dplyr::group_by(unit_name) %&gt;% dplyr::mutate( pct = scales::percent(n/sum(n), accuracy = 0.1) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(n = scales::comma(n,accuracy=1)) %&gt;% kableExtra::kbl( caption = &quot;Count of trees by FIA Forest Type Group by stand&quot; , digits = 2 , col.names = c( &quot;.&quot; , &quot;&quot; , &quot;# trees&quot; , &quot;% trees&quot; ) ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::collapse_rows(columns = 1, valign = &quot;top&quot;) Table 3.3: Count of trees by FIA Forest Type Group by stand . # trees % trees 300 RD to Rim Ponderosa pine group 46,637 97.3% Pinyon / juniper group 673 1.4% Woodland hardwoods group 521 1.1% Douglas-fir group 67 0.1% Fir / spruce / mountain hemlock group 42 0.1% Middle Kehl Canyon Ponderosa pine group 106,424 98.8% Douglas-fir group 522 0.5% Fir / spruce / mountain hemlock group 371 0.3% Woodland hardwoods group 246 0.2% Pinyon / juniper group 106 0.1% Aspen / birch group 15 0.0% plot it with a custom palette to ensure the colors are the same for forest type across unit plots # It&#39;s recommended to use a named vector lab_list_temp &lt;- treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(!is.na(forest_type_group)) %&gt;% dplyr::distinct(forest_type_group) %&gt;% dplyr::mutate(forest_type_group = stringr::str_remove(forest_type_group, &quot; group&quot;)) %&gt;% dplyr::pull(forest_type_group) # set our colors for the units if(length(lab_list_temp)&lt;=length(harrypotter::hp_palettes$lunalovegood)){ col_list_temp &lt;- harrypotter::hp(n=length(lab_list_temp), option = &quot;lunalovegood&quot;) }else{ col_list_temp &lt;- viridis::turbo(n=length(lab_list_temp)) } # palette ftype_pal &lt;- setNames(col_list_temp,lab_list_temp) plot treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::mutate(forest_type_group = stringr::str_remove(forest_type_group, &quot; group&quot;)) %&gt;% dplyr::count(unit_name, forest_type_group) %&gt;% dplyr::arrange(unit_name, desc(n)) %&gt;% dplyr::group_by(unit_name) %&gt;% dplyr::mutate( pct = n/sum(n) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(forest_type_group = forcats::fct_reorder(forest_type_group, pct)) %&gt;% ggplot2::ggplot( mapping = ggplot2::aes( x = pct, y = forest_type_group , fill = forest_type_group , label = paste0( scales::percent(pct, accuracy = 0.1) # , &quot;\\n&quot; # , scales::comma(n, accuracy = 0.1) ) ) ) + ggplot2::geom_col(width = 0.7) + ggplot2::geom_text(color = &quot;black&quot;, size = 3, hjust = -0.2) + ggplot2::scale_fill_manual(values = ftype_pal) + ggplot2::scale_x_continuous( labels = scales::percent_format(accuracy = 1) ,expand = expansion(mult = c(0, .08)) ) + ggplot2::facet_grid(rows = dplyr::vars(unit_name), scales = &quot;free_y&quot;) + ggplot2::labs( x = &quot;% trees&quot; , y = &quot;&quot; # &quot;Forest Type Group&quot; ) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.x = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) 3.2.4 Crown Biomass The cloud2trees::trees_biomass() function was used in our point cloud processing pipeline to estimate tree crown biomass in kilograms. We estimated biomass based on: 1) LANDFIRE’s Forest Canopy Bulk Density (CBD) data; and 2) based on the Cruz et al. (2003) equations and the FIA forest type group we got above the fire science community is likely more familiar with crown bulk density (CBD) values which represent the mass of flammable material per unit volume of the tree crown and are typically expressed in units of mass per unit volume (e.g., kilograms per cubic meter). We’ll look at distributions of CBD. 3.2.4.1 Cruz CBD treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = cruz_tree_kg_per_m3, color = unit_name, fill = unit_name)) + ggplot2::geom_density(alpha = 0.7) + ggplot2::facet_grid(cols = dplyr::vars(unit_name)) + ggplot2::scale_color_manual(values = my_pal) + ggplot2::scale_fill_manual(values = my_pal) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs( x = latex2exp::TeX(&quot;CBD ($kg \\\\cdot m^{-3}$)&quot;) , y = &quot;&quot; , subtitle = &quot;Cruz estimated tree Crown Bulk Density by stand&quot; ) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.y = ggplot2::element_blank() , axis.ticks.y = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) 3.2.4.2 LANDFIRE CBD treetops_sf %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = landfire_tree_kg_per_m3, color = unit_name, fill = unit_name)) + ggplot2::geom_density(alpha = 0.7) + ggplot2::facet_grid(cols = dplyr::vars(unit_name)) + ggplot2::scale_color_manual(values = my_pal) + ggplot2::scale_fill_manual(values = my_pal) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(11)) + ggplot2::labs( x = latex2exp::TeX(&quot;CBD ($kg \\\\cdot m^{-3}$)&quot;) , y = &quot;&quot; , subtitle = &quot;LANDFIRE estimated tree Crown Bulk Density by stand&quot; ) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.y = ggplot2::element_blank() , axis.ticks.y = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) 3.2.4.3 Missing Cruz Estimates The Cruz et al. (2003) study developed models to predict canopy fuel stratum at the stand level for four coniferous forest types common in the western US: Douglas-fir, ponderosa pine, lodgepole pine, and mixed conifer. Models for other forests types are currently lacking which limits the scope of this methodology. If the tree list has trees that are in a FIA forest type group not represented in the list above, then we will fill the values with LANDFIRE estimates which have complete coverage at the CONUS level How many trees overall across the two units have missing crown fuel load estimates? treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::mutate( missing_cruz = ifelse(is.na(cruz_crown_biomass_kg), &quot;missing Cruz&quot;, &quot;not missing Cruz&quot;) ) %&gt;% dplyr::count(missing_cruz) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::mutate( pct = scales::percent(n/sum(n), accuracy = 0.1) ) %&gt;% dplyr::mutate(n = scales::comma(n,accuracy=1)) %&gt;% kableExtra::kbl( caption = &quot;Count of trees missing Cruz fuel load estimate overall&quot; , digits = 2 , col.names = c( &quot;&quot; , &quot;# trees&quot; , &quot;% trees&quot; ) ) %&gt;% kableExtra::kable_styling() Table 3.4: Count of trees missing Cruz fuel load estimate overall # trees % trees not missing Cruz 154,063 99.0% missing Cruz 1,561 1.0% How many trees within each unit have missing crown fuel load estimates? treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::mutate( missing_cruz = ifelse(is.na(cruz_crown_biomass_kg), &quot;missing Cruz&quot;, &quot;not missing Cruz&quot;) ) %&gt;% dplyr::count(unit_name, missing_cruz) %&gt;% dplyr::arrange(unit_name, desc(n)) %&gt;% dplyr::group_by(unit_name) %&gt;% dplyr::mutate( pct = scales::percent(n/sum(n), accuracy = 0.1) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(n = scales::comma(n,accuracy=1)) %&gt;% kableExtra::kbl( caption = &quot;Count of trees missing Cruz fuel load estimate by stand&quot; , digits = 2 , col.names = c( &quot;.&quot; , &quot;&quot; , &quot;# trees&quot; , &quot;% trees&quot; ) ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::collapse_rows(columns = 1, valign = &quot;top&quot;) Table 3.5: Count of trees missing Cruz fuel load estimate by stand . # trees % trees 300 RD to Rim not missing Cruz 46,746 97.5% missing Cruz 1,194 2.5% Middle Kehl Canyon not missing Cruz 107,317 99.7% missing Cruz 367 0.3% let’s fill in these estimates with the LANDFIRE value treetops_sf &lt;- treetops_sf %&gt;% dplyr::mutate( is_missing_cruz = is.na(cruz_crown_biomass_kg) , cruz_tree_kg_per_m3 = dplyr::coalesce(cruz_tree_kg_per_m3, landfire_tree_kg_per_m3) , cruz_stand_kg_per_m3 = dplyr::coalesce(cruz_stand_kg_per_m3, landfire_stand_kg_per_m3) , cruz_crown_biomass_kg = dplyr::coalesce(cruz_crown_biomass_kg, landfire_crown_biomass_kg) ) 3.2.5 Summary Statistics let’s get summary statistics for selected metrics by stand unit # table_temp = treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::group_by(unit_id, unit_name) %&gt;% dplyr::summarise( dplyr::across( c(tree_height_m, dbh_cm, tree_cbh_m, cruz_tree_kg_per_m3, landfire_tree_kg_per_m3) , .fns = list(mean = mean, median = median, sd = sd, min = min, max = max) ) , n = dplyr::n() ) %&gt;% dplyr::ungroup() %&gt;% tidyr::pivot_longer(cols = -c(unit_id, unit_name,n)) %&gt;% dplyr::mutate( agg = stringr::word(name,-1,sep = &quot;_&quot;) , metric = stringr::str_remove_all(name, paste0(&quot;_&quot;,agg)) ) %&gt;% dplyr::select(-name) %&gt;% dplyr::mutate( value = dplyr::case_when( metric == &quot;tree_height_m&quot; ~ scales::comma(value,accuracy=0.1) , metric == &quot;dbh_cm&quot; ~ scales::comma(value,accuracy=0.1) , metric == &quot;tree_cbh_m&quot; ~ scales::comma(value,accuracy=0.1) , metric == &quot;cruz_tree_kg_per_m3&quot; ~ scales::comma(value,accuracy=0.001) , metric == &quot;landfire_tree_kg_per_m3&quot; ~ scales::comma(value,accuracy=0.001) , T ~ scales::comma(value,accuracy=0.1) ) ) %&gt;% tidyr::pivot_wider(names_from = agg, values_from = value) %&gt;% dplyr::mutate( unit_lab = paste0( unit_name ,&quot;&lt;br&gt;(&quot; , scales::comma(n,accuracy=1) ,&quot; trees)&quot; ) , range = paste0(min, &quot;—&quot;, max) ) %&gt;% dplyr::arrange(unit_id, unit_name, desc(n)) %&gt;% dplyr::select(-c(unit_id,unit_name,n,min,max)) %&gt;% dplyr::relocate(unit_lab) %&gt;% dplyr::mutate( metric = factor( metric , ordered = T , levels = c( &quot;tree_height_m&quot; , &quot;tree_cbh_m&quot; , &quot;dbh_cm&quot; , &quot;cruz_tree_kg_per_m3&quot; , &quot;landfire_tree_kg_per_m3&quot; ) , labels = c( &quot;Height (m)&quot; , &quot;Crown Base Ht. (m)&quot; , &quot;DBH (cm)&quot; , &quot;Cruz CBD&lt;br&gt;kg m&lt;sup&gt;-3&lt;/sup&gt;&quot; , &quot;LANDFIRE CBD&lt;br&gt;kg m&lt;sup&gt;-3&lt;/sup&gt;&quot; ) ) ) %&gt;% kableExtra::kbl( caption = &quot;Summary statistics for selected metrics by stand&quot; , col.names = c( &quot;Unit Name&quot;, &quot;Metric&quot; , &quot;Mean&quot;, &quot;Median&quot; , &quot;Std Dev&quot;, &quot;Range&quot; ) , escape = F # , digits = 2 ) %&gt;% kableExtra::kable_styling(font_size = 13) %&gt;% kableExtra::collapse_rows(columns = 1, valign = &quot;top&quot;) Table 3.6: Summary statistics for selected metrics by stand Unit Name Metric Mean Median Std Dev Range 300 RD to Rim(47,940 trees) Height (m) 12.0 9.8 8.9 2.0—43.0 DBH (cm) 24.2 16.3 20.8 3.7—113.1 Crown Base Ht. (m) 4.2 3.8 2.0 1.5—23.5 Cruz CBDkg m-3 0.255 0.240 0.124 0.009—1.894 LANDFIRE CBDkg m-3 0.126 0.109 0.123 0.009—1.868 Middle Kehl Canyon(107,684 trees) Height (m) 10.6 7.1 8.2 2.0—44.9 DBH (cm) 20.7 11.2 19.0 3.7—118.2 Crown Base Ht. (m) 4.2 3.6 2.0 1.5—23.5 Cruz CBDkg m-3 0.237 0.232 0.064 0.059—1.981 LANDFIRE CBDkg m-3 0.103 0.098 0.057 0.019—1.981 3.3 Spatial Forest Structure Because we have a spatial tree list, we can look at the spatial arrangement of forest structural metrics first, let’s make a function to crop the raster to a stand and plot the raster and stand together # function to crop and plot a raster data for a stand plot_raster_stand_fn &lt;- function( rast , stand , buffer=5 , des_res=1 , agg_fun = &quot;mean&quot; , stand_color = &quot;black&quot; , my_title = &quot;&quot; , scale_name = &quot;&quot; , bbox = F , lookup = NA , lookup_code = NA , lookup_lab = NA ) { # crop the raster crop_rast &lt;- rast %&gt;% terra::crop( stand %&gt;% sf::st_bbox() %&gt;% sf::st_as_sfc() %&gt;% sf::st_buffer(buffer) %&gt;% terra::vect() %&gt;% terra::project(terra::crs(rast)) ) if(!bbox){ crop_rast &lt;- crop_rast %&gt;% terra::mask( stand %&gt;% sf::st_buffer(buffer) %&gt;% terra::vect() %&gt;% terra::project(terra::crs(rast)) ) } # aggregate the raster res_temp &lt;- terra::res(rast)[1] if(res_temp&lt;des_res){ crop_rast &lt;- terra::aggregate( crop_rast , fact = round(des_res/res_temp) , fun = agg_fun , na.rm = T , cores = lasR::half_cores() ) } # plot it if(inherits(lookup,&quot;data.frame&quot;) &amp;&amp; !is.na(lookup_code) &amp;&amp; !is.na(lookup_lab)){ # create col to join lookup &lt;- lookup %&gt;% dplyr::mutate(dplyr::across( dplyr::all_of(lookup_code[1]) # only one allowed , .fns = list(cccc = as.character, nnnn = as.numeric) )) %&gt;% dplyr::rename_with( .fn = function(x){&quot;f&quot;} , .cols = tidyselect::ends_with(&quot;_cccc&quot;) ) %&gt;% dplyr::rename_with( .fn = function(x){&quot;f_lab&quot;} , .cols = tidyselect::ends_with(lookup_lab[1]) ) # rast to df and plot crop_rast %&gt;% terra::as.data.frame(xy=T) %&gt;% dplyr::rename(f=3) %&gt;% dplyr::mutate(f = as.character(f)) %&gt;% dplyr::left_join(lookup, by = &quot;f&quot;) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile( mapping = ggplot2::aes(x=x, y=y, fill = f_lab) ) + ggplot2::geom_sf( data = stand %&gt;% sf::st_transform(terra::crs(rast)) , color = stand_color, fill = NA, lwd = 1.5 ) + ggplot2::labs(title = my_title, fill = scale_name) + ggplot2::theme_void() + ggplot2::theme( legend.title = ggplot2::element_text(size=7) , plot.title = ggplot2::element_text(size = 10) , plot.subtitle = ggplot2::element_text(size = 9) ) }else{ crop_rast %&gt;% terra::as.data.frame(xy=T) %&gt;% dplyr::rename(f=3) %&gt;% ggplot2::ggplot() + ggplot2::geom_tile( mapping = ggplot2::aes(x=x, y=y, fill = f) ) + ggplot2::geom_sf( data = stand %&gt;% sf::st_transform(terra::crs(rast)) , color = stand_color, fill = NA, lwd = 1.5 ) + ggplot2::labs(title = my_title, fill = scale_name) + ggplot2::theme_void() + ggplot2::theme( legend.title = ggplot2::element_text(size=7) , plot.title = ggplot2::element_text(size = 10) , plot.subtitle = ggplot2::element_text(size = 9) ) } } # plot_raster_stand_fn( # dtm_rast # , stand = stand_sf[1,] # , buffer = 5 # , des_res = 1 # , stand_color = my_pal[1] # , my_title = &quot;DTM&quot; # , scale_name = &quot;DTM (m)&quot; # ) 3.3.1 CHM the canopy height model is a raster dataset that represents the height of objects above the ground, we set the minimum height at 2 m # get the plot for each stand plt_chm &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = max(terra::values(chm_rast), na.rm = T), ll = 0){ plot_raster_stand_fn( chm_rast , stand = stand_sf[x,] , buffer = 5 , des_res = 2 , stand_color = &quot;black&quot; , my_title = &quot;Canopy Height Model&quot; , scale_name = &quot;CHM (m)&quot; , agg_fun = &quot;max&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_viridis_c(option = &quot;plasma&quot;,limits = c(ll,ul)) }) check out our plots plt_chm ## [[1]] ## ## [[2]] 3.3.2 Function to Rasterize the CHM was easy to plot because it was already a raster dataset; what if we want to plot a raster of a metric based on our vector tree data? we need to define a function to rasterize our spatial tree list by aggregating a selected metric within a raster cell vect_to_rast_fn &lt;- function( vect , des_res = 1 , buffer = 5 , fun = &quot;mean&quot; # function(x){mean(x, na.rm=T)} , field = &quot;your_vector_attribute&quot; , zero_na = F ) { # sample sf object with a bounding box + buffer my_sf &lt;- vect %&gt;% sf::st_bbox() %&gt;% sf::st_as_sfc(crs = sf::st_crs(vect)) %&gt;% sf::st_buffer(buffer) # extract bbox coordinates bbox &lt;- sf::st_bbox(my_sf) # create a terra raster using the bbox my_raster &lt;- terra::rast( xmin = bbox[1], ymin = bbox[2], xmax = bbox[3], ymax = bbox[4] , resolution = des_res , crs = sf::st_crs(vect) ) # rasterize the vector data onto the grid, using a desired aggregation function (e.g., &quot;mean&quot;) rasterized_data &lt;- terra::rasterize( x = vect , y = my_raster , fun = fun , field = field ) if(zero_na){rasterized_data &lt;- terra::subst(rasterized_data,NA,0)} terra::crs(rasterized_data) &lt;- vect %&gt;% terra::vect() %&gt;% terra::crs() return(rasterized_data) } # vect_to_rast_fn( # treetops_sf %&gt;% dplyr::filter(unit_id==treetops_sf$unit_id[2]) # , des_res = 10 # , field = &quot;dbh_cm&quot; # , fun = &quot;mean&quot; #function(x){mean(x, na.rm=T)} # , zero_na = F # ) %&gt;% # # terra::plot() # terra::summary() we can combine our create a raster and plot it function # we can combine our create a raster and plot it function vect_to_rast_plot_fn &lt;- function( trees_vect , stand_vect , fun = &quot;mean&quot; # function(x){mean(x, na.rm=T)} , field = &quot;your_vector_attribute&quot; , zero_na = F , buffer = 5 , des_res = 10 , stand_color = &quot;black&quot; , my_title = &quot;&quot; , scale_name = &quot;&quot; ) { # get the raster rast &lt;- vect_to_rast_fn( vect = trees_vect , des_res = des_res , buffer = buffer , fun = fun , field = field , zero_na = zero_na ) # plot it plot_raster_stand_fn( rast = rast , stand = stand_vect , buffer = buffer , des_res = des_res , stand_color = stand_color , my_title = my_title , scale_name = scale_name ) } 3.3.3 Mean DBH take this for a spin to get a raster of mean DBH plotted with our stand boundary # get the plot for each stand plt_dbh &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = max(treetops_sf$dbh_cm,na.rm = T), ll = 0){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = &quot;mean&quot; , field = &quot;dbh_cm&quot; , zero_na = F , buffer = 5 , des_res = 30 , stand_color = &quot;black&quot; , my_title = &quot;Mean DBH&quot; , scale_name = &quot;DBH (cm)&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller(palette = &quot;Oranges&quot;, direction = 1, limits = c(ll,ul)) # ggplot2::scale_fill_viridis_c(option = &quot;mako&quot;, direction = -1, limits = c(ll,ul)) }) check out our plots plt_dbh ## [[1]] ## ## [[2]] 3.3.4 Mean Height now get a raster of mean tree height plotted with our stand boundary which will be similar to the CHM but aggregated as if at the inventory plot level # get the plot for each stand plt_height &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = max(treetops_sf$tree_height_m,na.rm = T), ll = 0){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = &quot;mean&quot; , field = &quot;tree_height_m&quot; , zero_na = F , buffer = 5 , des_res = 30 , stand_color = &quot;black&quot; , my_title = &quot;Mean Height&quot; , scale_name = &quot;Height (m)&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller(palette = &quot;YlOrBr&quot;, direction = 1, limits = c(ll,ul)) # ggplot2::scale_fill_viridis_c(option = &quot;mako&quot;, direction = -1, limits = c(ll,ul)) }) check out our plots plt_height ## [[1]] ## ## [[2]] 3.3.5 QMD to get the quadratic mean diameter (QMD) we are going to have to create a custom function that takes a list of DBH values and returns a single value QMD is a measure of the diameter of the tree of mean basal area: \\[ \\textrm{quadratic mean diameter (QMD)} = \\sqrt{\\frac{\\sum{d_{i}^{2}}}{n}} \\] , where \\(d_{i}\\) is the diameter at breast height of an individual tree, and \\(n\\) is the total number of trees. qmd_fn &lt;- function(x) { sqrt(sum(x^2, na.rm=T)/length(x[!is.na(x)])) } # get the plot for each stand plt_qmd &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = round(qmd_fn(max(treetops_sf$dbh_cm,na.rm = T))*.95), ll = 0){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = qmd_fn , field = &quot;dbh_cm&quot; , zero_na = F , buffer = 5 , des_res = 30 , stand_color = &quot;black&quot; , my_title = &quot;Quadratic Mean Diameter&quot; , scale_name = &quot;QMD (cm)&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller(palette = &quot;Blues&quot;, direction = 1, limits = c(ll,ul)) # ggplot2::scale_fill_viridis_c(option = &quot;cividis&quot;, begin = 0.55, limits = c(ll,ul)) }) check out our plots plt_qmd ## [[1]] ## ## [[2]] 3.3.6 Basal Area to get Basal Area (BA) in square meters per hectare we will also need to create a function that uses the area of the raster cell # get the plot for each stand # max(silv_metrics$basal_area_m2_per_ha)*2 plt_ba &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, my_res = 30, ul = NA, ll = NA){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = function(x){sum(x, na.rm=T)/((my_res^2)/10000)} , field = &quot;basal_area_m2&quot; , zero_na = F , buffer = 5 , des_res = my_res , stand_color = &quot;black&quot; , my_title = &quot;Basal Area&quot; , scale_name = latex2exp::TeX(&quot;BA ($m ^ 2 \\\\cdot ha^{-1}$)&quot;) ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller(palette = &quot;Purples&quot;, direction = 1, limits = c(ll,ul)) # harrypotter::scale_fill_hp(option = &quot;slytherin&quot;, limits = c(ll,ul)) }) check out our plots plt_ba ## [[1]] ## ## [[2]] 3.3.7 TPH to get trees per hectare (TPH) we will also need to create a function that uses the area of the raster cell # get the plot for each stand plt_tph &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, my_res = 30, ul = 2444, ll = NA){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = function(x){length(x)/((my_res^2)/10000)} , field = &quot;tree_height_m&quot; # just need something to count , zero_na = F , buffer = 5 , des_res = my_res , stand_color = &quot;black&quot; , my_title = &quot;Trees per Hectare&quot; , scale_name = &quot;TPH&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller(palette = &quot;Greys&quot;, direction = 1, limits = c(ll,ul)) # harrypotter::scale_fill_hp(option = &quot;mischief&quot;, limits = c(ll,ul)) }) check out our plots plt_tph ## [[1]] ## ## [[2]] 3.3.8 CFL Cruz to get canopy fuel load (CFL) we will also need to create a function that uses the area of the raster cell CFL is the total amount of flammable material (like leaves, needles, branches) within the tree crowns in a given area and expressed in units of mass per unit area (e.g., kilograms per square meter). First, we’ll look at CFL based on the Cruz et al. (2003) equations and the FIA forest type group # get the plot for each stand plt_cfl_cruz &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, my_res = 30, ul = 6.2, ll = 0){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = function(x){sum(x,na.rm = T)/(my_res^2)} , field = &quot;cruz_crown_biomass_kg&quot; , zero_na = F , buffer = 5 , des_res = my_res , stand_color = &quot;black&quot; , my_title = &quot;Cruz estimated Canopy Fuel Load&quot; , scale_name = latex2exp::TeX(&quot;Cruz CFL ($kg \\\\cdot m^{-2}$)&quot;) ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller(palette = &quot;Reds&quot;, direction = 1, limits = c(ll,ul)) # ggplot2::scale_fill_viridis_c(option = &quot;rocket&quot;, begin = 0.5, direction = -1, limits = c(ll,ul)) }) check out our plots plt_cfl_cruz ## [[1]] ## ## [[2]] 3.3.9 CFL LANDFIRE to get canopy fuel load (CFL) we will also need to create a function that uses the area of the raster cell CFL is the total amount of flammable material (like leaves, needles, branches) within the tree crowns in a given area and expressed in units of mass per unit area (e.g., kilograms per square meter). First, we’ll look at CFL based on the Cruz et al. (2003) equations and the FIA forest type group # get the plot for each stand plt_cfl_landfire &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, my_res = 30, ul = 6.2, ll = 0){ vect_to_rast_plot_fn( trees_vect = treetops_sf %&gt;% dplyr::filter(unit_id == stand_sf[x,]$unit_id) , stand_vect = stand_sf[x,] , fun = function(x){sum(x,na.rm = T)/(my_res^2)} , field = &quot;landfire_crown_biomass_kg&quot; , zero_na = F , buffer = 5 , des_res = my_res , stand_color = &quot;black&quot; , my_title = &quot;LANDFIRE estimated Canopy Fuel Load&quot; , scale_name = latex2exp::TeX(&quot;LANDFIRE CFL ($kg \\\\cdot m^{-2}$)&quot;) ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller(palette = &quot;Reds&quot;, direction = 1, limits = c(ll,ul)) }) check out our plots plt_cfl_landfire ## [[1]] ## ## [[2]] 3.3.10 FIA Forest Type Group We’ll load the FIA Forest Type Group raster data written to the output directory during the processing as well as the lookup table that gets downloaded during cloud2trees setup (cloud2trees::get_data()) # load raster foresttype_rast &lt;- terra::rast(&quot;../data/point_cloud_processing_delivery/type_rast.tif&quot;) # load lookup foresttype_lookup &lt;- file.path(cloud2trees::find_ext_data()[[&quot;foresttype_dir&quot;]], &quot;foresttype_lookup.csv&quot;) %&gt;% readr::read_csv(progress = F, show_col_types = F) %&gt;% dplyr::distinct(forest_type_group_code, forest_type_group, hardwood_softwood) %&gt;% dplyr::mutate(forest_type_group = stringr::str_remove(forest_type_group, &quot; group&quot;)) # what? foresttype_lookup %&gt;% dplyr::glimpse() ## Rows: 35 ## Columns: 3 ## $ forest_type_group_code &lt;dbl&gt; 100, 120, 140, 150, 160, 170, 180, 200, 220, 24… ## $ forest_type_group &lt;chr&gt; &quot;White / red / jack pine&quot;, &quot;Spruce / fir&quot;, &quot;Lon… ## $ hardwood_softwood &lt;chr&gt; &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;, &quot;Softwood&quot;,… plot with our plot_raster_stand_fn() function # get the plot for each stand plt_ftype &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function( x , my_lookup = foresttype_lookup , my_code = &quot;forest_type_group_code&quot; , my_lab = &quot;forest_type_group&quot; ){ plot_raster_stand_fn( foresttype_rast , stand = stand_sf[x,] , buffer = 10 , des_res = 30 , stand_color = &quot;black&quot; , my_title = &quot;FIA Forest Type Group&quot; , scale_name = &quot;&quot; , agg_fun = &quot;modal&quot; , lookup = my_lookup , lookup_code = my_code , lookup_lab = my_lab ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_manual(values = ftype_pal) }) check out our plots plt_ftype ## [[1]] ## ## [[2]] 3.3.11 Combine by stand unit put all desired plots into a list to combine by stand unit plt_list &lt;- list(plt_ftype, plt_ba, plt_qmd, plt_height, plt_tph, plt_cfl_landfire) combine the plots by stand unit plt_stands &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x,my_stand_sf=stand_sf,my_plt_list=plt_list){ # get just this stand plots plts &lt;- my_plt_list %&gt;% purrr::transpose() %&gt;% purrr::pluck(x) # patchwork them patchwork::wrap_plots(plts) + patchwork::plot_annotation( title = paste(&quot;Unit:&quot;,my_stand_sf[x,]$unit_name) , theme = ggplot2::theme(plot.title = ggplot2::element_text(size = 18, hjust = 0.5)) ) &amp; ggplot2::theme( plot.subtitle = ggplot2::element_blank() , legend.text = ggplot2::element_text(size=6.5) # , plot.background = ggplot2::element_rect(colour = &quot;gray77&quot;, fill=NA, linewidth=1) ) }) check out our plots plt_stands ## [[1]] ## ## [[2]] 3.4 Canopy Cover To get a raster of canopy cover, which refers to the proportional area of the ground covered by tree crowns, it is more straightforward (and computationally efficient) to use the CHM and not the tree crown polygons. As a reminder, the canopy height model is a raster dataset that represents the height of objects above the ground, we set the minimum height at 2 m we’ll first define a function to filter our CHM for cells over a certain height threshold so that users can determine what height constitutes the “canopy” # filter our CHM for cells over a certain height threshold chm_threshold_fn &lt;- function(chm_rast, min_ht_m = 2) { #option 1 new_rast &lt;- terra::clamp(chm_rast, lower = as.numeric(min_ht_m), values = F) # #option 3 # new_rast &lt;- terra::ifel(chm_rast &lt; as.numeric(min_ht_m), NA, chm_rast) return(new_rast) } # chm_threshold_fn(chm_rast, 11) %&gt;% terra::plot() now we can use our filtered CHM to calculate canopy cover at a coarser resolution by determining the proportion of cells in the aggregated cell that were originally non-NA (i.e. &gt; height threshold) agg_propotion_fn &lt;- function(x){ if(any(is.na(x))){ # return proportion of non-na cells return(sum(!is.na(x)) / length(x)) }else{ # return 1 if all cells are non-na return(1) } } # put all of this together to create a canopy cover function chm_to_canopy_cov_fn &lt;- function(chm_rast, min_ht_m = 2, des_res) { # filter our CHM for cells over a certain height threshold rast &lt;- chm_threshold_fn(chm_rast=chm_rast, min_ht_m=min_ht_m) # aggregate the raster res_temp &lt;- terra::res(rast)[1] if(res_temp&lt;des_res){ rast &lt;- terra::aggregate( rast , fact = round(des_res/res_temp) , fun = agg_propotion_fn , cores = lasR::half_cores() ) }else{ rast &lt;- terra::subst(rast, from = NA, to = NA, others = 1) } return(rast) } 3.4.1 Entire Area let’s get canopy cover for a canopy defined as heights &gt; 6 m and raster cells aggregated to 100 x 100 m (1 ha cells) and make a quick plot # get our canopy cover raster cc_rast &lt;- chm_to_canopy_cov_fn(chm_rast = chm_rast, min_ht_m = 6, des_res = 100) plot it cc_rast %&gt;% terra::plot( col = # viridis::mako(100, direction = -1) scales::pal_gradient_n( RColorBrewer::brewer.pal(n=9,name = &quot;Greens&quot;) # max is 9 )(seq(0, 1, length.out = 100)) , axes = F, alpha = 0.9 , main = &quot;Canopy Cover (%)&quot; ) terra::plot(terra::vect(stand_sf), add = T, col = NA, border = &quot;black&quot;, lwd = 2) 3.4.2 Stands we can use our canopy cover raster to get a plot for each stand # get the plot for each stand plt_cc &lt;- 1:nrow(stand_sf) %&gt;% purrr::map(function(x, ul = 1, ll = 0){ plot_raster_stand_fn( cc_rast , stand = stand_sf[x,] , buffer = 50 , des_res = 100 , stand_color = &quot;black&quot; , my_title = &quot;Canopy Cover&quot; , scale_name = &quot;Canopy Cover&quot; , agg_fun = &quot;max&quot; ) + ggplot2::labs(subtitle = paste(&quot;Unit:&quot;,stand_sf[x,]$unit_name)) + ggplot2::scale_fill_distiller( palette = &quot;Greens&quot;, direction = 1, limits = c(ll,ul) , labels = scales::percent ) }) check out our plots plt_cc ## [[1]] ## ## [[2]] 3.5 Write Data load in the tree crown polygon data from cloud2trees::cloud2trees() and filter based on the tree list # get the data from already run crowns_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_crowns.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() %&gt;% dplyr::select( -dplyr::any_of(c( &quot;cruz_tree_kg_per_m3&quot; , &quot;cruz_stand_kg_per_m3&quot; , &quot;cruz_crown_biomass_kg&quot; ))) %&gt;% dplyr::inner_join( treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select( treeID,unit_id,unit_name,tidyselect::starts_with(&quot;stand_area_&quot;) , is_missing_cruz , cruz_tree_kg_per_m3 , cruz_stand_kg_per_m3 , cruz_crown_biomass_kg ) , by = &quot;treeID&quot; ) # the records are the same identical(nrow(treetops_sf), nrow(crowns_sf)) # crowns_sf %&gt;% dplyr::filter(is.na(cruz_crown_biomass_kg)) %&gt;% nrow() save the data by unit for sharing with a boss, coworker, friend, etc. # write crowns and tree tops write_temp &lt;- crowns_sf$unit_id %&gt;% unique() %&gt;% purrr::map(function(x, new_crs = proj_crs, my_outdir = outdir){ # dsn&#39;s cf &lt;- file.path( my_outdir, paste0(&quot;final_detected_crowns_unit_&quot;,x,&quot;.gpkg&quot;) ) tf &lt;- file.path( my_outdir, paste0(&quot;final_detected_tree_tops_unit_&quot;,x,&quot;.gpkg&quot;) ) ### write the data to the disk # crown vector polygons sf::st_write( crowns_sf %&gt;% dplyr::filter(unit_id == x) %&gt;% sf::st_transform(new_crs) , dsn = cf , append = FALSE , quiet = TRUE ) # tree top vector points sf::st_write( # get tree points crowns_sf %&gt;% dplyr::filter(unit_id == x) %&gt;% sf::st_drop_geometry() %&gt;% sf::st_as_sf(coords = c(&quot;tree_x&quot;, &quot;tree_y&quot;), crs = sf::st_crs(crowns_sf)) %&gt;% sf::st_transform(new_crs) %&gt;% dplyr::mutate( tree_x = sf::st_coordinates(.)[,1] , tree_y = sf::st_coordinates(.)[,2] ) , dsn = tf , append = FALSE , quiet = TRUE ) }) we now need to attach the treeID to the point cloud for use in performing tree species classification based on the three-dimensional data based on the methods developed within our study area by Blackburn (2022) we’ll use the height normalized point cloud data dropped off by cloud2trees::cloud2trees() during our point cloud processing by loading the data in as a LAScatalog via lidR::readLAScatalog() and attaching the treeID attribute with cloud2trees::polygon_attribute_to_las() # use the height normalized las files dropped off by cloud2trees::cloud2trees() las_ctg &lt;- lidR::readLAScatalog(&quot;../data/point_cloud_processing_delivery/norm_las/&quot;) # set the lascatalog options lidR::opt_progress(las_ctg) &lt;- F lidR::opt_filter(las_ctg) &lt;- &quot;-drop_duplicates -drop_class 2 9 18&quot; ## 2 = ground; 9 = water; 18 = noise lidR::opt_select(las_ctg) &lt;- &quot;xyz0&quot; # 0 enables all extra bytes to be loaded...possibly treeID # lidR::opt_output_files(las_ctg) &lt;- paste0(tempdir(), &quot;/{*}_treed&quot;) # define a function to attach treeID to the point cloud and return only points with a treeID # this will get used in our lidR::catalog_apply() ctg_polygon_attribute_to_las &lt;- function( chunk , poly_df , attribute = &quot;treeID&quot; , force_crs = F ){ las &lt;- lidR::readLAS(chunk) if(lidR::is.empty(las)){return(NULL)} # attach treeID nlas_tree &lt;- cloud2trees::polygon_attribute_to_las( las , cloud2trees::simplify_multipolygon_crowns(poly_df) # if already simplified, does nothing , attribute = attribute , force_crs = force_crs ) # filter for only points with trees nlas_tree &lt;- lidR::filter_poi(nlas_tree, !is.na(treeID)) if(lidR::is.empty(nlas_tree)){ return(NULL) }else{ return(nlas_tree) } } now we’ll simplify the multipolygon crowns using cloud2trees::simplify_multipolygon_crowns() and attach the treeID attribute to the point cloud by applying our ctg_polygon_attribute_to_las() function defined above with lidR::catalog_apply() # simplify multipolygon crowns simp_crowns_sf &lt;- crowns_sf %&gt;% cloud2trees::simplify_multipolygon_crowns() # make the treeID numeric # !!!!! this only works because our treeID is already numeric and can be cast as such # !!!!! if you have a treeID that cannot be cast to numeric then you will need to make a unique treeID # !!!!! and save it as a backup numeric treeID so that your numeric treeID can be tied to the character treeID simp_crowns_sf &lt;- simp_crowns_sf %&gt;% dplyr::mutate(treeID = as.numeric(treeID)) # get the attribute attached to the las # apply it las &lt;- lidR::catalog_apply( ctg = las_ctg , FUN = ctg_polygon_attribute_to_las , .options = list(automerge = TRUE) # ctg_polygon_attribute_to_las options , poly_df = simp_crowns_sf , attribute = &quot;treeID&quot; , force_crs = T ) %&gt;% lidR::filter_duplicates() %&gt;% lidR::remove_lasattribute(&quot;buffer&quot;) %&gt;% lidR::filter_poi(!is.na(treeID) &amp; Z&gt;0) %&gt;% lidR::add_lasattribute(name = &quot;treeID&quot;, desc = &quot;treeID&quot;) check that we have treeID las %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% dplyr::glimpse() ## Rows: 26,405,551 ## Columns: 4 ## $ X &lt;dbl&gt; 470000.0, 469999.6, 469999.7, 469999.5, 469999.6, 469999.6, 469… ## $ Y &lt;dbl&gt; 3809318, 3809314, 3809315, 3809315, 3809315, 3809316, 3809316, … ## $ Z &lt;dbl&gt; 1.01, 1.63, 2.34, 1.62, 2.09, 2.26, 2.38, 2.49, 2.84, 1.76, 2.0… ## $ treeID &lt;dbl&gt; 1329405, 1329915, 1329915, 1334574, 1334574, 1334574, 1329405, … let’s look at our point cloud for a single tree (the tallest tree) # get the tallest tree cloud one_tree_las_temp &lt;- las %&gt;% lidR::filter_poi( treeID==( treetops_sf %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(tree_height_m==max(tree_height_m)) %&gt;% dplyr::slice(1) %&gt;% dplyr::pull(treeID) %&gt;% as.numeric() # because we converted to numeric for our las ) ) # plot it plot3D::scatter3D( x = one_tree_las_temp@data$X , y = one_tree_las_temp@data$Y , z = one_tree_las_temp@data$Z , colvar = one_tree_las_temp@data$Z , cex = 0.6, pch = 19 , colkey = T , phi = 6 , col = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) , main =&quot;single tree point cloud&quot; ) finally, we’ll save this point cloud data to our output folder for sharing lidR::writeLAS( las = las , file = file.path(outdir, &quot;norm_las_trees.las&quot;) , index = T ) "],["validate-tree-detection-and-crown-delineation.html", "Section 4 Validate Tree Detection and Crown Delineation 4.1 NeonTreeEvaluation overview 4.2 lidar data in NeonTreeEvaluation 4.3 Height threshold for “canopy” trees 4.4 Example validation process 4.5 ITD tuning 4.6 Training validation process 4.7 Evaluation validation process", " Section 4 Validate Tree Detection and Crown Delineation In this section we’ll use the benchmark data made available in the NeonTreeEvaluation data set (Weinstein et al. 2021) to evaluate our process for lidar-based tree detection. We’ll implement our tree detection process via the cloud2trees package. Puliti et al. (2023) also provide the benchmarking dataset “FOR-instance” for dense airborne laser scanning data. The dataset comprises five curated UAV-based laser scanning data collections from diverse global locations, representing various forest types. The data was collected over five sites in Norway, Czech Republic, Austria, Australia, and New Zealand and covers mature forests for the following forest types: boreal (42% of the number of trees) and temperate coniferous (5%), temperate mixed deciduous (29%), coniferous plantation (13%), and dry sclerophyll forests (11%). We might also consider this data for validation of our methods. First, load the standard libraries # bread-and-butter library(tidyverse) # the tidyverse library(viridis) # viridis colors library(harrypotter) # hp colors library(RColorBrewer) # brewer colors library(scales) # work with number and plot scales library(latex2exp) # visualization library(mapview) # interactive html maps library(kableExtra) # tables library(patchwork) # combine plots library(ggnewscale) # ggnewscale library(rgl) # rgl plotting # spatial analysis library(sf) # simple features library(lidR) # lidar data library(cloud2trees) # tha cloud2trees library(NeonTreeEvaluation) # benchmark data 4.1 NeonTreeEvaluation overview Weinstein et al. (2021) developed: a benchmark dataset of individual canopy crowns derived from multi-sensor imagery in the National Ecological Observatory Network (Table 1) that provides: 1) co-registered remote sensing data from multiple sensors (LiDAR, RGB imagery, and hyperspectral imagery) to allow comparisons of methods based on any single sensor (e.g., for LiDAR based methods), or any combination of sensors (e.g., combining RGB and hyperspectral), and 2) three types of evaluation data to allow assessing both ‘tree detection’, defined as the identifying the location of individual trees using evaluation data with a point at the crown center , and ‘crown delineation’ defined as identifying the boundary edge of crowns across a broad range of forest types. The benchmark is designed to allow flexibility in both workflow and sensor selection. (p. 2) Table 1. Summary of datasets included in the benchmark dataset. All sensor data has been cropped to the extent of NEON field sampling plots. Note the three data labeled as “Evaluation data” in the table. If you are asking “why three evaluation datasets?”, the authors provide some detail: The inclusion of multiple evaluation types is critical because each type of evaluation data has strengths and limitations in evaluating model performance. Field collected stems are the most common evaluation data used in crown detection work due to high confidence that each stem represents a location of a single tree. However, the position of a tree stem can fail to accurately represent the position of the crown as viewed from above due to a combination of spatial errors in alignment with the image data and the tendency for trees to grow at acute angles (tree lean is not measured in the NEON data), such that the center of the crown and position of the stem can be offset by several meters….Image-annotated crowns are relatively easy to scale, allowing the collection of data for a wide range of forest types and for annotation of every visible crown in the image. Using image-annotated crowns supports the evaluation of methods across a broad range of forest types and allows both recall and precision to be calculated. However, since these annotations are not generated by an observer in the field there can be errors due to interpreting the images. This problem is solved using field-annotated crowns in which an observer annotates the remote-sensing imagery on a tablet while in the field [33]. The main limitation to this approach is that it is labor intensive, meaning that only a relatively small amount of validation data can be collected, making it difficult to obtain a large number of crowns across broad scales or assess model precision. Given the tradeoffs in each evaluation type, providing multiple criteria is a useful way of balancing the need for broad scale model verification with rigorous evaluation of field-based measurements. (p. 14-15) To evaluate the performance of our aerial point cloud-based algorithm for 1) tree detection and 2) crown delineation using NeonTreeEvaluation we need to ensure our tree polygon data is formatted properly: This package takes a standard submission format of predicted crowns in either bounding box or polygons as input and returns the evaluation scores of the detections for each of the three evaluation datasets. This reproducible workflow will facilitate creating a transparent process for future comparisons among crown detection algorithms. (p. 14) The authors describe the “standard submission format” on the package GitHub: Each row contains information for one predicted bounding box. The plot_name should be named the same as the files in the dataset without extension (e.g. SJER_021_2018 not SJER_021_2018.tif) and not the full path to the file on disk. Not all evaluation data are available for all plots. Functions like evaluate_field_crowns and evaluate_image_crowns will look for matching plot name and ignore other plots. Depending on the speed of the algorithm, the simplest thing to do is predict all images in the RGB folder (see list_rgb()) and the package will handle matching images with the correct data to the correct evaluation procedure…Instead of bounding boxes, some methods may return polygons. To submit as polygons, create a single unprojected shapefile with polygons in image coordinates. Polygons must be complete with no holes. Here is an example of the above csv file in polygon format. Here the xmin, xmax, etc. columns are ignored since the information is stored in the geometry data. Simple feature collection with 6 features and 7 fields geometry type: POLYGON dimension: XY bbox: xmin: 30.39723 ymin: 122.1164 xmax: 397.5746 ymax: 400 CRS: NA xmin ymin xmax ymax score label plot_name 1 41.01716 230.8854 151.08607 342.6985 0.8098674 Tree DSNY_014_2019 2 357.32129 122.1164 397.57458 159.3758 0.6968824 Tree DSNY_014_2019 3 30.39723 136.9157 73.79434 184.9473 0.5713338 Tree DSNY_014_2019 4 260.65921 285.6689 299.68811 326.7933 0.5511004 Tree DSNY_014_2019 5 179.34564 371.6130 232.49385 400.0000 0.4697072 Tree DSNY_014_2019 6 316.27377 378.9802 363.67542 400.0000 0.3259409 Tree DSNY_014_2019 st_sfc.lst. 1 POLYGON ((41.01716 230.8854... 2 POLYGON ((357.3213 122.1164... 3 POLYGON ((30.39723 136.9157... 4 POLYGON ((260.6592 285.6689... 5 POLYGON ((179.3456 371.613,... 6 POLYGON ((316.2738 378.9802... So we are going to: run cloud2trees::cloud2trees() on all of the lidar data for which there is evaluation (i.e. ground truth) data, combine into a single tree list with a row unique by a detected tree and the plot_name column (e.g. “SJER_021_2018”), as an unprojected sf data with polygons in image coordinates. We may need to run cloud2trees::simplify_multipolygon_crowns() prior to submission. 4.2 lidar data in NeonTreeEvaluation we first have to download evaluation data from the Zenodo archive (1GB), use the NeonTreeEvaluation::download() function to place the data in the correct package location. Download the much larger training data, set training=TRUE. We are going to use the training data to determine the “best” ITD window function to use for a given NEON site since the cloud2trees package strongly recommends that “a different window size function is defined for each region of your study area with significantly different forest structure”. Weinstein et al. (2021) describe the training data: During our research on canopy crown detection algorithms, we annotated many geographic tiles separate from the evaluation data. The training sites were selected to capture a range of forest conditions…The training tiles were chosen at random from the NEON data portal, with the requirement that they did not contain a large amount of missing data and they did not overlap with any evaluation plots. Depending on the tree density at the site, we either annotated the entire 1 km2 tile or cropped it to a smaller size to create more tractable sizes for annotation. This data is released alongside the benchmark dataset; however, our goal is to promote the best possible crown-delineation algorithm regardless of training data, and it is not necessary to use this training data to generate predictions. Given the large size of training tiles, the training annotations were less thoroughly reviewed and were only based on the RGB imagery. (p. 11) Note: the current version of NeonTreeEvaluation::download() does not download the training data even if setting training = T. I dug around and found the archive of the training data at https://zenodo.org/records/5914554. I’ll need to circle back with a GitHub pull request to fix the NeonTreeEvaluation::download() function so that it gets the training data which is hosted at the link mentioned previously and does not reside at where the function currently points to (https://zenodo.org/records/4770593). I also manually copied the files in “NeonTreeEvaluation/extdata/NeonTreeEvaluation/training/RGB” to “NeonTreeEvaluation/extdata/NeonTreeEvaluation/evaluation/RGB” so that evaluation could be performed. See list.dirs(system.file(package = \"NeonTreeEvaluation\"),recursive=T) NeonTreeEvaluation::download(training = T, force = F) 4.2.1 Training data Let’s find what training data is available. We’ll use this data to determine the best window size function for use in our individual tree detection (ITD) process # i did some digging around and the TRAINING lidar data is here training_dir_temp &lt;- system.file(package = &quot;NeonTreeEvaluation&quot;, &quot;extdata&quot;, &quot;NeonTreeEvaluation&quot;, &quot;training&quot;, &quot;LiDAR&quot;) # files training_files_temp &lt;- training_dir_temp %&gt;% list.files(recursive = T, pattern = &quot;.*\\\\.(laz|las)$&quot;, full.names = T) %&gt;% unique() let’s pull out all NEON sites with point cloud data and create a data frame for tracking purposes # let&#39;s pull out all sites with `.laz` data and create a data frame for tracking purposes lidar_df &lt;- training_files_temp %&gt;% dplyr::as_tibble() %&gt;% dplyr::rename(f_path = 1) %&gt;% # create some other variables dplyr::mutate( plot_name = f_path %&gt;% basename() %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;) , data_type = &quot;training&quot; ) # what? lidar_df %&gt;% dplyr::glimpse() ## Rows: 16 ## Columns: 3 ## $ f_path &lt;chr&gt; &quot;C:/Program Files/R/R-4.3.0/library/NeonTreeEvaluation/extda… ## $ plot_name &lt;chr&gt; &quot;2018_BART_4_322000_4882000_image_crop&quot;, &quot;2018_HARV_5_733000… ## $ data_type &lt;chr&gt; &quot;training&quot;, &quot;training&quot;, &quot;training&quot;, &quot;training&quot;, &quot;training&quot;, … 4.2.2 Evaluation data Let’s find what evaluation data is available. We’ll use this data to evaluate our point cloud-based tree detection methodology. # i did some digging around and the EVALUATION lidar data is here eval_dir_temp &lt;- system.file(package = &quot;NeonTreeEvaluation&quot;, &quot;extdata&quot;, &quot;NeonTreeEvaluation&quot;, &quot;evaluation&quot;, &quot;LiDAR&quot;) # files eval_files_temp &lt;- eval_dir_temp %&gt;% list.files(recursive = T, pattern = &quot;.*\\\\.(laz|las)$&quot;, full.names = T) %&gt;% unique() bind the rows onto our tracking data lidar_df &lt;- lidar_df %&gt;% dplyr::bind_rows( eval_files_temp %&gt;% dplyr::as_tibble() %&gt;% dplyr::rename(f_path = 1) %&gt;% # create some other variables dplyr::mutate( plot_name = f_path %&gt;% basename() %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;) , data_type = &quot;evaluation&quot; ) ) %&gt;% dplyr::mutate(data_type = as.factor(data_type)) 4.2.3 Filter for ground truth data that’s a lot of files…let’s only process the sites with evaluation data or image annotated crowns classified as training annotations # there are functions to get a list of all evaluation data # let&#39;s use these to filter our lidar files plotnames_temp &lt;- c( NeonTreeEvaluation::list_annotations() , NeonTreeEvaluation::list_field_stems() # this one includes file paths, so we have to clean , NeonTreeEvaluation::list_field_crowns() %&gt;% stringr::str_match(pattern=&quot;(\\\\w+).tif&quot;) %&gt;% .[,2] # there are plot_names from the submission data too , NeonTreeEvaluation::submission_polygons$plot_name %&gt;% unique() , NeonTreeEvaluation::submission$plot_name %&gt;% unique() # make sure we have all image annotated crowns , system.file(package = &quot;NeonTreeEvaluation&quot;, &quot;extdata&quot;, &quot;NeonTreeEvaluation&quot;, &quot;annotations&quot;) %&gt;% list.files() %&gt;% basename() %&gt;% stringr::str_remove_all(&quot;\\\\.(xml)$&quot;) %&gt;% unique() ) %&gt;% unique() # huh? # plotnames_temp %&gt;% sample(11) %&gt;% sort() The field collected stem validation data will take a bit more to get the actual plots for which there is validation data. We have to pull clean_field_data() from NeonTreeEvaluation as an internal function which is used to filter NeonTreeEvaluation::field…we’ll than match other filters used in NeonTreeEvaluation::evaluate_field_stems() # this is an internal function from `NeonTreeEvaluation::evaluate_field_stems()` clean_field_data&lt;-function(field){ field$area&lt;-field$maxCrownDiameter*field$ninetyCrownDiameter field&lt;-field %&gt;% filter(!is.na(itcEasting),!stringr::str_detect(eventID,&quot;2014&quot;),growthForm %in% c(&quot;single bole tree&quot;,&quot;multi-bole tree&quot;,&quot;small tree&quot;,&quot;sapling&quot;),stemDiameter&gt;15) %&gt;% droplevels() %&gt;% filter(height&gt;3|is.na(height)) #Limit difference in heights to_remove&lt;-field %&gt;% group_by(individualID) %&gt;% summarize(mean=mean(height),sum_difference = abs(sum(diff(height)))) %&gt;% filter(sum_difference &gt; 8) field&lt;-field %&gt;% filter(!individualID %in% to_remove$individualID) } # apply all the filters for field collected stems used in `NeonTreeEvaluation::evaluate_field_stems()` field_plots_temp &lt;- NeonTreeEvaluation::field %&gt;% clean_field_data() %&gt;% dplyr::filter(height&gt;3) %&gt;% dplyr::group_by(plotID,individualID) %&gt;% dplyr::summarize(samples=length(unique(eventID))) %&gt;% dplyr::filter(samples&gt;1) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(plotID=as.character(plotID)) %&gt;% dplyr::pull(plotID) %&gt;% unique() # `NeonTreeEvaluation::evaluate_field_stems()` also requires rgb and chm data chm_temp &lt;- NeonTreeEvaluation::list_chm() %&gt;% basename() %&gt;% stringr::str_match(&quot;(\\\\w+)_\\\\d+&quot;) %&gt;% .[,2] %&gt;% unique() field_plots_temp &lt;- field_plots_temp[field_plots_temp %in% chm_temp] rgb_temp &lt;- NeonTreeEvaluation::list_rgb() %&gt;% basename() %&gt;% stringr::str_match(&quot;(\\\\w+)_\\\\d+&quot;) %&gt;% .[,2] %&gt;% unique() field_plots_temp &lt;- field_plots_temp[field_plots_temp %in% rgb_temp] filter our lidar data list to keep only point clouds in plots for which there is validation data lidar_df &lt;- lidar_df %&gt;% #filter based on plots in evaluation data dplyr::filter(plot_name %in% plotnames_temp) %&gt;% # filter for field plots # pull out site dplyr::mutate( plot_temp = stringr::str_extract( string = plot_name , pattern = field_plots_temp %&gt;% unique() %&gt;% toupper() %&gt;% paste(collapse = &quot;|&quot;) ) ) %&gt;% # flag the data with image annotated crowns or field stems which we&#39;ll use to train our ws fn # if this doesn&#39;t make sense yet, keep reading down dplyr::mutate( has_image_annotation = plot_name %in% NeonTreeEvaluation::list_annotations() , has_field_stems = !is.na(plot_temp) ) %&gt;% # keep only those with field stems or image annotated crowns dplyr::filter(has_image_annotation | has_field_stems) %&gt;% dplyr::select(-c(plot_temp)) We need to put the NEON site identifier in our data. We’ll use the field-collected tree data from NEON sites made available which will also serve to filter our point cloud list for forested NEON sites only. # get a list of siteID neon_sites_temp &lt;- NeonTreeEvaluation::field %&gt;% sf::st_drop_geometry() %&gt;% dplyr::distinct(siteID) %&gt;% # there are siteID&#39;s here too dplyr::bind_rows( NeonTreeEvaluation::crown_polygons %&gt;% sf::st_drop_geometry() %&gt;% dplyr::distinct(siteID) ) %&gt;% dplyr::mutate(siteID = as.character(siteID)) %&gt;% dplyr::distinct(siteID) %&gt;% dplyr::pull(siteID) # extract the siteID from the plot_name lidar_df &lt;- lidar_df %&gt;% # pull out site dplyr::mutate( siteID = stringr::str_extract( string = plot_name , pattern = neon_sites_temp %&gt;% unique() %&gt;% toupper() %&gt;% paste(collapse = &quot;|&quot;) ) ) %&gt;% dplyr::filter(!is.na(siteID)) what is the breakdown of training vs eval? lidar_df %&gt;% dplyr::count(data_type) ## # A tibble: 2 × 2 ## data_type n ## &lt;fct&gt; &lt;int&gt; ## 1 evaluation 522 ## 2 training 15 4.2.4 Filter for conifer sites we will want to limit our evaluation to only sites with conifer trees since cloud2trees implements methods developed specifically to quantify conifer forest structure that may not be appropriate for other forest types we’ll use the field data in the package to look for NEON sites with conifer trees. We’ll use the NEON plant list to identify conifer species: https://data.neonscience.org/taxonomic-lists?taxonTypeCode=PLANT (click “DOWNLOAD TAXONOMIC LIST”). We’ll filter for species belonging to Class Pinopsida. conifer_spp &lt;- readr::read_csv( &quot;../data/OS_TAXON_PLANT-20220330T142149.csv&quot; , show_col_types = F , progress = F ) %&gt;% dplyr::filter( tolower(`class`) %in% c(&quot;pinopsida&quot;) ) %&gt;% dplyr::mutate( taxonID = toupper(taxonID) , vernacularName = tolower(vernacularName) , genus = stringr::str_to_title(genus) ) %&gt;% dplyr::distinct(taxonID, vernacularName, genus) what are some of these conifers? # huh? conifer_spp %&gt;% dplyr::slice_sample(n = 10) %&gt;% kableExtra::kbl(caption = &quot;Conifer species taxonID examples&quot;) %&gt;% kableExtra::kable_styling() Table 4.1: Conifer species taxonID examples taxonID vernacularName genus PIEN2 apache pine Pinus HESPE5 western cypress Hesperocyparis TSJE hybrid hemlock Tsuga THPL western redcedar Thuja PILA sugar pine Pinus ABCO white fir Abies PIHA7 aleppo pine Pinus PIPI7 italian stone pine Pinus HEPI11 pygmy cypress Hesperocyparis HESA17 sargent’s cypress Hesperocyparis filter for NEON sites that have conifer trees based on field data from all terrestrial NEON sites with qualifying woody vegetation: https://data.neonscience.org/data-products/DP1.10098.001 conifer_sites &lt;- NeonTreeEvaluation::field %&gt;% dplyr::left_join( conifer_spp %&gt;% dplyr::mutate(is_conifer = 1) , by = &quot;taxonID&quot; ) %&gt;% dplyr::mutate(is_conifer = dplyr::coalesce(is_conifer, 0)) %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( tot = dplyr::n() , conifer = sum(is_conifer) , latitude = mean(plotLatitude) , longitude = mean(plotLongitude) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(pct_conifer = conifer/tot) what is the breakdown of woody vegetation sampled in NEON sites by the percent conifer? conifer_sites %&gt;% dplyr::select(-c(longitude,latitude)) %&gt;% dplyr::arrange(desc(pct_conifer), desc(tot)) %&gt;% dplyr::slice_head(n = 19) %&gt;% kableExtra::kbl( caption = &quot;Conifers in NEON sites&quot; , digits = 2 , col.names = c( &quot;site&quot;,&quot;total trees&quot;,&quot;conifer trees&quot;, &quot;% conifer&quot; ) ) %&gt;% kableExtra::kable_styling() Table 4.2: Conifers in NEON sites site total trees conifer trees % conifer NIWO 1804 1804 1.00 ONAQ 88 88 1.00 MOAB 29 29 1.00 HEAL 21 21 1.00 YELL 13 13 1.00 TEAK 621 619 1.00 DEJU 173 169 0.98 ABBY 268 241 0.90 RMNP 1375 1083 0.79 SOAP 503 389 0.77 DSNY 34 26 0.76 TALL 2144 1521 0.71 OSBS 1288 858 0.67 JERC 562 336 0.60 HARV 3736 1701 0.46 TREE 1303 430 0.33 BART 3636 1022 0.28 BONA 188 48 0.26 STEI 754 151 0.20 let’s only keep NEON sites with a minimum threshold of the woody vegetation sampled as conifer # minimum pct conifer min_conifer_pct &lt;- .5 now join on our proportion of conifer by NEON site to our lidar data list lidar_df &lt;- lidar_df %&gt;% dplyr::left_join(conifer_sites, by = &quot;siteID&quot;) %&gt;% sf::st_as_sf(coords = c(&quot;longitude&quot;,&quot;latitude&quot;), crs = 4326, remove = F) before we filter for conifer sites only, let’s check which sites we have lidar data from and whether or not they are classified as a conifer site based on this threshold of 50% lidar_df %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise(pct_conifer = dplyr::first(pct_conifer)) %&gt;% dplyr::mutate( is_conifer_site = pct_conifer&gt;min_conifer_pct , pct_conifer = scales::percent(pct_conifer,accuracy=0.1) ) %&gt;% mapview::mapview( zcol = &quot;is_conifer_site&quot;, legend = T , layer.name = &quot;conifer site?&quot; , col.regions = viridis::viridis(n=2,begin = 0.3,end = 0.7) ) looks reasonable, let’s filter our point cloud processing data for conifer sites based on our proportional threshold of 50% # data frame of sites lidar_df &lt;- lidar_df %&gt;% dplyr::filter(pct_conifer&gt;min_conifer_pct) %&gt;% # filter out corrupt las files # these don&#39;t load in lasR...maybe extrabytes issue??? dplyr::filter( !plot_name %in% c( &quot;NIWO_005_2018&quot; , &quot;SOAP_014_2018&quot; , &quot;MOAB_003_2018&quot; , &quot;NIWO_009_2018&quot; , &quot;TEAK_028_2018&quot; , &quot;YELL_058_2020&quot; , &quot;RMNP_011_2018&quot; , &quot;YELL_030_2018&quot; , &quot;YELL_051_2019&quot; , &quot;SOAP_014_2019&quot; , &quot;TEAK_005_2018&quot; , &quot;DSNY_019_2019&quot; , &quot;HEAL_004_2019&quot; , &quot;SOAP_049_2018&quot; , &quot;TEAK_016_2019&quot; , &quot;YELL_006_2018&quot; ) ) %&gt;% dplyr::mutate( # these plots not corrupt but there are no field trees to evaluate against has_field_stems = dplyr::case_when( plot_name %in% c( &quot;DEJU_021_2018&quot; , &quot;DEJU_021_2019&quot; , &quot;JERC_055_2018&quot; , &quot;JERC_055_2019&quot; , &quot;ABBY_064_2018&quot; , &quot;ABBY_064_2019&quot; ) ~ F , T ~ has_field_stems ) ) %&gt;% dplyr::filter( data_type == &quot;training&quot; | has_field_stems | has_image_annotation ) what NEON sites have conifers and training/evaluation data? lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( n = dplyr::n() , training_plots = sum(ifelse(data_type==&quot;training&quot;,1,0)) , evaluation_plots = sum(ifelse(data_type==&quot;evaluation&quot;,1,0)) , evaluation_ann_plots = sum(ifelse(has_image_annotation &amp; data_type==&quot;evaluation&quot;,1,0)) , evaluation_fld_plots = sum(ifelse(has_field_stems &amp; data_type==&quot;evaluation&quot;,1,0)) ) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::select(-n) %&gt;% kableExtra::kbl( caption = &quot;NEON sites with conifers and lidar plots&quot; , col.names = c( &quot;site&quot;,&quot;training plots&quot;,&quot;evaluation plots&quot; ,&quot;evaluation plots&lt;br&gt;with ground truth&lt;br&gt;image ann. crowns&quot; ,&quot;evaluation plots&lt;br&gt;with ground truth&lt;br&gt;field collected stems&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.3: NEON sites with conifers and lidar plots site training plots evaluation plots evaluation plotswith ground truthimage ann. crowns evaluation plotswith ground truthfield collected stems OSBS 3 49 14 46 TEAK 1 51 51 0 NIWO 1 42 10 41 JERC 1 22 6 21 TALL 0 23 2 22 ABBY 0 14 2 13 DEJU 0 14 0 14 DSNY 0 6 6 0 MOAB 0 2 0 2 SOAP 0 2 2 0 ONAQ 0 1 1 0 note that the sum of the evaluation plots with image annotated crowns and with field collected stems does not necessarily equate to the number of evaluation plots since one plot can have both image annotated and field collected data. what is the spatial distribution of these conifer sites for which lidar data is available? lidar_df %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( training_plots = sum(ifelse(data_type==&quot;training&quot;,1,0)) , evaluation_plots = sum(ifelse(data_type==&quot;evaluation&quot;,1,0)) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( data_type = dplyr::case_when( training_plots&gt;0 &amp; evaluation_plots==0 ~ &quot;training&quot; , training_plots&gt;0 &amp; evaluation_plots&gt;0 ~ &quot;training+evaluation&quot; , training_plots==0 &amp; evaluation_plots&gt;0 ~ &quot;evaluation&quot; , T ~ &quot;other&quot; ) ) %&gt;% mapview::mapview( zcol = &quot;data_type&quot;, legend = T , layer.name = &quot;data type&quot; , col.regions = viridis::magma(n=2,begin = 0.2,end = 0.8) ) 4.2.5 Augment training data While the map above shows that the training and evaluation data are in places that we expect to have conifers, the spatial coverage of training data is limited. This is especially true when one considers that even NEON sites that are close in proximity can have very different forest types. For example, the “TEAK” site east of Fresno, CA has both training and evaluation data and features vegetation described as “dominant tree species include red and white ﬁr (Abies magniﬁca and Abies concolor), Jeﬀrey pine (Pinus jeﬀreyi), and lodgepole pine (Pinus contorta)” (source). While the “SOAP” site that is close in proximity and lacks training data has vegetation described as “Ponderosa pine (Pinus ponderosa) and incense cedar (Calocedrus decurrens) dominate the overstory” (source). We will augment the training data by NEON site to validate against the image annotated crown ground truth using NeonTreeEvaluation::evaluate_image_crowns() by: if a site has image annotated crowns marked as training data, use those plots otherwise, if a site has at least 5 image annotated crown evaluation data sets borrow two plots to validate against otherwise, if a site has at least 4 image annotated crown evaluation data sets borrow one plot to validate against We will augment the training data by NEON site to validate against the field collected stems ground truth using NeonTreeEvaluation::evaluate_field_stems() by: if a site has at least 5 field collected stem validation data sets, borrow two plots to validate against even if the site has image annotated crowns marked as out-of-box training data otherwise, if a site has at least 4 field collected stem evaluation data sets, borrow one plot to validate against We will then use the results of the validation based on the training data to select the best variable window search function based on the precision and recall metrics by NEON site. If a NEON site does not have training data the best overall variable window function will be used. # set seed to replicate results set.seed(444) # randomly assign plots as training: lidar_df &lt;- lidar_df %&gt;% dplyr::ungroup() %&gt;% # generate rand dplyr::mutate(rand = runif(n=dplyr::n())) %&gt;% # count training/eval by site and flag new training dplyr::group_by(siteID) %&gt;% dplyr::mutate( training_plots = sum(ifelse(data_type==&quot;training&quot;,1,0)) , evaluation_ann_plots = sum(ifelse(has_image_annotation &amp; data_type==&quot;evaluation&quot;,1,0)) , evaluation_fld_plots = sum(ifelse(has_field_stems &amp; data_type==&quot;evaluation&quot;,1,0)) ) %&gt;% # sample img annotated data dplyr::arrange(siteID, has_image_annotation, rand) %&gt;% dplyr::group_by(siteID,has_image_annotation) %&gt;% dplyr::mutate( is_new_image_annotation_training = dplyr::case_when( training_plots == 0 &amp; evaluation_ann_plots&gt;=5 &amp; has_image_annotation == T &amp; dplyr::row_number()&lt;=2 ~ T , training_plots == 0 &amp; evaluation_ann_plots&gt;=4 &amp; has_image_annotation == T &amp; dplyr::row_number()&lt;=1 ~ T , T ~ F ) ) %&gt;% # sample fld stems data dplyr::arrange(siteID, has_field_stems, rand) %&gt;% dplyr::group_by(siteID,has_field_stems) %&gt;% dplyr::mutate( is_new_field_stems_training = dplyr::case_when( evaluation_fld_plots&gt;=5 &amp; has_field_stems == T &amp; dplyr::row_number()&lt;=2 ~ T , evaluation_fld_plots&gt;=4 &amp; has_field_stems == T &amp; dplyr::row_number()&lt;=1 ~ T , T ~ F ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(-c(rand,tot,conifer,tidyselect::ends_with(&quot;_plots&quot;))) check our new table of training/evaluation data by NEON site after we augmented the training data lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( n = dplyr::n() , training_plots = sum(ifelse(data_type==&quot;training&quot;,1,0)) # validation type , ann_plots = sum(ifelse(has_image_annotation,1,0)) , fld_plots = sum(ifelse(has_field_stems,1,0)) # training/eval , training_ann_plots = sum(ifelse(has_image_annotation &amp; (is_new_image_annotation_training | data_type==&quot;training&quot;),1,0)) , training_fld_plots = sum(ifelse(has_field_stems &amp; (is_new_field_stems_training | data_type==&quot;training&quot;),1,0)) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( # training/eval eval_ann_plots = ann_plots-training_ann_plots , eval_fld_plots = fld_plots-training_fld_plots ) %&gt;% dplyr::arrange(desc(n)) %&gt;% dplyr::select(-c(n,ann_plots,fld_plots)) %&gt;% dplyr::relocate( c(tidyselect::ends_with(&quot;ann_plots&quot;),tidyselect::ends_with(&quot;fld_plots&quot;)) ,.after = dplyr::last_col() ) %&gt;% kableExtra::kbl( caption = &quot;NEON sites with conifers and lidar plots using augmented training data&quot; , col.names = c( &quot;site&quot;,&quot;original&lt;br&gt;training plots&quot; ,&quot;training plots&quot;,&quot;evaluation plots&quot; ,&quot;training plots&quot;,&quot;evaluation plots&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::add_header_above( c(&quot; &quot;=2, &quot;image annotated crowns&quot; = 2, &quot;field collected stems&quot; = 2) ) Table 4.4: NEON sites with conifers and lidar plots using augmented training data image annotated crowns field collected stems site originaltraining plots training plots evaluation plots training plots evaluation plots OSBS 3 3 14 2 44 TEAK 1 1 51 0 0 NIWO 1 1 10 2 39 JERC 1 1 6 2 19 TALL 0 0 2 2 20 ABBY 0 0 2 2 11 DEJU 0 0 0 2 12 DSNY 0 2 4 0 0 MOAB 0 0 0 0 2 SOAP 0 0 2 0 0 ONAQ 0 0 1 0 0 4.3 Height threshold for “canopy” trees We’ll process the point cloud and get a tree list using our cloud2trees::cloud2trees() method with all default settings except for the ITD window function (see section below). However, an important consideration in the evaluation of our method is that the Weinstein et al. 2021 benchmark was developed specifically for “canopy” trees and the field-collected stems evaluation data only includes &gt;10 cm DBH trees: NEON field crews sample all trees within a plot that are greater than 10cm DBH, regardless of whether the tree crown can be seen in the remote sensing image data. While understory tree detection is an important area of future work, the scope of this benchmark is focused on crowns in the canopy that are visible from above. (p. 10) In order to align our point cloud-based algorithm with the evaluation data inclusion of only “canopy” trees, we’ll identify the shortest live tree in the field-collected stems evaluation data by NEON site. We’ll use this value to filter our cloud2trees::cloud2trees() tree list in post-processing. This filtering process is analogous to setting a diameter threshold during a forest inventory (e.g. &gt;10 cm DBH), meaning only trees with a DBH of 10 cm, for example, or greater are tallied and measured which a common practice because it focuses on trees that are potentially merchantable. In working with the NeonTreeEvaluation::field data we’ll use the filters found in clean_field_data() from NeonTreeEvaluation as an internal function # set the percentile to determine the shortest live tree in the field-collected stems evaluation data # using a percentile helps to avoid outlier measurements that might occur if we used a minimum value percentile_for_ht &lt;- 0.05 # get non-na heights from neon field measured trees neon_field_heights &lt;- NeonTreeEvaluation::field %&gt;% # filters found in `clean_field_data()` clean_field_data() %&gt;% dplyr::filter(!is.na(height) &amp; height&gt;3) %&gt;% # and finally, filter for our conifer sites only dplyr::inner_join( conifer_sites %&gt;% dplyr::filter(pct_conifer&gt;min_conifer_pct) ) # get 5th tile ht by site neon_site_heights &lt;- neon_field_heights %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( site_prcntl_ht = quantile(floor(height), probs = percentile_for_ht, na.rm = T) ) %&gt;% dplyr::ungroup() look at the summary of height data across all sites and plots summary(neon_field_heights$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.10 9.60 12.80 13.72 16.40 53.90 what does this look like for each NEON site? neon_field_heights %&gt;% dplyr::inner_join(neon_site_heights, by = &quot;siteID&quot;) %&gt;% ggplot2::ggplot(ggplot2::aes(x = height, group = siteID)) + ggplot2::geom_density(color = &quot;gold&quot;,fill = &quot;gold&quot;, alpha = 0.7, lwd = 1.2) + ggplot2::geom_vline(aes(xintercept = site_prcntl_ht), linetype = &quot;dashed&quot;) + ggplot2::facet_wrap(facets = vars(siteID), ncol = 6, scales = &quot;free&quot;) + ggplot2::scale_x_continuous(breaks = scales::breaks_extended(6)) + ggplot2::labs( x = &quot;height (m)&quot;, y = &quot;&quot; , subtitle = paste0( &quot;field-collected heights of \\&quot;canopy\\&quot; trees in conifer NEON sites with &quot; , scales::number(percentile_for_ht*100, accuracy = 1) , &quot;th percentile by site&quot; ) ) + ggplot2::theme_light() + ggplot2::theme( axis.text.y = ggplot2::element_blank() , axis.ticks.y = ggplot2::element_blank() , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) what is the overall 5th percentile to use for minimum height? (neon_min_ht &lt;- quantile(neon_field_heights$height, probs = percentile_for_ht) %&gt;% floor()) ## 5% ## 5 let’s attach this to our data frame of lidar data files lidar_df &lt;- lidar_df %&gt;% dplyr::left_join(neon_site_heights, by = &quot;siteID&quot;) %&gt;% dplyr::mutate( site_prcntl_ht = dplyr::coalesce(site_prcntl_ht,neon_min_ht) , plot_name_sans_yr = dplyr::case_when( data_type == &quot;training&quot; ~ NA , T ~ stringr::str_match(plot_name,&quot;(\\\\w+)_\\\\d+&quot;)[,2] ) ) # dplyr::glimpse(lidar_df) # save readr::write_csv(lidar_df, &quot;../data/NeonTreeEvaluation_lidar_df.csv&quot;, append = F, progress = F) 4.4 Example validation process now that we have our lidar data and post-processing height filter that we can test our point cloud-based tree detection and crown segmentation process against, let’s walk through the validation for a single point cloud we’ll test with a single evaluation point cloud lidar_df_row &lt;- lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::mutate( rn = ifelse( data_type==&quot;evaluation&quot; &amp; has_image_annotation &amp; has_field_stems ,dplyr::row_number() ,NA )) %&gt;% dplyr::filter(!is.na(rn)) %&gt;% dplyr::slice(1) %&gt;% dplyr::pull(rn) # lidar_df$f_path[lidar_df_row] %&gt;% lidR::readLAS() %&gt;% lidR::st_crs() 4.4.1 Preliminaries 4.4.1.1 View the point cloud This step isn’t necessary for validation, but let’s see what this point cloud data looks like. We can plot the point cloud with and color by the point height lidar_df$f_path[lidar_df_row] %&gt;% lidR::readLAS() %&gt;% lidR::plot( color = &quot;Z&quot;, breaks = &quot;quantile&quot;, bg = &quot;white&quot;, legend = T , pal = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) ) there are trees in there for sure (and conifer trees by the looks of it) let’s look at the RGB imagery and image annotated crowns (notice that the NeonTreeEvaluation commands rely on the deprecated raster package :/ ) # read rgb rgb_temp &lt;- lidar_df$plot_name[lidar_df_row] %&gt;% NeonTreeEvaluation::get_data(type = &quot;rgb&quot;) %&gt;% raster::stack() # read image annotated crown data and make polygons polys_temp &lt;- lidar_df$plot_name[lidar_df_row] %&gt;% NeonTreeEvaluation::get_data(type = &quot;annotations&quot;) %&gt;% NeonTreeEvaluation::xml_parse() polys_temp &lt;- NeonTreeEvaluation::boxes_to_spatial_polygons(polys_temp,rgb_temp) # plot terra::plotRGB(rgb_temp %&gt;% terra::rast()) terra::plot( polys_temp %&gt;% terra::vect() , col = NA, border = &quot;red&quot; , lwd = 2 , add = TRUE ) 4.4.2 cloud2trees::cloud2trees() for our example, we’ll use all default settings in cloud2trees::cloud2trees() to get a point cloud-detected tree list ans &lt;- cloud2trees::cloud2trees( input_las_dir = lidar_df$f_path[lidar_df_row] , output_dir = tempdir() ) # filter the tree list based on the height threshold ans$crowns_sf &lt;- ans$crowns_sf %&gt;% dplyr::filter(tree_height_m &gt;= lidar_df$site_prcntl_ht[lidar_df_row]) quick check of our heights ans$crowns_sf$tree_height_m %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.068 5.632 7.182 6.943 8.411 9.706 4.4.3 Format extracted tree polygons we need to format our extracted trees for NeonTreeEvaluation evaluation and submission first, we’ll simplify multipolygon crowns ans$crowns_sf &lt;- cloud2trees::simplify_multipolygon_crowns(ans$crowns_sf) check out our extracted trees ggplot2::ggplot() + ggplot2::geom_tile( data = ans$chm_rast %&gt;% terra::as.data.frame(xy=T) %&gt;% dplyr::rename(f=3) , mapping = ggplot2::aes(x = x, y = y, fill = f) , na.rm = T ) + harrypotter::scale_fill_hp( option = &quot;gryffindor&quot; , breaks = scales::breaks_extended(n=10) ) + ggplot2::geom_sf( data = ans$crowns_sf , fill = NA, color = &quot;gray44&quot;, lwd = 1.2 ) + ggplot2::scale_x_continuous(expand = c(0, 0)) + ggplot2::scale_y_continuous(expand = c(0, 0)) + ggplot2::labs(x = &quot;&quot;, y = &quot;&quot;, fill = &quot;CHM (m)&quot;) + ggplot2::theme_light() + ggplot2::theme(axis.text = ggplot2::element_blank()) we’ll reserve judgement and let the data talk format the data for NeonTreeEvaluation submission and evaluation return_sf &lt;- ans$crowns_sf %&gt;% sf::st_set_geometry(&quot;geometry&quot;) %&gt;% dplyr::rowwise(&quot;treeID&quot;) %&gt;% dplyr::mutate( xmin = sf::st_bbox(geometry)[1] , ymin = sf::st_bbox(geometry)[2] , xmax = sf::st_bbox(geometry)[3] , ymax = sf::st_bbox(geometry)[4] ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( label = &quot;Tree&quot; , plot_name = lidar_df$plot_name[lidar_df_row] ) %&gt;% dplyr::select(xmin,xmax,ymin,ymax,label,plot_name) %&gt;% sf::st_set_crs(NA) # what? return_sf %&gt;% dplyr::glimpse() ## Rows: 74 ## Columns: 7 ## $ xmin &lt;dbl&gt; 552095.5, 552089.8, 552081.2, 552085.2, 552084.2, 552080.2, … ## $ xmax &lt;dbl&gt; 552099.5, 552093.0, 552085.2, 552088.2, 552087.2, 552084.5, … ## $ ymin &lt;dbl&gt; 5067679, 5067678, 5067677, 5067677, 5067674, 5067674, 506767… ## $ ymax &lt;dbl&gt; 5067681, 5067681, 5067680, 5067679, 5067679, 5067678, 506767… ## $ label &lt;chr&gt; &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tre… ## $ plot_name &lt;chr&gt; &quot;ABBY_063_2019&quot;, &quot;ABBY_063_2019&quot;, &quot;ABBY_063_2019&quot;, &quot;ABBY_063… ## $ geometry &lt;POLYGON&gt; POLYGON ((552095.8 5067681,..., POLYGON ((552090.8 50676… does this match the submission polygon data from the NeonTreeEvaluation package? NeonTreeEvaluation::submission_polygons %&gt;% dplyr::glimpse() ## Rows: 126,574 ## Columns: 8 ## $ xmin &lt;dbl&gt; 41.01716, 357.32129, 30.39723, 260.65921, 179.34564, 316.2… ## $ ymin &lt;dbl&gt; 230.8854218, 122.1164017, 136.9156647, 285.6688843, 371.61… ## $ xmax &lt;dbl&gt; 151.08607, 397.57458, 73.79434, 299.68811, 232.49385, 363.… ## $ ymax &lt;dbl&gt; 342.69846, 159.37578, 184.94730, 326.79330, 400.00000, 400… ## $ score &lt;dbl&gt; 0.8098674, 0.6968824, 0.5713338, 0.5511004, 0.4697072, 0.3… ## $ label &lt;chr&gt; &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;T… ## $ plot_name &lt;chr&gt; &quot;DSNY_014_2019&quot;, &quot;DSNY_014_2019&quot;, &quot;DSNY_014_2019&quot;, &quot;DSNY_0… ## $ st_sfc.lst. &lt;POLYGON&gt; POLYGON ((41.01716 230.8854..., POLYGON ((357.3213 122… yes, except for the “score” column which I’m pretty sure is an artifact from after evaluation? 4.4.4 Test evaluation We compared our tree detection and crown delineation results to the three types of evaluation data (i.e. “ground truth” data) presented by Weinstein et al. (2021): field-collected stems, image-annotated crowns, and field-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective, and field-annotated crowns combine the benefits of both but are highly resource-intensive. 4.4.4.1 Scores for image-annotated crowns The main data source are image-annotated crowns, in which a single observer annotated visible trees in 200 40m x 40m images from across the United States. Get the benchmark score image-annotated “ground truth” data. in testing, including the sf polygon data did not work…switching to the bbox method with sf::st_drop_geometry() rslt_img_annttd_crwns &lt;- NeonTreeEvaluation::evaluate_image_crowns( predictions = return_sf %&gt;% sf::st_drop_geometry() , show = T , summarize = T ) ## [1] &quot;ABBY_063_2019&quot; in the plot (if there is a plot), “red” boxes are crowns our point cloud-based method extracted and “black” are the image annotated crowns it looks like the overlay is generally the same but we are still extracting trees that may not be considered “canopy” trees ; what is in the return from NeonTreeEvaluation::evaluate_image_crowns() ? rslt_img_annttd_crwns %&gt;% names() ## [1] &quot;overall&quot; &quot;by_site&quot; &quot;plot_level&quot; &quot;count_error&quot; overall: must be across all NEON sites, plots, and trees included for evaluation rslt_img_annttd_crwns$overall ## # A tibble: 1 × 2 ## precision recall ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.554 0.719 by_site: must be across plots, and trees included for evaluation in a NEON sites rslt_img_annttd_crwns$by_site ## # A tibble: 1 × 3 ## # Groups: Site [1] ## Site recall precision ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABBY 0.719 0.554 plot_level: must be across trees included for evaluation in a NEON site, plot combination rslt_img_annttd_crwns$plot_level ## # A tibble: 1 × 3 ## # Groups: plot_name [1] ## plot_name recall precision ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABBY_063_2019 0.719 0.554 count_error is the number of predicted trees minus the number of “ground truth” trees but in graphical form so we’ll skip it 4.4.4.2 Scores for field-annotated crowns The second data source is a small number of field-annotated crowns from two geographic sites. These crowns were drawn on a tablet while physically standing in the field, thereby reducing the uncertainty in crown segmentation. not all plots have field-annotated crowns NeonTreeEvaluation::evaluate_field_crowns() returns an error if “No plot names matching the field crown data, see list_field_crowns for paths to RGB field crown imagery.”… so, we’ll have to capture errors in our checks? # safe it safe_evaluate_field_crowns &lt;- purrr::safely(NeonTreeEvaluation::evaluate_field_crowns) # test it rslt_fld_crwns &lt;- safe_evaluate_field_crowns( predictions = return_sf %&gt;% sf::st_drop_geometry() , show = T , summarize = T ) did it do it? rslt_fld_crwns$error ## &lt;simpleError in .f(...): No plot names matching the field crown data, see list_field_crowns for paths to RGB field crown imagery.&gt; nope! 4.4.4.3 Scores for field-collected stems The third data source is the NEON Woody Vegetation Structure Dataset. Each tree stem is represented by a single point. This data has been filtered to represent overstory trees visible in the remote sensing imagery. not all plots have field-collected stems NeonTreeEvaluation::evaluate_field_stems() returns an error if “No submitted plot_names with matching field stem data, see list_field_stems()”… so, we’ll have to capture errors in our checks? # safe it safe_evaluate_field_stems &lt;- purrr::safely(NeonTreeEvaluation::evaluate_field_stems) # test it rslt_fld_stems &lt;- safe_evaluate_field_stems( predictions = return_sf %&gt;% sf::st_drop_geometry() , show = T , summarize = T ) ## [1] &quot;ABBY_063&quot; did it do it? rslt_fld_stems$error ## &lt;error/rlang_error&gt; ## Error in `summarize()`: ## ℹ In argument: `recall = mean(recall)`. ## Caused by error: ## ! object &#39;recall&#39; not found ## --- ## Backtrace: ## ▆ ## 1. ├─purrr (local) safe_evaluate_field_stems(...) ## 2. │ ├─purrr:::capture_error(.f(...), otherwise, quiet) ## 3. │ │ └─base::tryCatch(...) ## 4. │ │ └─base (local) tryCatchList(expr, classes, parentenv, handlers) ## 5. │ │ └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]]) ## 6. │ │ └─base (local) doTryCatch(return(expr), name, parentenv, handler) ## 7. │ └─NeonTreeEvaluation (local) .f(...) ## 8. │ └─results %&gt;% summarize(recall = mean(recall)) ## 9. ├─dplyr::summarize(., recall = mean(recall)) ## 10. ├─dplyr:::summarise.data.frame(., recall = mean(recall)) ## 11. │ └─dplyr:::summarise_cols(.data, dplyr_quosures(...), by, &quot;summarise&quot;) ## 12. │ ├─base::withCallingHandlers(...) ## 13. │ └─dplyr:::map(quosures, summarise_eval_one, mask = mask) ## 14. │ └─base::lapply(.x, .f, ...) ## 15. │ └─dplyr (local) FUN(X[[i]], ...) ## 16. │ └─mask$eval_all_summarise(quo) ## 17. │ └─dplyr (local) eval() ## 18. └─base::mean(recall) nope! 4.5 ITD tuning Before we perform tree extraction on the evaluation data we’ll use the training data to tune our process for lidar-based tree detection implemented via the [cloud2trees]https://github.com/georgewoolsey/cloud2trees) package. Specifically, we are going to use the training data to determine the “best” ITD window function to use for a given NEON site since the cloud2trees package strongly recommends that “a different window size function is defined for each region of your study area with significantly different forest structure”. See the lidR package book section by point cloud processing expert Jean-Romain Roussel for excellent detail on ITD and defining window size. 4.5.1 Default variable window functions We discussed our method for individual tree detection (ITD) in this prior section. For the purpose of working across different conifer sites with potentially vastly different forest structures we are going to augment the default ITD variable window size functions in cloud2trees::itd_ws_functions() Let’s see what the default variable window functions look like # get ws by ht for each fn ws_fn_df &lt;- 1:length(cloud2trees::itd_ws_functions()) %&gt;% purrr::map(function(x){ nm &lt;- cloud2trees::itd_ws_functions()[x] %&gt;% names() %&gt;% as.character() fn &lt;- cloud2trees::itd_ws_functions()[[x]] # est height &lt;- seq(from=0,to=60,by=0.5) ws &lt;- fn(height) %&gt;% unlist() df &lt;- dplyr::tibble( height = height , ws = ws ) %&gt;% dplyr::mutate(ws_fn_nm = nm) %&gt;% dplyr::relocate(ws_fn_nm) return(df) }) %&gt;% dplyr::bind_rows() # huh? ws_fn_df %&gt;% dplyr::glimpse() ## Rows: 363 ## Columns: 3 ## $ ws_fn_nm &lt;chr&gt; &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;… ## $ height &lt;dbl&gt; 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6… ## $ ws &lt;dbl&gt; 1.00, 1.00, 1.00, 1.00, 1.03, 1.10, 1.17, 1.24, 1.31, 1.38, 1… plot of cloud2trees default ITD variable window function # define custom colors pal_ws &lt;- c( RColorBrewer::brewer.pal(n = 7, name = &quot;Purples&quot;)[2:6] %&gt;% rev() , RColorBrewer::brewer.pal(n = 11, name = &quot;BrBG&quot;)[7:11] %&gt;% rev() , RColorBrewer::brewer.pal(n = 7, name = &quot;Oranges&quot;)[2:6] %&gt;% rev() , RColorBrewer::brewer.pal(n = 7, name = &quot;Greys&quot;)[2:6] %&gt;% rev() ) # scales::show_col(pal_ws) # plot ws_fn_df %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = height, y = ws, color = ws_fn_nm)) + ggplot2::geom_line(lwd=1) + ggplot2::scale_color_manual(values = pal_ws[c(1,6,11)]) + ggplot2::xlim(-3,NA) + ggplot2::ylim(-0.1,NA) + ggplot2::labs( x = &quot;heights&quot;, y = &quot;ws&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;`cloud2trees` default ITD variable window function&quot; ) + ggplot2::theme_light() + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(lwd = 6)) ) 4.5.2 Augmented variable window functions now we’ll augment the default functions to make: the exponential function (concave up) have slightly less (“les”), less (“less”), much less (“lesss”) and slightly more (“mor”) concavity (4 new functions) the logarithmic function (concave down) have slightly more (“mor”), more (“morr”), slightly less (“les”), and less (“less”) concavity (4 new functions) the linear function have a higher and lower intercept and a higher and lower slope (4 new functions) for a total of 15 functions tested # set up initial list with default functions my_ws_functions &lt;- cloud2trees::itd_ws_functions() # add to list my_ws_functions$log_les_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; exp(5)-1 ~ 5 , TRUE ~ log(x+1) ) return(y) } # add to list my_ws_functions$log_less_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; exp(3/0.5)-1 ~ 3 , TRUE ~ 0.5*log(x+1) ) return(y) } # add to list my_ws_functions$log_morr_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; exp(9/2)-1 ~ 9 , TRUE ~ 2*log(x+1) ) return(y) } # add to list my_ws_functions$log_mor_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; exp(7/1.5)-1 ~ 7 , TRUE ~ 1.5*log(x+1) ) return(y) } # add to list my_ws_functions$exp_mor_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; log(8+1)/log(1.1) ~ 8 , TRUE ~ (1.1^x)-1 ) return(y) } # add to list my_ws_functions$exp_lesss_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; log(5+1)/log(1.015) ~ 5 , TRUE ~ (1.015^x)-1 ) return(y) } # add to list my_ws_functions$exp_less_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; log(5+1)/log(1.03) ~ 5 , TRUE ~ (1.03^x)-1 ) return(y) } # add to list my_ws_functions$exp_les_ccv_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; log(5+1)/log(1.05) ~ 5 , TRUE ~ (1.05^x)-1 ) return(y) } # add to list my_ws_functions$lin_hi_int_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; (8-1.2)/0.14 ~ 8 , TRUE ~ 1.2 + (x * 0.14) ) return(y) } # add to list my_ws_functions$lin_lo_int_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; (0.5+2)/0.14 ~ 0.5 , x &gt; (6+2)/0.14 ~ 6 , TRUE ~ -2 + (x * 0.14) ) return(y) } # add to list my_ws_functions$lin_hi_slp_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; (8-0.75)/0.3 ~ 8 , TRUE ~ 0.75 + (x * 0.3) ) return(y) } # add to list my_ws_functions$lin_lo_slp_fn &lt;- function (x) { y &lt;- dplyr::case_when( is.na(x) ~ 0.001 , x &lt; 0 ~ 0.001 , x &gt; (4-0.75)/0.04 ~ 4 , TRUE ~ 0.75 + (x * 0.04) ) return(y) } run each function over a range of heights to see what they return on a plot # get ws by ht for each fn ws_fn_df &lt;- 1:length(my_ws_functions) %&gt;% purrr::map(function(x){ nm &lt;- my_ws_functions[x] %&gt;% names() %&gt;% as.character() fn &lt;- my_ws_functions[[x]] # est height &lt;- seq(from=0,to=60,by=0.5) ws &lt;- fn(height) %&gt;% unlist() df &lt;- dplyr::tibble( height = height , ws = ws ) %&gt;% dplyr::mutate(ws_fn_nm = nm) %&gt;% dplyr::relocate(ws_fn_nm) return(df) }) %&gt;% dplyr::bind_rows() # huh? ws_fn_df %&gt;% dplyr::glimpse() ## Rows: 1,815 ## Columns: 3 ## $ ws_fn_nm &lt;chr&gt; &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;… ## $ height &lt;dbl&gt; 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6… ## $ ws &lt;dbl&gt; 1.00, 1.00, 1.00, 1.00, 1.03, 1.10, 1.17, 1.24, 1.31, 1.38, 1… plot of all ITD variable window functions for testing ws_fn_df %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(x = height, y = ws, color = ws_fn_nm)) + ggplot2::geom_line(lwd=1) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::xlim(-3,NA) + ggplot2::ylim(-0.1,NA) + ggplot2::labs( x = &quot;heights&quot;, y = &quot;ws&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;ITD variable window functions for testing&quot; ) + ggplot2::theme_light() + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(lwd = 6)) ) 4.6 Training validation process Now that we have been through the validation process for a single sample plot and defined different variable window functions for use in ITD, we’ll create a function to detect trees and delineate tree crowns using our cloud2trees::cloud2trees() method to apply over each lidar data set. We’ll extract trees on each training data set using all 15 variable window functions, compare our point cloud-detected trees to the image annotated crowns (i.e. ground truth), and store the best variable window function by NEON site for use in validation on the evaluation lidar data. If a NEON site does not have training data, the best variable window function across NEON sites will be used. 4.6.1 Summary of training lidar data Let’s get a quick summary of the lidar data available for training. Note, we expect our augmented training data sets to be sized around 40m x 40m while the original training data sets will cover a larger area. training_lidar_sum_temp &lt;- lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(data_type==&quot;training&quot; | is_new_image_annotation_training | is_new_field_stems_training) %&gt;% dplyr::pull(f_path) %&gt;% purrr::map(function(x){ ctg &lt;- lidR::readLAScatalog(x) ctg@data %&gt;% dplyr::summarise( num_points = sum(Number.of.point.records) , area_m2 = sum(as.numeric(sf::st_area(.))) ) %&gt;% dplyr::mutate(points_per_m2 = num_points/area_m2) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::mutate( plot_name = x %&gt;% basename() %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;) ) %&gt;% dplyr::relocate(plot_name) }) %&gt;% dplyr::bind_rows() # check it training_lidar_sum_temp %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(plot_name,siteID) , by = &quot;plot_name&quot; ) %&gt;% dplyr::mutate(dplyr::across(dplyr::where(is.numeric),~scales::comma(.x,accuracy=.1))) %&gt;% dplyr::relocate(siteID) %&gt;% dplyr::arrange(siteID,plot_name) %&gt;% kableExtra::kbl( caption = &quot;summary of training lidar data&quot; , digits = 1 , col.names = c( &quot;site&quot;,&quot;plot&quot;,&quot;# points&quot;,&quot;area m&lt;sup&gt;2&lt;/sup&gt;&quot;, &quot;points&lt;br&gt;per m&lt;sup&gt;2&lt;/sup&gt;&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::scroll_box(height = &quot;622px&quot;,fixed_thead = T) %&gt;% kableExtra::collapse_rows(columns = 1, valign = &quot;top&quot;) Table 4.5: summary of training lidar data site plot # points area m2 pointsper m2 ABBY ABBY_065_2018 29,310.0 1,599.8 18.3 ABBY_075_2018 21,142.0 1,599.8 13.2 DEJU DEJU_014_2018 11,293.0 1,599.2 7.1 DEJU_016_2019 9,292.0 1,599.2 5.8 DSNY DSNY_005_2018 11,215.0 1,599.2 7.0 DSNY_025_2018 6,559.0 1,599.2 4.1 JERC 2018_JERC_4_742000_3451000_image_crop 63,925.0 16,129.8 4.0 JERC_049_2018 17,010.0 1,599.2 10.6 JERC_063_2018 4,299.0 1,597.6 2.7 NIWO 2018_NIWO_2_450000_4426000_image_crop 2,101,996.0 103,753.9 20.3 NIWO_009_2020 8,899.0 1,599.2 5.6 NIWO_040_2020 10,535.0 1,599.2 6.6 OSBS 2018_OSBS_4_405000_3286000_image 3,813,514.0 999,980.0 3.8 2019_OSBS_5_405000_3287000_image_crop 145,549.0 30,315.4 4.8 2019_OSBS_5_405000_3287000_image_crop2 237,076.0 59,741.7 4.0 OSBS_004_2019 13,684.0 1,599.8 8.6 OSBS_035_2019 9,981.0 1,599.3 6.2 TALL TALL_053_2018 22,711.0 1,599.6 14.2 TALL_061_2019 8,117.0 1,598.4 5.1 TEAK 2018_TEAK_3_315000_4094000_image_crop 2,817,056.0 359,998.8 7.8 note the out-of-box training plots are much larger – that’s why we didn’t augment the training set for these sites – and check out the spread in point density do the 2019 “OSBS” point clouds overlap? ggplot2::ggplot() + ggplot2::geom_sf( data = lidR::readLAScatalog( lidar_df %&gt;% dplyr::filter(plot_name == &quot;2019_OSBS_5_405000_3287000_image_crop&quot;) %&gt;% dplyr::pull(f_path) ) %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% dplyr::mutate(plot_name = basename(filename) %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;)) , ggplot2::aes(color = plot_name) , fill = NA, lwd = 2 ) + ggplot2::geom_sf( data = lidR::readLAScatalog( lidar_df %&gt;% dplyr::filter(plot_name == &quot;2019_OSBS_5_405000_3287000_image_crop2&quot;) %&gt;% dplyr::pull(f_path) ) %&gt;% purrr::pluck(&quot;data&quot;) %&gt;% dplyr::mutate(plot_name = basename(filename) %&gt;% stringr::str_remove_all(&quot;\\\\.(laz|las)$&quot;)) , ggplot2::aes(color = plot_name) , fill = NA, lwd = 2 ) + ggplot2::scale_color_manual(values = c(&quot;navy&quot;,&quot;gold&quot;)) + ggplot2::labs(color=&quot;&quot;) + ggplot2::theme_light() + ggplot2::theme(legend.position = &quot;top&quot;,axis.text = ggplot2::element_text(size=6)) + ggplot2::guides(col = ggplot2::guide_legend(ncol = 1)) no let’s look at the RGB imagery and image annotated crowns (notice that the NeonTreeEvaluation commands rely on the deprecated raster package :) neontree_plot_fn &lt;- function(plot_name,lwd=2) { # read rgb rgb_temp &lt;- NeonTreeEvaluation::get_data(plot_name = plot_name, type = &quot;rgb&quot;) %&gt;% raster::stack() # read image annotated crown data and make polygons polys_temp &lt;- NeonTreeEvaluation::get_data(plot_name = plot_name, type = &quot;annotations&quot;) %&gt;% NeonTreeEvaluation::xml_parse() polys_temp &lt;- NeonTreeEvaluation::boxes_to_spatial_polygons(polys_temp,rgb_temp) # plot terra::plotRGB(rgb_temp %&gt;% terra::rast(), main = plot_name) terra::plot( polys_temp %&gt;% terra::vect() , col = NA, border = &quot;red&quot; , lwd = lwd , add = TRUE ) } # 2019_OSBS_5_405000_3287000_image_crop neontree_plot_fn(&quot;2019_OSBS_5_405000_3287000_image_crop&quot;) # 2019_OSBS_5_405000_3287000_image_crop2 neontree_plot_fn(&quot;2019_OSBS_5_405000_3287000_image_crop2&quot;) 4.6.2 Function to extract trees let’s create a function to extract trees and format for evaluation using our lidar_df data which includes a data frame of file paths with the appropriate plot name cloud2trees_for_eval &lt;- function(lidar_df_row, lidar_df, ws) { # message message(paste0(&quot;doing the work for ...... &quot;, lidar_df$plot_name[lidar_df_row])) # run c2t qc2t &lt;- purrr::safely(cloud2trees::cloud2trees) ans &lt;- qc2t( input_las_dir = lidar_df$f_path[lidar_df_row] , output_dir = tempdir() , ws = ws ) if(!is.null(ans$error)){return(NULL)} ans &lt;- ans$result # simp ans$crowns_sf &lt;- cloud2trees::simplify_multipolygon_crowns(ans$crowns_sf) # return return_sf &lt;- ans$crowns_sf %&gt;% sf::st_set_geometry(&quot;geometry&quot;) %&gt;% dplyr::rowwise(&quot;treeID&quot;) %&gt;% dplyr::mutate( xmin = sf::st_bbox(geometry)[1] , ymin = sf::st_bbox(geometry)[2] , xmax = sf::st_bbox(geometry)[3] , ymax = sf::st_bbox(geometry)[4] ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( label = &quot;Tree&quot; , plot_name = lidar_df$plot_name[lidar_df_row] ) %&gt;% dplyr::select(xmin,xmax,ymin,ymax,label,plot_name,tree_height_m) %&gt;% sf::st_set_crs(NA) # give the workers a rest Sys.sleep(2) return(return_sf) } # function to apply cloud2trees_for_eval over each ws ws_fn_eval &lt;- function(lidar_df, lidar_df_row, ws_fn_list) { 1:length(ws_fn_list) %&gt;% purrr::map(function(x){ ans &lt;- cloud2trees_for_eval( lidar_df_row = lidar_df_row , lidar_df = lidar_df , ws = ws_fn_list[[x]] ) ### some of these ws functions may not extract any trees ### account for this if(is.null(ans) || !inherits(ans,&quot;data.frame&quot;)){ return(NULL) }else{ return( ans %&gt;% dplyr::mutate(ws_fn_nm = ws_fn_list[x] %&gt;% names() %&gt;% as.character()) ) } }) %&gt;% dplyr::bind_rows() } 4.6.3 Extract trees for training data Now we’ll extract trees from the training lidar data using each variable window function # where should we save the file? submission_fn &lt;- &quot;../data/NeonTreeEvaluation_training_submission.gpkg&quot; # if we don&#39;t already have the data, run it if(!file.exists(submission_fn)){ # if(T){ ### !!!!!!! take out when it gets real # just get training training_df_temp &lt;- lidar_df %&gt;% dplyr::filter(data_type == &quot;training&quot; | is_new_image_annotation_training | is_new_field_stems_training) # get trees for each training plot and ws fn combination training_submission &lt;- 1:nrow(training_df_temp) %&gt;% purrr::map(\\(x) ws_fn_eval( lidar_df = training_df_temp , lidar_df_row = x , ws_fn_list = my_ws_functions ) ) %&gt;% dplyr::bind_rows() # save it sf::st_write(training_submission, submission_fn, append = F) }else{ training_submission &lt;- sf::st_read(submission_fn, quiet = T) } what did we get? training_submission %&gt;% dplyr::glimpse() ## Rows: 1,676,534 ## Columns: 9 ## $ xmin &lt;dbl&gt; 551846.5, 551848.5, 551856.2, 551865.0, 551866.2, 551875… ## $ xmax &lt;dbl&gt; 551848.8, 551849.8, 551857.0, 551866.2, 551867.5, 551877… ## $ ymin &lt;dbl&gt; 5067861, 5067862, 5067862, 5067862, 5067861, 5067862, 50… ## $ ymax &lt;dbl&gt; 5067863, 5067863, 5067863, 5067863, 5067863, 5067863, 50… ## $ label &lt;chr&gt; &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, … ## $ plot_name &lt;chr&gt; &quot;ABBY_065_2018&quot;, &quot;ABBY_065_2018&quot;, &quot;ABBY_065_2018&quot;, &quot;ABBY… ## $ tree_height_m &lt;dbl&gt; 5.064, 3.792, 2.466, 5.998, 3.700, 4.503, 2.574, 6.829, … ## $ ws_fn_nm &lt;chr&gt; &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_fn&quot;, &quot;lin_f… ## $ geom &lt;POLYGON&gt; POLYGON ((551846.5 5067863,..., POLYGON ((551848.5 5… now we need to filter our tree list based on the height threshold for “canopy” trees training_submission &lt;- training_submission %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter( data_type==&quot;training&quot; | is_new_image_annotation_training | is_new_field_stems_training ) %&gt;% dplyr::select(plot_name,site_prcntl_ht) , by = &quot;plot_name&quot; ) %&gt;% dplyr::filter(tree_height_m&gt;=site_prcntl_ht) %&gt;% dplyr::select(-c(tree_height_m,site_prcntl_ht)) tabulate detected “canopy” trees by plot and variable window function and plot it training_submission %&gt;% sf::st_drop_geometry() %&gt;% dplyr::count(plot_name,ws_fn_nm) %&gt;% ggplot2::ggplot(ggplot2::aes(x = n, y = plot_name, color = ws_fn_nm)) + ggplot2::geom_point(size = 5) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::scale_x_log10(labels=scales::comma) + ggplot2::labs( x = &quot;# trees&quot;, y = &quot;&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Training data: number of detected canopy trees&quot; ) + ggplot2::theme_light() + ggplot2::theme( axis.text.y = ggplot2::element_text(size=8) , panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) ) + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) ) 4.6.4 Training: validation against ground truth We’ll compare our aerial point cloud-based tree detection and crown delineation results to two types of evaluation data (i.e. “ground truth” data) presented by (Weinstein et al. 2021): field-collected stems and image-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective. Precision quantifies the proportion of correctly identified tree crowns among all detected crowns (how many of the trees it found were real trees). Recall measures the proportion of correctly identified crowns relative to the total image-annotated tree crowns (how many of the actual trees it managed to find). NeonTreeEvaluation::evaluate_image_crowns() is used to evaluate our data against the image-annotated crowns ground truth. This function returns precision and recall per image (i.e. “plot”). NeonTreeEvaluation::evaluate_field_stems() is used to evaluate our data against the field collected stems ground truth. This function returns recall – the proportion of field stems that fall within a single predicted crown – per image (i.e. “plot”). We will apply both evaluation methods over each variable window function tested for each plot marked as training data. 4.6.4.1 Scores for image-annotated crowns Get the benchmark score compared to image-annotated “ground truth” data. in testing, including the sf polygon data did not work…switching to the bbox method with sf::st_drop_geometry() # where should we save the file? training_eval_temp &lt;- &quot;../data/NeonTreeEvaluation_training_evaluate_image_crowns.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(training_eval_temp)){ # if(T){ # filter for only those with this type of validation data submission_temp &lt;- training_submission %&gt;% ## !!!!!!!!!!!! do these separate b/c memory crash :/ ## hate that this is manual but i&#39;m.....tired dplyr::filter( !plot_name %in% c(&quot;2018_NIWO_2_450000_4426000_image_crop&quot;,&quot;2018_TEAK_3_315000_4094000_image_crop&quot;) ) %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_image_annotation==T &amp; (data_type == &quot;training&quot; | is_new_image_annotation_training)) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported # would need to dig through NeonTreeEvaluation, fix, and pull request... safe_evaluate_image_crowns &lt;- purrr::safely(NeonTreeEvaluation::evaluate_image_crowns) # validate training_img_annttd_crwns &lt;- submission_temp$ws_fn_nm %&gt;% unique() %&gt;% purrr::map(function(x){ ans &lt;- safe_evaluate_image_crowns( predictions = submission_temp %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(ws_fn_nm == x) %&gt;% dplyr::select(xmin,ymin,xmax,ymax,label,plot_name) , show = F , summarize = T ) if(!is.null(ans$error)){ return(NULL) }else{ return( ans$result %&gt;% purrr::pluck(&quot;plot_level&quot;) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(ws_fn_nm = x) ) } }) %&gt;% dplyr::bind_rows() # save readr::write_csv(training_img_annttd_crwns, file = training_eval_temp) ################################# # now for the big ones so we don&#39;t crash memory ## hate that this is manual but i&#39;m.....tired ################################# # filter for only those with this type of validation data submission_temp &lt;- training_submission %&gt;% ## !!!!!!!!!!!! do these separate dplyr::filter(plot_name %in% c(&quot;2018_NIWO_2_450000_4426000_image_crop&quot;)) %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_image_annotation==T) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) # validate training_img_annttd_crwns_temp &lt;- submission_temp$ws_fn_nm %&gt;% unique() %&gt;% purrr::map(function(x){ ans &lt;- safe_evaluate_image_crowns( predictions = submission_temp %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(ws_fn_nm == x) %&gt;% dplyr::select(xmin,ymin,xmax,ymax,label,plot_name) , show = F , summarize = T ) if(!is.null(ans$error)){ return(NULL) }else{ return( ans$result %&gt;% purrr::pluck(&quot;plot_level&quot;) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(ws_fn_nm = x) ) } }) %&gt;% dplyr::bind_rows() # add it training_img_annttd_crwns &lt;- training_img_annttd_crwns %&gt;% dplyr::bind_rows(training_img_annttd_crwns_temp) # save readr::write_csv(training_img_annttd_crwns, file = training_eval_temp) ################################# # now for the big ones so we don&#39;t crash memory ## hate that this is manual but i&#39;m.....tired ################################# # filter for only those with this type of validation data submission_temp &lt;- training_submission %&gt;% ## !!!!!!!!!!!! do these separate dplyr::filter(plot_name %in% c(&quot;2018_TEAK_3_315000_4094000_image_crop&quot;)) %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_image_annotation==T) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) # validate training_img_annttd_crwns_temp &lt;- submission_temp$ws_fn_nm %&gt;% unique() %&gt;% purrr::map(function(x){ ans &lt;- safe_evaluate_image_crowns( predictions = submission_temp %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(ws_fn_nm == x) %&gt;% dplyr::select(xmin,ymin,xmax,ymax,label,plot_name) , show = F , summarize = T ) if(!is.null(ans$error)){ return(NULL) }else{ return( ans$result %&gt;% purrr::pluck(&quot;plot_level&quot;) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(ws_fn_nm = x) ) } }) %&gt;% dplyr::bind_rows() # add it training_img_annttd_crwns &lt;- training_img_annttd_crwns %&gt;% dplyr::bind_rows(training_img_annttd_crwns_temp) # save readr::write_csv(training_img_annttd_crwns, file = training_eval_temp) }else{ training_img_annttd_crwns &lt;- readr::read_csv(training_eval_temp, progress = F, show_col_types = F) } # training_img_annttd_crwns %&gt;% dplyr::glimpse() our return data should include precision and recall values for each plot and variable window function that resulted in trees extracted (some window functions might not have extracted any trees) with image annotated crown evaluation data…unless the NeonTreeEvaluation::evaluate_image_crowns() resulted in an error count of the variable window functions which resulted in trees extracted and were successfully evaluated using image annotated crown ground truth data by plot training_submission %&gt;% sf::st_drop_geometry() %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter( has_image_annotation==T &amp; (data_type==&quot;training&quot; | is_new_image_annotation_training) ) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) %&gt;% dplyr::count(plot_name,ws_fn_nm) %&gt;% dplyr::left_join( training_img_annttd_crwns , by = dplyr::join_by(plot_name,ws_fn_nm) ) %&gt;% dplyr::group_by(plot_name) %&gt;% dplyr::summarize(ws_trees = dplyr::n(), ws_eval = sum(ifelse(is.na(precision),0,1))) %&gt;% dplyr::arrange(plot_name) %&gt;% kableExtra::kbl( caption = &quot;Count of variable window functions tested and successfully evaluated using image annotated crowns&quot; , digits = 0 , col.names = c( &quot;plot&quot;,&quot;window functions&lt;br&gt;with trees&quot;,&quot;window functions&lt;br&gt;successfully evaluated&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.6: Count of variable window functions tested and successfully evaluated using image annotated crowns plot window functionswith trees window functionssuccessfully evaluated 2018_JERC_4_742000_3451000_image_crop 15 15 2018_NIWO_2_450000_4426000_image_crop 15 13 2018_OSBS_4_405000_3286000_image 14 14 2018_TEAK_3_315000_4094000_image_crop 15 11 2019_OSBS_5_405000_3287000_image_crop 14 14 2019_OSBS_5_405000_3287000_image_crop2 15 15 DSNY_005_2018 13 13 DSNY_025_2018 13 13 plot precision and recall by plot training_img_annttd_crwns %&gt;% tidyr::pivot_longer(cols = c(precision,recall)) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, y = plot_name, color = ws_fn_nm)) + # ggplot2::geom_point(size = 4) + ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::labs( x = &quot;&quot;, y = &quot;&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Training data: validation against image annotated crowns ground truth&quot; ) + ggplot2::theme_light() + ggplot2::theme( axis.text.y = ggplot2::element_text(size=8) , panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) ) we’ll quickly compare the variable window functions by averaging across sites and making a “quadrant” plot whereby we overlay the median value to differentiate the “best” window size by the two dimensions (precision, recall)…the upper-right quadrant is the “best” of the variable window functions tested based on these training plots evaluated against image annotated crowns agg_pr_temp &lt;- training_img_annttd_crwns %&gt;% dplyr::group_by(ws_fn_nm) %&gt;% dplyr::summarise(dplyr::across(dplyr::where(is.numeric),mean)) agg_pr_temp %&gt;% ggplot2::ggplot(ggplot2::aes(x=precision,y=recall,color=ws_fn_nm)) + ggplot2::geom_hline(yintercept = median(agg_pr_temp$recall),linetype=&quot;dashed&quot;) + ggplot2::geom_vline(xintercept = median(agg_pr_temp$precision),linetype=&quot;dashed&quot;) + ggplot2::geom_point(size = 6) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::scale_x_continuous(limits = c(0,0.5)) + ggplot2::scale_y_continuous(limits = c(0,0.5)) + ggplot2::labs( color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Training data: validation against image annotated crowns ground truth&quot; , caption = &quot;*values are averaged across sites and plots&quot; ) + ggplot2::theme_light() + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) ) 4.6.4.2 Scores for field-collected stems # where should we save the file? training_eval_temp &lt;- &quot;../data/NeonTreeEvaluation_training_evaluate_field_stems.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(training_eval_temp)){ # if(T){ # filter for only those with this type of validation data submission_temp &lt;- training_submission %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_field_stems==T &amp; (data_type == &quot;training&quot; | is_new_field_stems_training)) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported # would need to dig through NeonTreeEvaluation, fix, and pull request... safe_evaluate_field_stems &lt;- purrr::safely(NeonTreeEvaluation::evaluate_field_stems) # validate training_fld_cllctd_stems &lt;- submission_temp$ws_fn_nm %&gt;% unique() %&gt;% purrr::map(function(x){ ans &lt;- safe_evaluate_field_stems( predictions = submission_temp %&gt;% # sf::st_drop_geometry() %&gt;% ### polygons work here ! dplyr::filter(ws_fn_nm == x) %&gt;% dplyr::select(xmin,ymin,xmax,ymax,label,plot_name) , show = F , summarize = T ) if(!is.null(ans$error)){ return(NULL) }else{ return( ans$result %&gt;% purrr::pluck(&quot;plot_level&quot;) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(ws_fn_nm = x) ) } }) %&gt;% dplyr::bind_rows() # save readr::write_csv(training_fld_cllctd_stems, file = training_eval_temp) }else{ training_fld_cllctd_stems &lt;- readr::read_csv(training_eval_temp, progress = F, show_col_types = F) } # training_fld_cllctd_stems %&gt;% dplyr::glimpse() our return data should include recall values for each plot and variable window function that resulted in trees extracted (some window functions might not have extracted any trees) with image annotated crown evaluation data…unless the NeonTreeEvaluation::evaluate_field_stems() resulted in an error count of the variable window functions which resulted in trees extracted and were successfully evaluated using image annotated crown ground truth data by plot training_submission %&gt;% sf::st_drop_geometry() %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter( has_field_stems==T &amp; (data_type==&quot;training&quot; | is_new_field_stems_training) ) %&gt;% dplyr::distinct(plot_name,plot_name_sans_yr) , by = &quot;plot_name&quot; ) %&gt;% dplyr::count(plot_name,plot_name_sans_yr,ws_fn_nm) %&gt;% dplyr::left_join( training_fld_cllctd_stems %&gt;% dplyr::select(plot_name,ws_fn_nm,recall) , by = dplyr::join_by(plot_name_sans_yr==plot_name,ws_fn_nm) ) %&gt;% dplyr::group_by(plot_name) %&gt;% dplyr::summarize(ws_trees = dplyr::n(), ws_eval = sum(ifelse(is.na(recall),0,1))) %&gt;% dplyr::arrange(plot_name) %&gt;% kableExtra::kbl( caption = &quot;Count of variable window functions tested and successfully evaluated using field collected stems&quot; , digits = 0 , col.names = c( &quot;plot&quot;,&quot;window functions&lt;br&gt;with trees&quot;,&quot;window functions&lt;br&gt;successfully evaluated&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.7: Count of variable window functions tested and successfully evaluated using field collected stems plot window functionswith trees window functionssuccessfully evaluated ABBY_065_2018 15 15 ABBY_075_2018 15 15 DEJU_014_2018 14 14 DEJU_016_2019 12 12 JERC_049_2018 14 14 JERC_063_2018 14 14 NIWO_009_2020 14 14 NIWO_040_2020 14 14 OSBS_004_2019 14 14 OSBS_035_2019 14 14 TALL_053_2018 15 15 TALL_061_2019 15 15 plot recall by plot training_fld_cllctd_stems %&gt;% tidyr::pivot_longer(cols = c(n,recall)) %&gt;% dplyr::filter(name==&quot;recall&quot;) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, y = plot_name, color = ws_fn_nm)) + # ggplot2::geom_point(size = 4) + ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::labs( x = &quot;&quot;, y = &quot;&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Training data: validation against field collected stems ground truth&quot; ) + ggplot2::theme_light() + ggplot2::theme( panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) ) 4.6.5 Best variable window function We are going to use the training data to determine the “best” ITD window function to use for a given NEON site since the cloud2trees package strongly recommends that “a different window size function is defined for each region of your study area with significantly different forest structure”. We’ll get the precision and recall for each plot and variable window function combination and then select the window function that results in the best tree detection by NEON site. We’ll also average across sites to select the best variable window function overall which we’ll use for NEON sites that did not have training data. We will determine the “best” variable window function by calculating the F-score which incorporates true positive, commission, and omission rates to determine how well the aerial point cloud-detected trees match the ground truth trees. As a measure of predictive performance, the highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero. \\[ \\textrm{F-score} = 2 \\times \\frac{\\bigl(precision \\times recall \\bigr)}{\\bigl(precision + recall \\bigr)} \\] let’s combine our evaluation results, collapse across plot to calculate an average precision and recall for each NEON site and variable window function combination, calculate F-score, choose the highest F-score acheived by a variable window function for each NEON site and overall by averaging over each site. # combine eval results training_eval_combined_temp &lt;- training_img_annttd_crwns %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::distinct(plot_name,siteID) , by = &quot;plot_name&quot; ) %&gt;% dplyr::select(siteID,plot_name,ws_fn_nm,recall,precision) %&gt;% dplyr::mutate(eval_type = &quot;img_annttd_crwns&quot;) %&gt;% dplyr::bind_rows( training_fld_cllctd_stems %&gt;% dplyr::select(siteID,plot_name,ws_fn_nm,recall) %&gt;% dplyr::mutate(eval_type = &quot;fld_cllctd_stems&quot;) ) # overall eval results training_eval_overall &lt;- training_eval_combined_temp %&gt;% dplyr::group_by(ws_fn_nm) %&gt;% dplyr::summarise( precision = mean(precision, na.rm = T) , recall = mean(recall, na.rm = T) , n_fld_cllctd_stems = sum(ifelse(eval_type == &quot;fld_cllctd_stems&quot;,1,0)) , n_img_annttd_crwns = sum(ifelse(eval_type == &quot;img_annttd_crwns&quot;,1,0)) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( dplyr::across(.cols = c(precision,recall), ~ifelse(is.nan(.x),as.numeric(NA),.x)) , f_score = dplyr::case_when( is.na(precision) | is.na(recall) ~ as.numeric(NA) , (precision+recall) == 0 ~ 0 , T ~ 2 * ( (precision*recall)/(precision+recall) ) ) ) %&gt;% dplyr::arrange(desc(f_score),desc(precision),desc(recall),desc(n_fld_cllctd_stems),desc(n_img_annttd_crwns)) %&gt;% dplyr::mutate(is_best_fn = dplyr::row_number()==1) # save it readr::write_csv(training_eval_overall, &quot;../data/NeonTreeEvaluation_training_eval_overall.csv&quot;, append = F, progress = F) # site eval results training_eval_by_site &lt;- training_eval_combined_temp %&gt;% dplyr::group_by(siteID,ws_fn_nm) %&gt;% dplyr::summarise( precision = mean(precision, na.rm = T) , recall = mean(recall, na.rm = T) , n_fld_cllctd_stems = sum(ifelse(eval_type == &quot;fld_cllctd_stems&quot;,1,0)) , n_img_annttd_crwns = sum(ifelse(eval_type == &quot;img_annttd_crwns&quot;,1,0)) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( dplyr::across(.cols = c(precision,recall), ~ifelse(is.nan(.x),as.numeric(NA),.x)) , f_score = dplyr::case_when( is.na(precision) | is.na(recall) ~ as.numeric(NA) , (precision+recall) == 0 ~ 0 , T ~ 2 * ( (precision*recall)/(precision+recall) ) ) ) # save it after cleaning 4.6.5.1 Best variable window function by NEON site we collapsed across plot to calculate an average precision and recall for each NEON site and variable window function combination training_eval_by_site %&gt;% tidyr::pivot_longer(cols = c(precision,recall,f_score)) %&gt;% dplyr::mutate( name = ifelse(name==&quot;f_score&quot;,&quot;F-score&quot;,name) %&gt;% forcats::fct_rev() , siteID = forcats::fct_rev(siteID) ) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = ws_fn_nm)) + # ggplot2::geom_point(size = 4) + ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::labs( x = &quot;&quot;, y = &quot;&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Training data: overall validation against ground truth by NEON site&quot; ) + ggplot2::theme_light() + ggplot2::theme( axis.text.x = ggplot2::element_text(size=7) , panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) ) Note that not every NEON site has a precision value filled (and no F-score). These NEON sites did not have enough data to use for training data to be evaluated against the image annotated crown ground truth data. For these sites, we’ll select the variable window function that resulted in the best recall as determined by comparing the training data to the field collected stems ground truth data. Ties across window functions will be broken by selecting the window function that resulted in the fewest trees extracted from the point cloud since, for the sites with recall only, the window function able to best detect trees by using fewer trees will be more precise trees_site_fn_temp &lt;- training_submission %&gt;% dplyr::mutate(area = sf::st_area(.)) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::group_by(plot_name,ws_fn_nm) %&gt;% dplyr::summarise(trees = dplyr::n(), area = sum(area, na.rm = T)) %&gt;% dplyr::ungroup() %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::distinct(plot_name,siteID) , by = &quot;plot_name&quot; ) %&gt;% dplyr::group_by(siteID,ws_fn_nm) %&gt;% dplyr::summarise( plots = dplyr::n() , trees = mean(trees,na.rm = T) , area = mean(area,na.rm = T)) %&gt;% dplyr::ungroup() # trees_site_fn_temp %&gt;% dplyr::glimpse() let’s look at the trees detected by NEON site and variable window function trees_site_fn_temp %&gt;% tidyr::pivot_longer(cols = c(trees,area)) %&gt;% dplyr::mutate( siteID = forcats::fct_rev(siteID) ) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = ws_fn_nm)) + # ggplot2::geom_point(size = 4) + ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) + ggplot2::facet_grid(cols = dplyr::vars(name), scales = &quot;free_x&quot;) + ggplot2::scale_x_log10() + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::labs( x = &quot;&quot;, y = &quot;&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Training data: trees detected and area covered by tree crowns&quot; ) + ggplot2::theme_light() + ggplot2::theme( panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) , axis.text.x = ggplot2::element_blank() ) + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) ) There are some variable window functions that resulted in many more trees extracted than others. If we only have recall to judge by, these functions will perform better than others which may actually be more suitable simply because there are more trees (more of the plot area is covered so it is more likely that trees will be “detected” with no penalty for including more trees). To put some checks in-place to detect these instances, we’ll exclude functions from consideration to be selected if recall is the only evaluation metric and the number of trees detected is more than two standard deviations from the mean number of trees detected in a plot at a site. We’ll apply the same constraints for the area covered by tree crowns as well to catch cases when there may be few trees detected but the individual crown area is very large so it covers more area even with fewer trees. trees_site_fn_temp &lt;- trees_site_fn_temp %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::mutate( mean_trees = mean(trees,na.rm = T), sd_trees = sd(trees,na.rm = T) , mean_area = mean(area,na.rm = T), sd_area = sd(area,na.rm = T) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( exclude_these_fns = ( abs(trees-mean_trees)&gt;(2*sd_trees) ) | ( abs(area-mean_area)&gt;(2*sd_area) ) ) pick the best window function training_eval_by_site &lt;- training_eval_by_site %&gt;% # attach trees dplyr::left_join( trees_site_fn_temp %&gt;% dplyr::select(-plots) , by = dplyr::join_by(siteID,ws_fn_nm) ) %&gt;% dplyr::arrange(siteID,desc(f_score),desc(precision),exclude_these_fns,desc(recall),area,trees) %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::mutate(is_best_fn = dplyr::row_number()==1) %&gt;% dplyr::ungroup() # save it readr::write_csv(training_eval_by_site, &quot;../data/NeonTreeEvaluation_training_eval_by_site.csv&quot;, append = F, progress = F) show the best window function by site training_eval_by_site %&gt;% tidyr::pivot_longer(cols = c(precision,recall,f_score)) %&gt;% dplyr::mutate( name = ifelse(name==&quot;f_score&quot;,&quot;F-score&quot;,name) %&gt;% forcats::fct_rev() , siteID = forcats::fct_rev(siteID) ) %&gt;% dplyr::arrange(is_best_fn) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = ws_fn_nm)) + # ggplot2::geom_point(size = 4) + ggplot2::geom_jitter( ggplot2::aes(shape = is_best_fn) ,height = 0.12 , width = 0 , size = 5 ) + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::scale_shape_manual(values = c(20,15)) + ggplot2::labs( x = &quot;&quot;, y = &quot;&quot; , color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Best variable window function by NEON site based on training data evaluation&quot; , caption = &quot;*a square indicates the best function&quot; ) + ggplot2::theme_light() + ggplot2::theme( axis.text.x = ggplot2::element_text(size=7) , panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) , shape = &quot;none&quot; ) table of the best function by site training_eval_by_site %&gt;% dplyr::filter(is_best_fn) %&gt;% dplyr::select(siteID,ws_fn_nm,precision,recall,f_score) %&gt;% dplyr::arrange(siteID) %&gt;% kableExtra::kbl( caption = &quot;Best variable window function by NEON site based on training data evaluation&quot; , digits = 2 , col.names = c( &quot;site&quot;,&quot;variable&lt;br&gt;window&lt;br&gt;function&quot; ,&quot;precision&quot;,&quot;recall&quot;,&quot;F-score&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.8: Best variable window function by NEON site based on training data evaluation site variablewindowfunction precision recall F-score ABBY log_less_ccv_fn NA 1.00 NA DEJU log_less_ccv_fn NA 0.86 NA DSNY lin_hi_slp_fn 0.46 0.76 0.58 JERC log_mor_ccv_fn 0.20 0.51 0.29 NIWO exp_fn 0.22 0.49 0.30 OSBS log_fn 0.14 0.38 0.20 TALL lin_lo_slp_fn NA 0.94 NA TEAK log_fn 0.37 0.38 0.37 4.6.5.2 Best variable window function overall to select the best overall variable window function to apply to NEON sites that lacked training data, we collapsed across all training sites and plots to calculate an average precision and recall at the variable window function level we can look at the full results table training_eval_overall %&gt;% dplyr::select(-c(is_best_fn)) %&gt;% dplyr::relocate(tidyselect::starts_with(&quot;n_&quot;), .after = dplyr::last_col()) %&gt;% kableExtra::kbl( caption = &quot;Best variable window function overall based on training data evaluation&quot; , digits = 2 , col.names = c( &quot;variable&lt;br&gt;window&lt;br&gt;function&quot; ,&quot;precision&quot;,&quot;recall&quot;,&quot;F-score&quot; ,&quot;plots&lt;br&gt;with ground truth&lt;br&gt;field collected stems&quot; ,&quot;plots&lt;br&gt;with ground truth&lt;br&gt;image ann. crowns&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.9: Best variable window function overall based on training data evaluation variablewindowfunction precision recall F-score plotswith ground truthfield collected stems plotswith ground truthimage ann. crowns log_mor_ccv_fn 0.26 0.57 0.36 12 8 lin_hi_slp_fn 0.25 0.51 0.34 12 8 log_morr_ccv_fn 0.24 0.50 0.33 12 8 log_fn 0.22 0.56 0.32 12 8 lin_hi_int_fn 0.20 0.61 0.30 12 8 log_les_ccv_fn 0.18 0.64 0.28 12 8 lin_fn 0.17 0.63 0.27 12 8 exp_fn 0.15 0.61 0.24 12 8 exp_mor_ccv_fn 0.06 0.48 0.10 12 8 lin_lo_slp_fn 0.04 0.64 0.08 12 8 log_less_ccv_fn 0.04 0.64 0.08 12 8 exp_les_ccv_fn 0.00 0.45 0.01 11 6 exp_less_ccv_fn 0.00 0.39 0.00 11 5 lin_lo_int_fn 0.00 0.60 0.00 12 6 exp_lesss_ccv_fn 0.00 0.18 0.00 4 3 keep in mind that these are average values across plots and sites so the actual metric values matter little…it’s the relative value that we care about we can view this “quadrant” plot whereby we overlay the median value to differentiate the “best” window size by the two dimensions (precision, recall)…the upper-right quadrant is the “best” of the variable window functions tested based on these training plots evaluated against image annotated crowns and field collected stems ground truth data training_eval_overall %&gt;% ggplot2::ggplot(ggplot2::aes(x=precision,y=recall,color=ws_fn_nm)) + ggplot2::geom_hline(yintercept = median(training_eval_overall$recall),linetype=&quot;dashed&quot;) + ggplot2::geom_vline(xintercept = median(training_eval_overall$precision),linetype=&quot;dashed&quot;) + ggplot2::geom_point(ggplot2::aes(shape=is_best_fn),size = 6) + ggplot2::scale_color_manual(values = pal_ws) + ggplot2::scale_shape_manual(values = c(20,15)) + ggplot2::scale_x_continuous(limits = c(0,0.4)) + ggplot2::scale_y_continuous(limits = c(0,0.7)) + ggplot2::labs( color = &quot;variable\\nwindow\\nfunction&quot; , subtitle = &quot;Best variable window function overall based on training data evaluation&quot; , caption = &quot;*a square indicates the best function&quot; ) + ggplot2::theme_light() + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(shape = 15, size = 6)) , shape = &quot;none&quot; ) now we’ll attach the best variable window function to our processing data lidar_df &lt;- lidar_df %&gt;% dplyr::left_join( training_eval_by_site %&gt;% dplyr::filter(is_best_fn) %&gt;% dplyr::select(siteID,ws_fn_nm) , by = &quot;siteID&quot; ) %&gt;% dplyr::mutate( ws_fn_nm = dplyr::coalesce( ws_fn_nm , training_eval_overall %&gt;% dplyr::filter(is_best_fn) %&gt;% dplyr::pull(ws_fn_nm) ) ) # save readr::write_csv(lidar_df, &quot;../data/NeonTreeEvaluation_lidar_df.csv&quot;, append = F, progress = F) 4.7 Evaluation validation process We can finally process the evaluation data using all that we have gleaned in our analysis with the training data 4.7.1 Extract trees for evaluation data as a reminder, here’s how many evaluation plots we have by NEON site lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter( data_type == &quot;evaluation&quot; &amp; !is_new_image_annotation_training &amp; !is_new_field_stems_training ) %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarise( n = dplyr::n() , evaluation_ann_plots = sum(ifelse(has_image_annotation,1,0)) , evaluation_fld_plots = sum(ifelse(has_field_stems,1,0)) ) %&gt;% kableExtra::kbl( caption = &quot;NEON sites with conifers and lidar plots for evaluation&quot; , col.names = c( &quot;site&quot;,&quot;unique&lt;br&gt;evaluation plots&quot; ,&quot;evaluation plots&lt;br&gt;with ground truth&lt;br&gt;image ann. crowns&quot; ,&quot;evaluation plots&lt;br&gt;with ground truth&lt;br&gt;field collected stems&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.10: NEON sites with conifers and lidar plots for evaluation site uniqueevaluation plots evaluation plotswith ground truthimage ann. crowns evaluation plotswith ground truthfield collected stems ABBY 12 2 11 DEJU 12 0 12 DSNY 4 4 0 JERC 20 6 19 MOAB 2 0 2 NIWO 40 10 39 ONAQ 1 1 0 OSBS 47 14 44 SOAP 2 2 0 TALL 21 2 20 TEAK 51 51 0 Now we’ll extract trees from the evaluation lidar data using the best window function for a site based on the training data # where should we save the file? submission_fn_temp &lt;- &quot;../data/NeonTreeEvaluation_evaluation_submission.gpkg&quot; # if we don&#39;t already have the data, run it if(!file.exists(submission_fn_temp)){ # if(T){ ### !!!!!!! take out when it gets real # just get evaluation evaluation_df_temp &lt;- lidar_df %&gt;% dplyr::filter( data_type == &quot;evaluation&quot; &amp; !is_new_image_annotation_training &amp; !is_new_field_stems_training ) # get trees for each evaluation plot and ws fn combination evaluation_submission &lt;- 1:nrow(evaluation_df_temp) %&gt;% purrr::map(function(x){ # get ws fn my_fn_nm &lt;- evaluation_df_temp %&gt;% dplyr::slice(x) %&gt;% dplyr::pull(ws_fn_nm) # run it ans &lt;- cloud2trees_for_eval( lidar_df_row = x , lidar_df = evaluation_df_temp , ws = my_ws_functions[[my_fn_nm]] ) ### some of these ws functions may not extract any trees ### account for this if(is.null(ans) || !inherits(ans,&quot;data.frame&quot;)){ return(NULL) }else{ return( ans # %&gt;% dplyr::mutate(ws_fn_nm = my_fn_nm) # uncomment for testing ) } }) %&gt;% dplyr::bind_rows() # save it sf::st_write(evaluation_submission, submission_fn_temp, append = F) }else{ evaluation_submission &lt;- sf::st_read(submission_fn_temp, quiet = T) } now we need to filter our tree list based on the height threshold for “canopy” trees evaluation_submission &lt;- evaluation_submission %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(plot_name,site_prcntl_ht) , by = &quot;plot_name&quot; ) %&gt;% dplyr::filter(tree_height_m&gt;=site_prcntl_ht) %&gt;% dplyr::select(-c(tree_height_m,site_prcntl_ht)) what did we get? evaluation_submission %&gt;% dplyr::glimpse() ## Rows: 26,831 ## Columns: 7 ## $ xmin &lt;dbl&gt; 552088.8, 552089.5, 552099.2, 552100.5, 552101.5, 552103.2, … ## $ xmax &lt;dbl&gt; 552089.5, 552090.2, 552099.8, 552101.5, 552101.8, 552104.2, … ## $ ymin &lt;dbl&gt; 5068041, 5068041, 5068041, 5068041, 5068041, 5068041, 506804… ## $ ymax &lt;dbl&gt; 5068042, 5068042, 5068042, 5068042, 5068042, 5068042, 506804… ## $ label &lt;chr&gt; &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tree&quot;, &quot;Tre… ## $ plot_name &lt;chr&gt; &quot;ABBY_076_2019&quot;, &quot;ABBY_076_2019&quot;, &quot;ABBY_076_2019&quot;, &quot;ABBY_076… ## $ geom &lt;POLYGON&gt; POLYGON ((552089 5068042, 5..., POLYGON ((552089.5 50680… 4.7.2 Evaluation: validation against ground truth We’ll compare our aerial point cloud-based tree detection and crown delineation results to two types of evaluation data (i.e. “ground truth” data) presented by (Weinstein et al. 2021): field-collected stems and image-annotated crowns. Field-collected stems offer precise tree locations but might not align with the position of the tree crown as viewed from above (e.g. due to tree lean), image-annotated crowns outline crown boundaries but are subjective. 4.7.2.1 Scores for image-annotated crowns # where should we save the file? evaluation_eval_temp &lt;- &quot;../data/NeonTreeEvaluation_evaluation_evaluate_image_crowns.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(evaluation_eval_temp)){ # if(T){ # filter for only those with this type of validation data submission_temp &lt;- evaluation_submission %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_image_annotation) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported # would need to dig through NeonTreeEvaluation, fix, and pull request... safe_evaluate_image_crowns &lt;- purrr::safely(NeonTreeEvaluation::evaluate_image_crowns) # validate evaluation_img_annttd_crwns &lt;- safe_evaluate_image_crowns( predictions = submission_temp %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(xmin,ymin,xmax,ymax,label,plot_name) , show = F , summarize = T ) if(!is.null(evaluation_img_annttd_crwns$error)){ stop(evaluation_img_annttd_crwns$error) }else{ evaluation_img_annttd_crwns &lt;- evaluation_img_annttd_crwns$result %&gt;% purrr::pluck(&quot;plot_level&quot;) %&gt;% dplyr::ungroup() } # calculate f-score evaluation_img_annttd_crwns &lt;- evaluation_img_annttd_crwns %&gt;% dplyr::mutate( dplyr::across(.cols = c(precision,recall), ~ifelse(is.nan(.x),as.numeric(NA),.x)) , f_score = dplyr::case_when( is.na(precision) | is.na(recall) ~ as.numeric(NA) , (precision+recall) == 0 ~ 0 , T ~ 2 * ( (precision*recall)/(precision+recall) ) ) ) # save readr::write_csv(evaluation_img_annttd_crwns, file = evaluation_eval_temp) }else{ evaluation_img_annttd_crwns &lt;- readr::read_csv(evaluation_eval_temp, progress = F, show_col_types = F) } # evaluation_img_annttd_crwns %&gt;% dplyr::glimpse() our return data should include precision and recall values for each plot that had trees extracted (some evaluation plots might not have extracted any trees) with image annotated crown evaluation data…unless the NeonTreeEvaluation::evaluate_image_crowns() resulted in an error count of the plots which resulted in trees extracted and were successfully evaluated using image annotated crown ground truth data evaluation_submission %&gt;% sf::st_drop_geometry() %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_image_annotation) %&gt;% dplyr::distinct(plot_name,siteID) , by = &quot;plot_name&quot; ) %&gt;% dplyr::count(siteID,plot_name) %&gt;% dplyr::left_join( evaluation_img_annttd_crwns , by = &quot;plot_name&quot; ) %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarize(plots = dplyr::n(), plots_eval = sum(ifelse(is.na(f_score),0,1))) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(pct = scales::percent(plots_eval/plots,accuracy = 1)) %&gt;% dplyr::arrange(siteID) %&gt;% kableExtra::kbl( caption = &quot;Count of plots with trees detected that were successfully evaluated using image annotated crowns&quot; , digits = 0 , col.names = c( &quot;plot&quot;,&quot;# plots&lt;br&gt;with trees&quot;,&quot;# plots&lt;br&gt;successfully evaluated&quot; ,&quot;% plots&lt;br&gt;successfully evaluated&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.11: Count of plots with trees detected that were successfully evaluated using image annotated crowns plot # plotswith trees # plotssuccessfully evaluated % plotssuccessfully evaluated ABBY 2 2 100% DSNY 4 4 100% JERC 6 6 100% NIWO 10 10 100% ONAQ 1 1 100% OSBS 14 14 100% SOAP 2 2 100% TALL 2 2 100% TEAK 51 51 100% plot precision, recall, and F-score by NEON site evaluation_img_annttd_crwns %&gt;% tidyr::pivot_longer(cols = c(precision,recall,f_score)) %&gt;% dplyr::inner_join(lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::distinct(plot_name,siteID),by=&quot;plot_name&quot;) %&gt;% dplyr::mutate( name = ifelse(name==&quot;f_score&quot;,&quot;F-score&quot;,name) %&gt;% forcats::fct_rev() , siteID = forcats::fct_rev(siteID) ) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = name)) + ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) + ggplot2::geom_boxplot( width=0.3,staplewidth=0.4,lwd=0.7 ,fill=NA,color = &quot;gray9&quot; , outliers = F ) + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_color_viridis_d(begin=0.3,end=0.7) + ggplot2::labs( x = &quot;&quot;, y = &quot;&quot;, color = &quot;&quot; , subtitle = &quot;Evaluation data: validation against image annotated crowns ground truth by NEON site&quot; ) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.x = ggplot2::element_text(size=7) , panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) where are these NEON sites that we were able to evaluate using the image annotated crowns ground truth data lidar_df %&gt;% dplyr::inner_join( evaluation_img_annttd_crwns %&gt;% dplyr::distinct(plot_name) ) %&gt;% dplyr::count(siteID) %&gt;% dplyr::rename(plots_evaluated=n) %&gt;% mapview::mapview( cex = &quot;plots_evaluated&quot; , label = &quot;siteID&quot; , legend = F # , layer.name = &quot;# plots evaluated&quot; , col.regions = &quot;firebrick&quot; ) let’s review some figures comparing the image-annotated crowns versus the bounding box of our delineated crown polygons in the RGB plots: “red” boxes are crowns from our point cloud-based method extracted and “black” are the image annotated crowns set.seed(888) evaluation_img_annttd_crwns %&gt;% dplyr::ungroup() %&gt;% dplyr::slice_sample(n=3) %&gt;% dplyr::pull(plot_name) %&gt;% purrr::map(\\(x) NeonTreeEvaluation::image_crowns( predictions = evaluation_submission %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_image_annotation) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(plot_name==x) , show = T , project_boxes = F )) in the RGB plots: “red” boxes are crowns from our point cloud-based method extracted and “black” are the image annotated crowns 4.7.2.2 Scores for field-collected stems # where should we save the file? evaluation_eval_temp &lt;- &quot;../data/NeonTreeEvaluation_evaluation_evaluate_field_stems.csv&quot; # if we don&#39;t already have the data, run it if(!file.exists(evaluation_eval_temp)){ # if(T){ # filter for only those with this type of validation data submission_temp &lt;- evaluation_submission %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_field_stems) %&gt;% dplyr::distinct(plot_name) , by = &quot;plot_name&quot; ) # safe it bc: error in `clue::solve_LSAP()`: long vectors (argument 1) are not supported # would need to dig through NeonTreeEvaluation, fix, and pull request... safe_evaluate_field_stems &lt;- purrr::safely(NeonTreeEvaluation::evaluate_field_stems) # validate # validate evaluation_fld_cllctd_stems &lt;- safe_evaluate_field_stems( predictions = submission_temp %&gt;% # sf::st_drop_geometry() %&gt;% ### polygons work here ! dplyr::select(xmin,ymin,xmax,ymax,label,plot_name) , show = F , summarize = T ) if(!is.null(evaluation_fld_cllctd_stems$error)){ stop(evaluation_fld_cllctd_stems$error) }else{ evaluation_fld_cllctd_stems &lt;- evaluation_fld_cllctd_stems$result %&gt;% purrr::pluck(&quot;plot_level&quot;) %&gt;% dplyr::ungroup() } # save readr::write_csv(evaluation_fld_cllctd_stems, file = evaluation_eval_temp) }else{ evaluation_fld_cllctd_stems &lt;- readr::read_csv(evaluation_eval_temp, progress = F, show_col_types = F) } our return data should include recall values for each plot and variable window function that resulted in trees extracted (some window functions might not have extracted any trees) with image annotated crown evaluation data…unless the NeonTreeEvaluation::evaluate_field_stems() resulted in an error count of the plots which resulted in trees extracted and were successfully evaluated using field collected stems ground truth data evaluation_submission %&gt;% sf::st_drop_geometry() %&gt;% dplyr::inner_join( lidar_df %&gt;% sf::st_drop_geometry() %&gt;% dplyr::filter(has_field_stems) %&gt;% dplyr::distinct(plot_name,siteID,plot_name_sans_yr) , by = &quot;plot_name&quot; ) %&gt;% dplyr::count(siteID,plot_name,plot_name_sans_yr) %&gt;% dplyr::left_join( evaluation_fld_cllctd_stems %&gt;% dplyr::select(-siteID) , by = dplyr::join_by(plot_name_sans_yr==plot_name) ) %&gt;% dplyr::group_by(siteID) %&gt;% dplyr::summarize(plots = dplyr::n(), plots_eval = sum(ifelse(is.na(recall),0,1))) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(pct = scales::percent(plots_eval/plots,accuracy = 1)) %&gt;% dplyr::arrange(siteID) %&gt;% kableExtra::kbl( caption = &quot;Count of plots with trees detected that were successfully evaluated using field collected stems&quot; , digits = 0 , col.names = c( &quot;plot&quot;,&quot;# plots&lt;br&gt;with trees&quot;,&quot;# plots&lt;br&gt;successfully evaluated&quot; ,&quot;% plots&lt;br&gt;successfully evaluated&quot; ) , escape = F ) %&gt;% kableExtra::kable_styling() Table 4.12: Count of plots with trees detected that were successfully evaluated using field collected stems plot # plotswith trees # plotssuccessfully evaluated % plotssuccessfully evaluated ABBY 11 11 100% DEJU 12 12 100% JERC 19 19 100% MOAB 2 2 100% NIWO 38 38 100% OSBS 44 44 100% TALL 20 18 90% plot recall by NEON site evaluation_fld_cllctd_stems %&gt;% tidyr::pivot_longer(cols = c(recall,n)) %&gt;% dplyr::filter(name==&quot;recall&quot;) %&gt;% dplyr::mutate( siteID = forcats::fct_rev(siteID) ) %&gt;% ggplot2::ggplot(ggplot2::aes(x = value, y = siteID, color = name)) + ggplot2::geom_jitter(height = 0.12, width = 0, size = 4) + ggplot2::geom_boxplot( width=0.3,staplewidth=0.4,lwd=0.7 ,fill=NA,color = &quot;gray9&quot; , outliers = F ) + ggplot2::facet_grid(cols = dplyr::vars(name)) + ggplot2::scale_color_manual(values=viridis::viridis(n=3,begin=0.3,end=0.7)[1]) + ggplot2::labs( x = &quot;&quot;, y = &quot;&quot;, color = &quot;&quot; , subtitle = &quot;Evaluation data: validation against field collected ground truth by NEON site&quot; ) + ggplot2::theme_light() + ggplot2::theme( legend.position = &quot;none&quot; , axis.text.x = ggplot2::element_text(size=7) , panel.grid.major.y = ggplot2::element_line(color=&quot;gray22&quot;) , strip.text = ggplot2::element_text(color = &quot;black&quot;, size = 10) ) the recall values for our lidar-based tree list are much better using the field collected stems for validation that using the image annotated crowns…might this suggest that our “canopy” filter is aligned for field stems (which we based the filter on) but not for crowns visible from aerial photography "],["appendix.html", "Section 5 Appendix 5.1 Example lidar data 5.2 CBH Process 5.3 HMD Process", " Section 5 Appendix In this section we’ll provide some more detail and work through examples for select processes to extract individual tree components using aerial point cloud data 5.1 Example lidar data we’ll use some of our trees and height-normalized point cloud data generated in our point cloud processing load the tree crown polygons # get the data from already run crowns_sf &lt;- list.files( &quot;../data/point_cloud_processing_delivery&quot; , pattern = &quot;final_detected_crowns.*\\\\.gpkg$&quot; , full.names = T ) %&gt;% normalizePath() %&gt;% purrr::map(\\(x) sf::st_read( dsn = x , quiet = T ) ) %&gt;% dplyr::bind_rows() load the central 0.1 ha area of our study area aoi &lt;- sf::st_read(&quot;../data/point_cloud_processing_delivery/raw_las_ctg_info.gpkg&quot;) %&gt;% sf::st_union() %&gt;% sf::st_centroid() %&gt;% sf::st_buffer(sqrt(1000/4), endCapStyle = &quot;SQUARE&quot;) %&gt;% sf::st_transform(sf::st_crs(crowns_sf)) ## Reading layer `raw_las_ctg_info&#39; from data source ## `C:\\Data\\usfs\\lidar_phys_fire_mods\\data\\point_cloud_processing_delivery\\raw_las_ctg_info.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 42 features and 34 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 467000 ymin: 3808000 xmax: 473000 ymax: 3815000 ## Projected CRS: NAD83(2011) / UTM zone 12N filter for trees in the aoi crowns_sf &lt;- crowns_sf %&gt;% # join so we get the full crown dplyr::inner_join( sf::st_intersection(crowns_sf, aoi) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::distinct(treeID) , by = &quot;treeID&quot; ) %&gt;% # make treeID numeric for later dplyr::mutate( treeID_bu = treeID , treeID = treeID %&gt;% as.factor %&gt;% as.numeric() ) %&gt;% cloud2trees::simplify_multipolygon_crowns() now, we’ll load in the point cloud data for our aoi las_ctg &lt;- lidR::readLAScatalog(&quot;../data/point_cloud_processing_delivery/norm_las/&quot;) lidR::opt_progress(las_ctg) &lt;- F las &lt;- lidR::clip_roi(las_ctg, crowns_sf %&gt;% sf::st_bbox() %&gt;% sf::st_as_sfc() %&gt;% sf::st_buffer(2)) plot a sample las %&gt;% lidR::plot( color = &quot;Z&quot;, bg = &quot;white&quot;, legend = T , pal = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) ) now we’ll attach the treeID column to the normalized las file las &lt;- cloud2trees::polygon_attribute_to_las(las = las, poly_df = crowns_sf, attribute = &quot;treeID&quot;, force_crs = T) let’s look at our point cloud colored by treeID las %&gt;% lidR::filter_poi(!is.na(treeID)) %&gt;% lidR::plot(color = &quot;treeID&quot;, bg = &quot;white&quot;, legend = F) let’s filter for a single tree # sf one_tree_sf &lt;- crowns_sf %&gt;% dplyr::filter(tree_height_m == max(tree_height_m)) %&gt;% dplyr::slice(1) # las one_tree_las &lt;- las %&gt;% lidR::filter_poi(treeID==one_tree_sf$treeID) check our one tree point cloud # plot it plot3D::scatter3D( x = one_tree_las@data$X , y = one_tree_las@data$Y , z = one_tree_las@data$Z , colvar = one_tree_las@data$Z , cex = 0.6, pch = 19 , colkey = T , phi = 11 , col = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) , main =&quot;single tree point cloud&quot; ) 5.2 CBH Process Let’s go through the process to extract CBH from the point cloud using the same settings as in our point cloud processing. CBH is extracted directly from the height normalized point cloud using the process outlined in Viedma et al. (2024) and implemented via cloud2trees::ladderfuelsr_cbh(). See cloud2trees::trees_cbh() for a simple way to extract tree CBH from a height normalized point cloud based on a tree list with crown polygons. # CALL IT ladderfuelsr_cbh_ans &lt;- cloud2trees::ladderfuelsr_cbh( las = one_tree_las , min_vhp_n = 3 , voxel_grain_size_m = 1 , dist_btwn_bins_m = 1 , min_fuel_layer_ht_m = 1 , lad_pct_gap = 25 , lad_pct_base = 25 , num_jump_steps = 1 , min_lad_pct = 10 , frst_layer_min_ht_m = 1 ) let’s see what we got ladderfuelsr_cbh_ans %&gt;% names() ## [1] &quot;gaps_fbhs&quot; &quot;lad_profile&quot; &quot;gaps_perc&quot; &quot;metrics_distance&quot; ## [5] &quot;metrics_depth&quot; &quot;real_fbh&quot; &quot;real_depth&quot; &quot;eff_gap&quot; ## [9] &quot;layers_lad_df&quot; &quot;cbh_metrics&quot; ladderfuelsr_cbh_ans$lad_profile %&gt;% dplyr::glimpse() ## Rows: 30 ## Columns: 3 ## $ treeID &lt;fct&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,… ## $ height &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, … ## $ lad &lt;dbl&gt; 0.000000000, 0.002752691, 0.000000000, 0.000000000, 0.000000000… Create our own plot of the gaps and fuel layers base height in the vertical tree leaf area density (LAD) profile. Almeida et al. (2019) describe the process for generating leaf area density profiles and make the process accessible via the leafR package. In cloud2trees, methods to generate LAD profiles are packaged in cloud2trees::leafr_for_ladderfuelsr() which enables batch processing of individual tree point clouds and generates the input needed to extract tree CBH ggplot() + geom_path(data = ladderfuelsr_cbh_ans$lad_profile, mapping = aes(x = lad, y = height)) + geom_point(data = ladderfuelsr_cbh_ans$lad_profile, mapping = aes(x = lad, y = height)) + # gaps data geom_hline( data = ladderfuelsr_cbh_ans$gaps_fbhs %&gt;% dplyr::select( tidyselect::starts_with(&quot;gap&quot;) &amp; !tidyselect::starts_with(&quot;gap_&quot;) ) %&gt;% tidyr::pivot_longer(dplyr::everything()) , aes(yintercept = value, color = &quot;gaps&quot;) , linetype = &quot;dotted&quot; , lwd = 1.2 ) + # fbh data geom_hline( data = ladderfuelsr_cbh_ans$gaps_fbhs %&gt;% dplyr::select( tidyselect::starts_with(&quot;cbh&quot;) &amp; !tidyselect::starts_with(&quot;cbh_&quot;) ) %&gt;% tidyr::pivot_longer(dplyr::everything()) , aes(yintercept = value, color = &quot;FBHs&quot;) , linetype = &quot;dotdash&quot; , lwd = 1.2 ) + scale_color_manual(values = c(&quot;green4&quot;, &quot;red&quot;), name = &quot;&quot;) + scale_y_continuous(breaks = scales::extended_breaks(10)) + theme_light() + theme(legend.position = &quot;top&quot;) note, the gap and cbh columns in ladderfuelsr_cbh_ans$gaps_fbhs contain the data needed for the plot ( but not the gap_ and cbh_ columns ;/ ). but what are these columns? cbh - Height of the fuel layer base height (m) gap - Height of gap between fuel layers (m) 5.2.1 Return CBH Metrics ladderfuelsr_cbh_ans$cbh_metrics %&gt;% dplyr::glimpse() ## Rows: 1 ## Columns: 29 ## $ treeID &lt;fct&gt; 21 ## $ treeID1 &lt;dbl&gt; 21 ## $ dptf1 &lt;dbl&gt; 19 ## $ effdist1 &lt;dbl&gt; 10 ## $ Hcbh1 &lt;dbl&gt; 11.5 ## $ Hdist1 &lt;dbl&gt; 10.5 ## $ Hdptf1 &lt;dbl&gt; 30.5 ## $ max1 &lt;dbl&gt; 30.5 ## $ Hcbh1_Hdptf1 &lt;dbl&gt; 98.31702 ## $ max_height &lt;dbl&gt; 30.5 ## $ nlayers &lt;dbl&gt; 1 ## $ maxlad_Hcbh &lt;dbl&gt; 11.5 ## $ maxlad_Hdist &lt;dbl&gt; 10.5 ## $ maxlad_Hdptf &lt;dbl&gt; 30.5 ## $ maxlad_dptf &lt;dbl&gt; 19 ## $ maxlad_effdist &lt;dbl&gt; 10 ## $ maxlad_lad &lt;dbl&gt; 98.31702 ## $ max_Hcbh &lt;dbl&gt; 11.5 ## $ max_Hdist &lt;dbl&gt; 10.5 ## $ max_Hdptf &lt;dbl&gt; 30.5 ## $ max_dptf &lt;dbl&gt; 19 ## $ max_effdist &lt;dbl&gt; 10 ## $ max_lad &lt;dbl&gt; 98.31702 ## $ last_Hcbh &lt;dbl&gt; 11.5 ## $ last_Hdist &lt;dbl&gt; 10.5 ## $ last_Hdptf &lt;dbl&gt; 30.5 ## $ last_dptf &lt;dbl&gt; 19 ## $ last_effdist &lt;dbl&gt; 10 ## $ last_lad &lt;dbl&gt; 98.31702 what are these? treeID: tree ID with strings and numeric values treeID1: tree ID with only numeric values dptf: Depth of fuel layers (m) after considering distances greater than the actual height bin step effdist: Effective distance between consecutive fuel layers (m) after considering distances greater than any number of steps Hcbh: Base height of each fuel separated by a distance greater than the certain number of steps Hdptf: Height of the depth of fuel layers (m) after considering distances greater than the actual step Hdist: Height of the distance (&gt; any number of steps) between consecutive fuel layers (m) Hcbh_Hdptf - Percentage of LAD values comprised in each effective fuel layer maxlad_Hcbh - Height of the CBH of the segmented tree based on the maximum LAD percentage maxlad1_Hcbh - Height of the CBH from the second fuel layer when the maximum LAD occurred in the first fuel layer but its depth &lt;= “hdepth1_height” max_Hcbh - Height of the CBH of the segmented tree based on the maximum distance found in its profile last_Hcbh - Height of the CBH of the segmented tree based on the last distance found in its profile maxlad_ - Values of distance and fuel depth and their corresponding heights at the maximum LAD percentage maxlad1_ - Values of distance and fuel depth and their corresponding heights for the second fuel layer when the maximum LAD occurred in the first fuel layer but its depth &lt;= “hdepth1_height” max_ - Values of distance and fuel depth and their corresponding heights at the maximum distance last_ - Values of distance and fuel depth and their corresponding heights at the last distance nlayers - Number of effective fuel layers max_height - Maximum height of the tree profile there are also some plotting functions # Generate plots for fuels LAD metrics plots_cbh_maxlad &lt;- LadderFuelsR::get_plots_cbh_LAD( LAD_profiles = ladderfuelsr_cbh_ans$lad_profile , cbh_metrics = ladderfuelsr_cbh_ans$cbh_metrics , min_height = 0.5 ) plots_cbh_maxdist &lt;- LadderFuelsR::get_plots_cbh_maxdist( LAD_profiles = ladderfuelsr_cbh_ans$lad_profile , cbh_metrics = ladderfuelsr_cbh_ans$cbh_metrics , min_height = 0.5 ) plots_cbh_lastdist &lt;- LadderFuelsR::get_plots_cbh_lastdist( LAD_profiles = ladderfuelsr_cbh_ans$lad_profile , cbh_metrics = ladderfuelsr_cbh_ans$cbh_metrics , min_height = 0.5 ) # patchwork them (plots_cbh_maxlad[[1]] + labs(title = &quot;get_plots_cbh_LAD&quot;)) + (plots_cbh_maxdist[[1]] + labs(title = &quot;get_plots_cbh_maxdist&quot;)) + (plots_cbh_lastdist[[1]] + labs(title = &quot;get_plots_cbh_lastdist&quot;)) + patchwork::plot_layout(ncol = 2) these plots represent the three criteria to define the CBH in a segmented tree: get_plots_cbh_LAD = the fuel layer containing the maximum LAD percentage (column named maxlad_Hcbh) get_plots_cbh_maxdist = the fuel layer located at the highest distance (column named max_Hcbh) get_plots_cbh_lastdist = the fuel layer separated by the last effective distance (column named last_Hcbh) in our point cloud processing we used the fuel layer separated by the last effective distance (column named last_Hcbh) by setting the parameter which_cbh = \"lowest\" in cloud2trees::trees_cbh() plots_cbh_lastdist[[1]] 5.2.2 CBH on the point cloud make a view of the CBH we estimated plotted on the point cloud # make a matrix to represent the cbh x_temp &lt;- seq( min(one_tree_las@data$X) , max(one_tree_las@data$X) , length.out = 2 ) y_temp &lt;- seq( min(one_tree_las@data$Y) , max(one_tree_las@data$Y) , length.out = 2 ) xy_temp &lt;- expand.grid(x = x_temp, y = y_temp) z_temp &lt;- matrix( rep( ladderfuelsr_cbh_ans$cbh_metrics$last_Hcbh , nrow(xy_temp) ) , nrow = length(x_temp), ncol = length(y_temp) ) # plot it plot3D::scatter3D( x = one_tree_las@data$X , y = one_tree_las@data$Y , z = one_tree_las@data$Z , colvar = one_tree_las@data$Z , cex = 0.6, pch = 19 , colkey = T , phi = 11 , col = harrypotter::hp(n=50, house = &quot;gryffindor&quot;) , main =&quot;CBH shown in black&quot; , surf = list( x = x_temp , y = y_temp , z = z_temp , facets = NA , border = &quot;black&quot; , lwd = 2 ) ) 5.3 HMD Process Let’s go through the process to extract height of the maximum crown diameter (HMD) from the point cloud using the same settings as in our point cloud processing. HMD is extracted directly from the height normalized point cloud by finding the height of the non-ground point farthest from the tree center (i.e. tree top). See cloud2trees::trees_hmd() for a simple way to extract tree CBH from a height normalized point cloud based on a tree list with crown polygons. 5.3.1 One Tree Example this is the tidy data process using the data frame of the point cloud grouped by treeID implemented in cloud2trees::trees_hmd() look at this point cloud data one_tree_las@data %&gt;% dplyr::glimpse() ## Rows: 868 ## Columns: 19 ## $ X &lt;dbl&gt; 469989.5, 469989.4, 469989.4, 469989.3, 469989.2, 46… ## $ Y &lt;dbl&gt; 3811497, 3811498, 3811498, 3811498, 3811499, 3811499… ## $ Z &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ gpstime &lt;dbl&gt; 62346815, 62346815, 62346815, 62346815, 62346815, 62… ## $ Intensity &lt;int&gt; 35209, 35466, 33924, 32896, 33410, 33667, 34438, 357… ## $ ReturnNumber &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ NumberOfReturns &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ ScanDirectionFlag &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0… ## $ EdgeOfFlightline &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Classification &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ ScannerChannel &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Synthetic_flag &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ Keypoint_flag &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ Withheld_flag &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ Overlap_flag &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ ScanAngle &lt;dbl&gt; -10.002, -10.002, -10.002, -10.002, -10.002, -10.002… ## $ UserData &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ PointSourceID &lt;int&gt; 488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 48… ## $ treeID &lt;dbl&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, … the key components we’ll use are the xyz coordinates pts_temp &lt;- one_tree_las@data %&gt;% dplyr::rename_with(.cols = -c(treeID), .fn = tolower) %&gt;% # get rid of ground, water, and noise points dplyr::filter(!classification %in% c(2,9,18)) %&gt;% dplyr::group_by(treeID) %&gt;% # this is key dplyr::mutate( # find the highest point in the tree as the tree &quot;center&quot; is_tree_center = z==max(z) , tree_center_x = dplyr::first(ifelse(is_tree_center, x, NA), na_rm = T) # first in case many points are center , tree_center_y = dplyr::first(ifelse(is_tree_center, y, NA), na_rm = T) # first in case many points are center # Calculate the distance from the center to the point , dist_to_center = sqrt((x - tree_center_x)^2 + (y - tree_center_y)^2) , is_max_dist_to_center = dist_to_center == max(dist_to_center) # classify points for plotting , grp_pt = dplyr::case_when( is_tree_center ~ 0 , is_max_dist_to_center ~ 1 , T ~ 2 ) %&gt;% factor( ordered = T , levels = 0:2 , labels = c(&quot;tree center&quot;, &quot;farthest point\\nfrom center&quot;, &quot;other&quot;) ) ) %&gt;% dplyr::ungroup() let’s plot all of that in 2-D space as if we are looking down at the tree from above # plot ggplot(data = pts_temp, mapping = aes(x=x,y=y,color=z, shape = grp_pt)) + geom_point( mapping = aes( size = ifelse(is_tree_center | is_max_dist_to_center, 1.5, 1) ) ) + harrypotter::scale_color_hp(&quot;gryffindor&quot;, name = &quot;Z (ht m)&quot;) + scale_shape_manual(values = c(17, 15, 16), name = &quot;&quot;) + labs( title = &quot;single tree point cloud HMD process&quot; , caption = paste0( &quot;HMD = &quot; , round( pts_temp %&gt;% dplyr::filter(is_max_dist_to_center) %&gt;% dplyr::slice_head(n=1) %&gt;% dplyr::pull(z) , 1 ) , &quot; m&quot; , &quot;\\n(ht of point farthest from center)&quot; ) ) + theme_light() + theme(axis.text = element_blank()) + guides(size = &quot;none&quot;) look at the data on the point farthest from the tree center pts_temp %&gt;% dplyr::filter(is_max_dist_to_center) %&gt;% dplyr::select(x,y,z,dist_to_center,is_max_dist_to_center) %&gt;% dplyr::glimpse() ## Rows: 1 ## Columns: 5 ## $ x &lt;dbl&gt; 469987.8 ## $ y &lt;dbl&gt; 3811496 ## $ z &lt;dbl&gt; 13.68 ## $ dist_to_center &lt;dbl&gt; 5.433323 ## $ is_max_dist_to_center &lt;lgl&gt; TRUE the HMD for this tree is contained in the z column the cloud2trees::trees_hmd() process will give us the same answer (hopefully) # the function requires a LAScatalog file_temp &lt;- tempfile(fileext = &quot;.las&quot;) lidR::writeLAS(one_tree_las, file = file_temp) # trees_hmd that cloud2trees::trees_hmd(trees_poly = one_tree_sf, norm_las = file_temp) %&gt;% sf::st_drop_geometry() %&gt;% dplyr::select(treeID, max_crown_diam_height_m) ## treeID max_crown_diam_height_m ## 1 21 13.68 success! 5.3.2 Tree List Example let’s work with our clipped las data and tree crown list that we created above and use cloud2trees::trees_hmd() to attempt a full census of tree HMD extraction, if HMD extraction is unsuccessful we’ll estimate the missing HMD values using the successfully extracted trees as training data # the function requires a LAScatalog file_temp &lt;- tempfile(fileext = &quot;.las&quot;) lidR::writeLAS(las, file = file_temp) # call the function trees_hmd_ans &lt;- cloud2trees::trees_hmd( trees_poly = crowns_sf , norm_las = file_temp , tree_sample_prop = 1 , estimate_missing_hmd = T ) What is this data? trees_hmd_ans %&gt;% dplyr::select(treeID, tree_height_m, max_crown_diam_height_m, is_training_hmd) %&gt;% dplyr::glimpse() ## Rows: 34 ## Columns: 5 ## $ treeID &lt;dbl&gt; 5, 25, 34, 21, 6, 24, 26, 9, 3, 8, 30, 32, 23,… ## $ tree_height_m &lt;dbl&gt; 19.19, 30.10, 25.27, 30.38, 15.31, 24.46, 25.6… ## $ max_crown_diam_height_m &lt;dbl&gt; 5.44, 21.17, 13.59, 13.68, 7.94, 13.99, 19.88,… ## $ is_training_hmd &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… ## $ geom &lt;POLYGON [m]&gt; POLYGON ((469982.8 3811510,..., POLYGO… Let’s look at the relationship between tree height and tree HMD as extracted from the point cloud. Note, that we do not expect a perfect linear relationship between tree height and HMD throughout the entire height range because HMD is also determined spatially (e.g. as a fire moves through a stand). trees_hmd_ans %&gt;% dplyr::arrange(is_training_cbh) %&gt;% ggplot2::ggplot( mapping = ggplot2::aes(x = tree_height_m, y = max_crown_diam_height_m, color=is_training_hmd) ) + ggplot2::geom_point() + ggplot2::labs(x = &quot;tree ht. (m)&quot;, y = &quot;tree HMD (m)&quot;) + ggplot2::scale_y_continuous(breaks = scales::extended_breaks(n=12)) + ggplot2::scale_x_continuous(breaks = scales::extended_breaks(n=14)) + ggplot2::scale_color_viridis_d(option = &quot;turbo&quot;, begin = 0.2, alpha = 0.8, name = &quot;is HMD\\nfrom cloud?&quot;) + ggplot2::theme_light() We can look at this data spatially too. trees_hmd_ans %&gt;% dplyr::arrange(is_training_hmd) %&gt;% ggplot2::ggplot(mapping = ggplot2::aes(fill = max_crown_diam_height_m, color=is_training_hmd)) + ggplot2::geom_sf() + ggplot2::scale_color_viridis_d(option = &quot;turbo&quot;, begin = 0.2, alpha = 0.8, name = &quot;is HMD\\nfrom cloud?&quot;) + ggplot2::scale_fill_distiller(palette = &quot;Greys&quot;, name = &quot;tree HMD (m)&quot;, direction = 1) + ggplot2::theme_void() + ggplot2::theme( legend.position = &quot;top&quot;, legend.direction = &quot;horizontal&quot; , panel.border = ggplot2::element_rect(color = &quot;black&quot;, fill = NA) ) + ggplot2::guides( color = ggplot2::guide_legend(override.aes = list(lwd = 3, fill = NA)) ) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
